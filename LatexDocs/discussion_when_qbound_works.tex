% UPDATED DISCUSSION SECTION: WHEN QBOUND WORKS AND WHY

\subsection{When to Use QBound: Evidence-Based Recommendations}
\label{subsec:when-to-use-qbound}

Based on comprehensive evaluation across 10 environments with 5 seeds (50 independent runs), we provide evidence-based guidelines for QBound applicability.

\subsubsection{Decision Framework}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1.5cm and 2cm,
  decision/.style={diamond, draw, fill=blue!20, text width=5em, text centered, inner sep=0pt, minimum height=3em},
  block/.style={rectangle, draw, fill=green!20, text width=8em, text centered, rounded corners, minimum height=2em},
  fail/.style={rectangle, draw, fill=red!20, text width=8em, text centered, rounded corners, minimum height=2em},
  line/.style={draw, -latex'}
]

\node [decision] (reward-sign) {Rewards positive?};
\node [block, below left=of reward-sign] (positive-yes) {Use Hard QBound (+12-34\%)};
\node [decision, below right=of reward-sign] (continuous) {Continuous actions?};
\node [block, below left=of continuous] (soft-qbound) {Use Soft QBound (+15-25\%)};
\node [fail, below right=of continuous] (negative-fail) {Don't use QBound (-3 to -47\%)};

\path [line] (reward-sign) -- node[above left] {Yes} (positive-yes);
\path [line] (reward-sign) -- node[above right] {No} (continuous);
\path [line] (continuous) -- node[above left] {Yes (DDPG/TD3)} (soft-qbound);
\path [line] (continuous) -- node[above right] {No/PPO} (negative-fail);

\end{tikzpicture}
\caption{QBound Decision Tree Based on Reward Structure}
\label{fig:qbound-decision-tree}
\end{figure}

\subsubsection{Case 1: Positive Dense Rewards -- STRONG SUCCESS}

\textbf{Criteria:}
\begin{itemize}
    \item Rewards are positive ($r > 0$) and accumulate over time
    \item Examples: CartPole (+1 per step), survival tasks, score accumulation games
\end{itemize}

\textbf{Why QBound Works:}
\begin{enumerate}
    \item \textbf{No natural upper bound:} Neural networks with linear output layers can produce arbitrarily large Q-values
    \item \textbf{Overestimation vulnerability:} Positive rewards compound through bootstrapping: $Q(s,a) = r + \gamma \max Q(s',a')$
    \item \textbf{QBound provides essential constraint:} Explicit $Q_{\max}$ prevents unbounded growth
\end{enumerate}

\textbf{Implementation:}
\begin{lstlisting}[language=Python]
# Hard QBound for positive dense rewards
Q_min = 0  # Worst case: immediate failure
Q_max = r_step * (1 - gamma**H) / (1 - gamma)  # Geometric series
# Example (CartPole): Q_max = 1 * (1-0.99^500)/(1-0.99) = 99.34

# During training:
Q_next = target_network(next_states).max(dim=1)
Q_next_clipped = torch.clamp(Q_next, Q_min, Q_max)
Q_target = rewards + gamma * Q_next_clipped
\end{lstlisting}

\textbf{Empirical Evidence:}
\begin{itemize}
    \item CartPole DQN: +12.0\% ($p < 0.001$, 5 seeds)
    \item CartPole DDQN: +33.6\% ($p < 0.001$, 5 seeds)
    \item CartPole Dueling: +22.5\% ($p < 0.001$, 5 seeds)
    \item CartPole Double-Dueling: +15.5\% ($p < 0.01$, 5 seeds)
\end{itemize}

\textbf{Recommendation:} \checkmark \textbf{STRONGLY RECOMMENDED}

\subsubsection{Case 2: Continuous Control with Soft QBound -- SUCCESS}

\textbf{Criteria:}
\begin{itemize}
    \item Continuous action spaces ($\mathcal{A} \subseteq \mathbb{R}^d$)
    \item Actor-critic methods (DDPG, TD3)
    \item Rewards can be positive or negative
\end{itemize}

\textbf{Why Soft QBound Works:}
\begin{enumerate}
    \item \textbf{Stabilization, not bounding:} Soft QBound provides critic stabilization through smooth clipping
    \item \textbf{Gradient preservation:} Softplus maintains gradients even at boundaries: $\text{softplus}(x) = \log(1 + e^x)$
    \item \textbf{Variance reduction:} 51-87\% std reduction observed empirically
    \item \textbf{Complementary to target networks:} Can partially replace or augment target network stabilization
\end{enumerate}

\textbf{Implementation:}
\begin{lstlisting}[language=Python]
# Soft QBound for continuous control
def softplus_clip(Q, Q_min, Q_max, beta=1.0):
    """Smooth, differentiable clipping"""
    Q_upper = Q_max - F.softplus(Q_max - Q, beta=beta)
    Q_clipped = Q_min + F.softplus(Q_upper - Q_min, beta=beta)
    return Q_clipped

# During critic update:
Q_next = target_critic(next_states, target_actor(next_states))
Q_next_soft = softplus_clip(Q_next, Q_min, Q_max)
Q_target = rewards + gamma * Q_next_soft
\end{lstlisting}

\textbf{Empirical Evidence:}
\begin{itemize}
    \item Pendulum DDPG: +25.0\%, std reduced 87\% (89.26 → 11.66)
    \item Pendulum TD3: +15.3\%, std reduced 51\% (71.92 → 34.90)
\end{itemize}

\textbf{Critical Note:} Use \textit{Soft QBound}, NOT Hard QBound. Hard clipping disrupts smooth critic gradients required for actor learning.

\textbf{Recommendation:} \checkmark \textbf{RECOMMENDED for DDPG/TD3}

\subsubsection{Case 3: Negative Dense Rewards -- FAILURE}

\textbf{Criteria:}
\begin{itemize}
    \item Rewards are negative ($r \leq 0$) and accumulate over time
    \item Examples: Pendulum DQN ($r \in [-16, 0]$), MountainCar ($r = -1$), cost minimization tasks
\end{itemize}

\textbf{Why QBound Fails:}
\begin{enumerate}
    \item \textbf{Upper bound naturally satisfied:} Bellman equation with $r \leq 0$ implies $Q(s,a) \leq 0$ (Theorem~\ref{thm:negative-reward-implicit-bound})
    \item \textbf{Empirical verification:} 0.0000 violations of $Q > 0$ observed across 250,000+ updates
    \item \textbf{Lower bound irrelevant:} RL is reward \textit{maximization}—lower bound does not constrain optimization objective
    \item \textbf{Redundant constraint causes interference:} Explicit clipping adds no information but may disrupt learning dynamics
\end{enumerate}

\textbf{Theoretical Explanation:}

For negative rewards, the Bellman recursion naturally constrains Q-values:
$$Q(s,a) = \underbrace{r}_{\leq 0} + \gamma \underbrace{\max_{a'} Q(s',a')}_{\leq 0 \text{ (inductive)}} \leq 0$$

The network learns this pattern through gradient descent on targets that are always $\leq 0$:
\begin{align*}
\mathcal{L} &= (Q_\theta(s,a) - Q_{\text{target}})^2 \\
Q_{\text{target}} &= r + \gamma \max Q_{\bar{\theta}}(s',a') \leq 0
\end{align*}

After 100,000+ updates with negative targets, the network learns: ``Q-values in this environment are negative.''

\textbf{Empirical Evidence:}
\begin{itemize}
    \item Pendulum DQN: -7.0\% degradation (Baseline: $-156.25$, QBound: $-167.19$)
    \item Pendulum DDQN: -3.3\% degradation
    \item MountainCar DQN: -8.2\% degradation
    \item MountainCar DDQN: \textbf{-47.4\% degradation} (severe failure)
    \item Acrobot DQN/DDQN: -3.6\% to -4.9\% degradation
\end{itemize}

\textbf{Recommendation:} $\times$ \textbf{DO NOT USE} -- Use alternatives (Double DQN, gradient clipping, Huber loss)

\subsubsection{Case 4: On-Policy Methods (PPO) -- FAILURE}

\textbf{Criteria:}
\begin{itemize}
    \item On-policy algorithms (PPO, A2C, REINFORCE)
    \item Policy gradient-based optimization
\end{itemize}

\textbf{Why QBound Fails for PPO:}
\begin{enumerate}
    \item \textbf{On-policy sampling reduces overestimation:} PPO updates with recent on-policy data, not arbitrary past experiences
    \item \textbf{Relative advantage estimates:} $A(s,a) = Q(s,a) - V(s)$ are less sensitive to absolute value errors
    \item \textbf{Built-in value clipping:} PPO already includes clipped value loss:
    $$\mathcal{L}_V = \max\left[(V - V_{\text{target}})^2, (\text{clip}(V, V_{\text{old}} \pm \epsilon) - V_{\text{target}})^2\right]$$
    \item \textbf{Interference with policy-value interaction:} Adding explicit QBound disrupts PPO's carefully tuned optimization
\end{enumerate}

\textbf{Theoretical Justification:}

Off-policy methods (DQN) suffer from overestimation because bootstrap targets use \textit{maximization over arbitrary past experiences}:
$$Q_{\text{target}}^{\text{off}} = r + \gamma \max_{a'} Q(s',a') \quad \text{where } (s',a') \text{ sampled from replay buffer}$$

On-policy methods (PPO) bootstrap from \textit{current policy samples}:
$$V_{\text{target}}^{\text{on}} = r + \gamma V(s') \quad \text{where } s' \sim \pi_{\text{current}}$$

The current policy distribution $\pi_{\text{current}}$ provides a natural regularization that reduces overestimation.

\textbf{Empirical Evidence:}
\begin{itemize}
    \item Pendulum PPO: -20.4\% degradation (Baseline: $-784.96$, QBound: $-945.09$)
\end{itemize}

\textbf{Recommendation:} $\times$ \textbf{DO NOT USE} -- PPO's built-in mechanisms sufficient

\subsubsection{Case 5: Sparse Terminal Rewards -- NO EFFECT}

\textbf{Criteria:}
\begin{itemize}
    \item Reward only at episode end (GridWorld, FrozenLake)
    \item $r = 0$ for all intermediate steps, $r = 1$ at goal
\end{itemize}

\textbf{Why QBound Has No Effect:}
\begin{enumerate}
    \item \textbf{Trivial bounds:} $Q \in [0, 1]$ for reach-once tasks
    \item \textbf{No accumulation:} Rewards don't compound, so overestimation less problematic
    \item \textbf{Exploration-limited:} Performance bottleneck is exploration, not value accuracy
\end{enumerate}

\textbf{Empirical Evidence:}
\begin{itemize}
    \item GridWorld: -1.0\% (not significant)
    \item FrozenLake: -1.7\% (not significant)
\end{itemize}

\textbf{Recommendation:} \textcolor{gray}{$\sim$} \textbf{NEUTRAL} -- No benefit, minimal harm

\subsubsection{Summary Table: When to Use QBound}

\begin{table}[h]
\centering
\caption{QBound Applicability Guide}
\label{tab:qbound-applicability}
\begin{tabular}{lccccc}
\toprule
Scenario & Reward Sign & QBound Type & Expected Gain & Use? & Example \\
\midrule
Positive Dense & $r > 0$ & Hard & +12-34\% & \checkmark & CartPole \\
Continuous Control & Any & Soft & +15-25\% & \checkmark & DDPG/TD3 \\
Negative Dense & $r < 0$ & --- & -3 to -47\% & $\times$ & Pendulum DQN \\
On-Policy & Any & --- & -20\% & $\times$ & PPO \\
Sparse Terminal & $r = 0/1$ & Hard & $\approx 0\%$ & $\sim$ & GridWorld \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Practical Recommendations}

\textbf{Before using QBound, check:}
\begin{enumerate}
    \item \textbf{Reward sign:} Is $r > 0$ consistently? If yes, QBound likely helps. If $r \leq 0$, DON'T use QBound.
    \item \textbf{Reward density:} Are rewards per-step or terminal? Dense rewards benefit more.
    \item \textbf{Action space:} Discrete → Hard QBound, Continuous → Soft QBound.
    \item \textbf{Algorithm type:} Off-policy (DQN, DDPG) → potential benefit. On-policy (PPO) → likely harm.
\end{enumerate}

\textbf{Implementation checklist:}
\begin{itemize}
    \item[$\square$] Verify theoretical $Q_{\max}$ derivation (geometric series for dense rewards)
    \item[$\square$] Use Hard QBound for DQN/DDQN with positive rewards
    \item[$\square$] Use Soft QBound for DDPG/TD3 (preserves gradients)
    \item[$\square$] Never use QBound for negative reward DQN or PPO
    \item[$\square$] Validate with 3+ seeds for statistical significance
\end{itemize}

\textbf{If QBound doesn't help, consider alternatives:}
\begin{itemize}
    \item \textbf{Double DQN:} Reduces overestimation through decoupled action selection/evaluation
    \item \textbf{Gradient clipping:} Stabilizes training without constraining values
    \item \textbf{Huber loss:} Robust to outliers, better for negative rewards
    \item \textbf{Target networks:} Already provides stabilization (QBound can augment, not replace)
\end{itemize}
