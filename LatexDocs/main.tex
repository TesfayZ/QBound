\documentclass[11pt]{article}

\usepackage{arxiv}
\usepackage{natbib}  % For \citep{} and \citet{} commands
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}

% Theorem environments
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\argmax}{\operatorname*{argmax}}
\newcommand{\argmin}{\operatorname*{argmin}}
\DeclareMathOperator{\clip}{clip}

% Keywords command
\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}

% Code listings setup
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    language=Python
}

\title{QBound: Environment-Aware Q-Value Bounds for Stable Temporal Difference Learning}

\author{
  Tesfay \\
  \texttt{tzemuy13@gmail.com} \\[0.5em]
  \textit{Working Draft -- Contributions Welcome} \\
  \url{https://github.com/tzemuy13/QBound}
}

\begin{document}

\maketitle

\begin{abstract}
We present QBound, a stabilization mechanism for value-based reinforcement learning that derives and enforces Q-value bounds from known reward structures. \textbf{Critical finding: QBound's effectiveness fundamentally depends on reward sign.} Comprehensive evaluation across 8 environments with 5 seeds (40 runs) reveals that QBound improves performance only for positive dense rewards, while consistently degrading performance for negative rewards. This reward-sign dependence is the paper's central empirical finding and limits QBound's applicability.

QBound addresses overestimation bias \citep{thrun1993issues, van2016deep}---where bootstrapped Q-value estimates systematically exceed true values---by clipping next-state Q-values to environment-specific bounds $[Q_{\min}, Q_{\max}]$, which propagate naturally through temporal difference learning.

\textbf{Scope:} QBound targets \textit{off-policy} value-based methods (DQN, DDQN, Dueling DQN) and off-policy actor-critic methods (DDPG, TD3) that use experience replay. On-policy methods (e.g., PPO, A2C, REINFORCE) are outside QBound's scope because they do not suffer from the same overestimation dynamics.

\textbf{Results by reward sign:} For positive dense reward environments (e.g., CartPole: $r = +1$ per timestep), QBound achieves mean improvements of 12\% to 34\% across DQN variants, but per-seed analysis reveals inconsistency: CartPole Dueling shows 100\% win rate (5/5 seeds improve), while CartPole DQN shows only 80\% win rate (4/5 seeds improve, with one seed degrading by 8.7\%). Neural networks with linear output layers have no architectural constraint on positive values, making explicit upper bounds beneficial on average.

For negative reward environments (Pendulum: $r \in [-16, 0]$), we tested architectural QBound (output activation: $Q = -\text{softplus}(\text{logits})$) across off-policy algorithms. QBound degrades performance for most algorithms: architectural QBound degrades DQN by 3.3\%, Double DQN by 7.1\%, and DDPG by 8.0\%. TD3 is an exception, showing 4.1\% improvement with architectural QBound ($-183.25 \rightarrow -175.66$) but with 72\% higher variance ($\pm 40.15$ vs $\pm 23.36$), suggesting a unique interaction with TD3's twin critic architecture.

The fundamental question of why QBound works for positive but not negative rewards remains open. Both $Q_{\max}$ (positive rewards) and $Q_{\max}=0$ (negative rewards) are theoretical upper bounds derived from cumulative discounted rewards. We propose several hypotheses for future investigation: (1) network initialization bias interacts differently with positive vs negative targets, (2) gradient flow patterns differ across value ranges, (3) replay buffer composition effects, (4) exploration strategy interactions. Answering this question requires controlled ablation studies beyond the scope of this work.

This work provides: (1) systematic per-seed analysis across 40 runs showing QBound's effectiveness varies by algorithm (100\% win rate for Dueling DQN, 80\% for standard DQN), (2) comprehensive negative results for negative rewards (40-60\% win rates, no better than chance), (3) honest reporting of seed-dependent failures even in successful cases, (4) clear recommendations based on win rates rather than mean improvements alone. Implementation imposes negligible overhead ($<$2\%). All experiments use 5 seeds with full reproducibility protocols.

\textbf{Recommendations:} (1) Use hard clipping QBound for positive dense rewards with Dueling DQN (100\% win rate, +22.5\% mean improvement). (2) For standard DQN on positive rewards, QBound helps on average (+12\%) but expect 20\% of seeds to degrade. (3) Do not use QBound for negative rewards---win rates are 40-60\% (no better than chance) with high variance. (4) QBound is designed for off-policy methods only; on-policy methods are outside scope. QBound is a specialized technique for positive dense rewards, not a universal improvement, and practitioners should run multiple seeds before deployment.
\end{abstract}

\keywords{Reinforcement Learning \and Temporal Difference Learning \and Q-Learning \and Overestimation Bias \and Value Stability \and Sample Efficiency}

\section{Introduction}

\subsection{Motivation: Instability in Value Learning Limits Sample Efficiency}

Reinforcement learning has achieved remarkable successes in games \citep{mnih2015human}, robotics \citep{levine2016end}, and complex decision-making tasks \citep{vinyals2019grandmaster}. However, a critical bottleneck remains: \textbf{sample efficiency}—the number of environment interactions required to learn effective policies. In many real-world applications, environment samples are the limiting resource:

\begin{itemize}
    \item \textbf{Robotics:} Physical interactions cost time, energy, and risk hardware damage \citep{kalashnikov2018qt}
    \item \textbf{Clinical trials:} Patient interactions are limited by enrollment, ethics, and cost \citep{dulac2019challenges}
    \item \textbf{Financial trading:} Historical data is finite, live testing is risky
    \item \textbf{Industrial control:} Plant operations are expensive and safety-critical \citep{dulac2019challenges}
    \item \textbf{Autonomous vehicles:} Real-world testing is dangerous and expensive
    \item \textbf{Game design:} Human playtesting is time-consuming and costly
\end{itemize}

Current deep RL methods vary dramatically in sample efficiency \citep{duan2016benchmarking, SpinningUp2018}. Pure policy gradient methods like REINFORCE \citep{williams1992simple} require 50M-100M+ environment steps due to high variance gradient estimates \citep{schulman2015trust}. Actor-critic methods like DDPG \citep{lillicrap2015continuous}, TD3 \citep{fujimoto2018addressing}, and SAC \citep{haarnoja2018soft} achieve 2M-20M steps by combining policy gradients with value function learning \citep{haarnoja2018soft2}. Pure value-based methods like DQN \citep{mnih2015human} and its variants achieve the highest sample efficiency at 1M-10M steps through bootstrap learning with experience replay \citep{lin1992self, mnih2015human}.

The sample efficiency hierarchy correlates directly with whether methods learn value functions. This suggests that improving value function learning improves sample efficiency across the entire spectrum of methods that use critics. However, value-based methods face a fundamental challenge: bootstrapped Q-value estimates suffer from instability and overestimation bias \citep{thrun1993issues, van2016deep}, which directly undermines their sample efficiency advantage. Stabilizing value learning is therefore essential for achieving better sample efficiency.

\subsection{The Bootstrapping Instability Problem}

All methods that learn value functions face a fundamental challenge: bootstrapping with imperfect function approximators produces unbounded, inconsistent value estimates \citep{tsitsiklis1997analysis}. During training, Q-values frequently:
\begin{enumerate}
    \item Diverge to arbitrary magnitudes ($Q(s,a) \to \pm\infty$)
    \item Violate theoretical constraints (e.g., $Q(s,a) > Q_{\max}$ when $Q_{\max}$ is derivable from environment structure)
    \item Exhibit high variance in bootstrap targets, leading to unstable learning
    \item Create poorly scaled gradient signals that slow convergence
\end{enumerate}

Prior stabilization work includes target networks \citep{mnih2015human}, clipped double-Q \citep{fujimoto2018addressing}, reward clipping \citep{mnih2013playing}, and gradient clipping \citep{pascanu2013difficulty}. However, these approaches do not directly enforce theoretically-derived bounds based on environment structure.

\subsection{Our Approach: QBound}

We propose QBound, a stabilization mechanism that prevents overestimation bias by exploiting known environment structure. QBound addresses the root cause of instability in bootstrapped value learning—unbounded overestimation—by deriving and enforcing environment-aware Q-value bounds. Unlike generic pessimism (e.g., Double-Q \citep{van2016deep}), QBound's bounds are environment-specific, providing stabilization without excessive conservatism.

\textbf{Important scope limitation:} QBound is a \textit{specialized technique}, not a universal improvement. Our comprehensive evaluation reveals that QBound provides consistent benefits \textit{only} for positive dense reward environments (+12-34\% on CartPole). For negative reward environments, QBound causes degradation (-3\% to -8\%). For sparse terminal rewards, QBound provides no meaningful benefit. Practitioners should carefully assess reward structure before applying QBound.

\textbf{Core Mechanism:}
\begin{itemize}
    \item Derive tight bounds $[Q_{\min}, Q_{\max}]$ from environment reward structure
    \item Clip next-state Q-values during bootstrapping: $Q_{\text{next}} \gets \clip(Q_{\text{next}}, Q_{\min}, Q_{\max})$
    \item Compute bounded targets: $Q_{\text{target}} = r + \gamma \cdot Q_{\text{next}}^{\text{clipped}}$
    \item Standard TD loss propagates bounds through the network naturally
\end{itemize}

\textbf{Key Insight:} Clipping next-state Q-values in the bootstrap target—$y = r + \gamma \cdot \clip(Q(s',a'), Q_{\min}, Q_{\max})$—naturally propagates bounds through temporal difference learning \citep{sutton2018reinforcement}. Since today's $Q(s,a)$ is trained toward this clipped target, it becomes tomorrow's $Q(s',a')$ for other states, creating a self-reinforcing bounded value propagation through the Bellman backup chain.

\textbf{Key Benefits:}
\begin{itemize}
    \item \textbf{Primary:} Prevents overestimation bias, stabilizing temporal difference learning \citep{sutton2018reinforcement}
    \item \textbf{Outcome:} 12-34\% improvement on positive dense reward environments (CartPole) with 5-seed validation; no benefit or degradation on negative/sparse reward environments
    \item Reduces variance in bootstrapped targets, particularly during early training
    \item Negligible computational overhead ($<2\%$)
    \item Works with off-policy algorithms that learn Q-functions (DQN, DDQN, Dueling DQN, DDPG, TD3)
\end{itemize}

\textbf{Target Applications:} QBound is most effective for positive dense reward environments (e.g., CartPole with $r=+1$ per timestep). For such environments, explicit upper bounds prevent unbounded Q-value growth. QBound is not recommended for negative reward or sparse terminal reward environments where it provides no benefit or causes degradation.

\subsection{QBound Variants: A Taxonomy}

We implement three QBound variants depending on action space and reward sign. Table~\ref{tab:qbound-taxonomy} summarizes when to use each.

\begin{table}[h]
\centering
\caption{QBound Implementation Variants}
\label{tab:qbound-taxonomy}
\small
\begin{tabular}{p{2.5cm}p{3.5cm}p{4cm}p{3cm}}
\toprule
\textbf{Variant} & \textbf{Mechanism} & \textbf{Use Case} & \textbf{Algorithms} \\
\midrule
\textbf{Hard QBound} & Direct clipping: $Q \gets \clip(Q, Q_{\min}, Q_{\max})$ & Discrete actions, positive rewards & DQN, DDQN, Dueling \\
\textbf{Soft QBound} & Penalty loss for violations (preserves gradients) & Continuous actions (theoretical; not evaluated) & DDPG, TD3, SAC \\
\textbf{Architectural} & Output activation: $Q = -\text{softplus}(\cdot)$ enforces $Q \leq 0$ & Negative rewards (empirically fails) & Any Q-learning \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key distinction:} Hard and Soft QBound enforce bounds \textit{algorithmically} (post-hoc on network outputs), while Architectural QBound enforces bounds \textit{structurally} through network architecture. Our experiments reveal that Hard QBound succeeds for positive rewards (+12-34\%), but Architectural QBound largely fails for negative rewards despite theoretical motivation (Sec.~3.4.3).

\section{Related Work}

\subsection{Value-Based Reinforcement Learning}

\textbf{Q-learning} \citep{watkins1992q} learns action-value functions through temporal difference bootstrapping, with convergence guarantees proven for tabular settings \citep{jaakkola1994convergence, melo2001convergence}. The foundational analysis by \citet{watkins1989learning} established the theoretical framework that underlies modern value-based methods.

\textbf{Deep Q-Networks (DQN)} \citep{mnih2013playing, mnih2015human} revolutionized RL by combining Q-learning with deep neural networks, experience replay \citep{lin1992self}, and target networks. \textbf{Double Q-Learning} \citep{van2016deep} addresses overestimation bias but does not bound absolute value magnitudes. Recent advances include dueling architectures \citep{wang2016dueling}, distributional methods \citep{bellemare2017distributional, dabney2018implicit}, and Rainbow combinations \citep{hessel2018rainbow}.

\subsection{Actor-Critic Methods}

\textbf{Actor-critic methods} \citep{konda2000actor} combine policy gradients \citep{sutton2000policy} with value function learning. Classical methods include A2C/A3C \citep{mnih2016asynchronous} for discrete control. For continuous control, \textbf{DDPG} \citep{lillicrap2015continuous} pioneered deterministic policy gradients, while \textbf{TD3} \citep{fujimoto2018addressing} added clipped double-Q estimation and delayed policy updates. \textbf{SAC} \citep{haarnoja2018soft, haarnoja2018soft2} maximizes entropy-augmented objectives for improved exploration. Trust region methods like TRPO \citep{schulman2015trust} and PPO \citep{schulman2017proximal} provide stable policy updates.

\subsection{Sample Efficiency and Experience Replay}

Experience replay \citep{lin1992self} dramatically improves sample efficiency by reusing transitions. \textbf{Prioritized experience replay} \citep{schaul2015prioritized} focuses on important transitions, while \textbf{hindsight experience replay} \citep{andrychowicz2017hindsight} creates synthetic successes for sparse reward environments. Recent work \citep{fedus2020revisiting} revisits replay fundamentals, showing that simple improvements can be highly effective.

\subsection{Stabilization and Optimization}

Deep RL stability has been improved through various techniques: target networks \citep{mnih2015human}, gradient clipping \citep{pascanu2013difficulty}, batch normalization \citep{ioffe2015batch}, and optimizers like Adam \citep{kingma2014adam}. \citet{henderson2018deep} highlighted reproducibility issues and the importance of proper baselines, while theoretical work \citep{szepesvari2010algorithms} provides PAC-MDP analysis for tabular settings.

\subsection{Activation Functions in Value Networks}

The choice of output activation functions for value networks is rarely discussed in RL literature, with most work using linear (identity) output layers by default. However, recent work has begun exploring architectural constraints:

\textbf{Bounded activation functions:} Some early DQN implementations experimented with sigmoid or tanh output activations to bound Q-values \citep{mnih2013playing}, but this was abandoned in favor of linear outputs in DQN \citep{mnih2015human} due to representational limitations. The consensus has been that linear outputs provide the most flexibility.

\textbf{Transformed Bellman operators:} \citet{pohlen2018observe} introduced a transformed Bellman operator that addresses varying Q-value scales across Atari games through reward transformation, enabling stable training with high discount factors ($\gamma = 0.999$). Their approach bounds Q-values implicitly through invertible transformations rather than explicit architectural constraints. While effective for scale normalization, their method does not leverage environment-specific reward bounds or address reward-sign-dependent implementation strategies. Our work complements this by showing that explicit bounds derived from reward structure can provide additional benefits, particularly for known reward ranges.

\textbf{Architectural inductive biases:} Recent work in supervised learning has emphasized the importance of architectural inductive biases matching problem structure \citep{battaglia2018relational}. \citet{goyal2021inductive} showed that architectural choices encoding prior knowledge outperform learned representations in many domains. However, this principle has seen limited application to value network design in RL.

\textbf{Initialization bias and exploration:} \citet{kumar2020implicit} showed that network initialization creates implicit regularization in RL, affecting the learned Q-function. \citet{fortunato2017noisy} demonstrated that proper initialization and exploration mechanisms significantly impact learning dynamics. Our work extends this by showing that \textit{output activation functions} can serve as architectural inductive biases that guide exploration space from initialization.

\textbf{Value clipping vs. architectural constraints:} Most RL algorithms apply value clipping \textit{algorithmically} (e.g., PPO's value function clipping \citep{schulman2017proximal}, SAC's target entropy \citep{haarnoja2018soft}). However, these operate post-hoc on network outputs. Our work demonstrates that \textit{architectural enforcement} through output activations can be more effective than algorithmic clipping for certain reward structures.

\subsection{Recent Work on Value Bounding and Constraints}

Recent work has explored constraining Q-values in various contexts. \citet{liu2024boosting} proposed bounding techniques for soft Q-learning in offline settings, demonstrating improved stability through bounded value function approximation. \citet{adamczyk2023bounding} derived theoretical bounds for compositional RL, providing double-sided inequalities relating optimal composite value functions to primitive task values.

\textbf{Adaptive pessimism:} \citet{wang2024adaptive} introduced APTQ (Adaptive Pessimism via Target Q-value), which balances pessimism constraints with RL objectives by adaptively weighting conservatism based on task-specific Q-value distributions. Their key insight—that optimal constraint strength varies across tasks—parallels our finding that optimal \textit{implementation} (hard clipping vs. architectural constraints) varies across reward structures. However, their approach targets offline RL with out-of-distribution action mitigation, while QBound addresses online learning with environment-derived bounds. Both works highlight that Q-value constraints should be task-aware rather than fixed.

Work on overestimation bias has also progressed. \citet{elasticdqn2023} proposed Elastic Step DQN, which alleviates overestimation by dynamically varying multi-step horizons based on state similarity. \citet{twosample2024bias} addressed maximization bias through two-sample testing, framing the problem statistically and proposing the T-Estimator that flexibly interpolates between over- and underestimation. \citet{imagination2025limited} introduced Imagination-Limited Q-Learning, which envisions reasonable values and appropriately limits potential over-estimations using maximum behavior values.

For sparse reward environments, \citet{efficient2024sparse} achieved 2$\times$ better sample efficiency through high replay ratio methods with regularization, while \citet{llmreward2024shaping} demonstrated significant improvements by extracting heuristics from large language models for reward shaping.

\subsection{Related Bias Mitigation Methods}

Several recent methods address Q-value overestimation through alternative mechanisms. We discuss their relationship to QBound and acknowledge that direct empirical comparisons remain future work.

\textbf{Q-Clip in Discrete SAC (SD-SAC):} \citet{zhou2022sd} introduced Q-clip for discrete Soft Actor-Critic, explicitly bounding critic updates to stabilize training. Unlike QBound's environment-derived bounds, Q-clip uses fixed or learned bounds without leveraging reward structure. A head-to-head comparison on shared discrete benchmarks would clarify whether environment-aware bounds provide additional benefits over generic Q-clip.

\textbf{Doubly Bounded Estimators (DB-ADP):} \citet{ren2022doubly} employ both upper and lower bounds on Q-targets via a conservative dynamic programming estimator, demonstrating robust gains on Atari. DB-ADP provides complementary lower-bound enforcement that QBound does not address. Combining QBound's environment-aware upper bounds with DB-ADP's lower bounds is a promising direction.

\textbf{Distributional Ensemble Methods (CTD4):} \citet{wu2023ctd4} use distributional ensembles with variance-aware Kalman-style fusion to reduce overestimation. This approach is orthogonal to QBound: distributional methods model return uncertainty, while QBound enforces hard constraints from environment structure. Combining both could provide complementary benefits.

\textbf{Adaptive Bias Control (BE-TD3):} \citet{lee2022betd3} adaptively select between min and max operators based on estimated bias direction. This online adaptation contrasts with QBound's static, environment-derived bounds. Whether QBound plus adaptive bias control achieves additive benefits is an open question.

\textbf{Relationship to QBound:} These methods differ from QBound in key aspects: (1) they typically use learned or fixed bounds rather than environment-derived bounds; (2) they focus on the bias-variance tradeoff rather than theoretical constraint satisfaction; (3) they do not explicitly leverage reward structure information. QBound's contribution is showing that environment-aware bounds can provide benefits for specific reward structures (positive dense rewards), though our empirical evaluation is limited to simpler benchmarks. Direct comparisons on standard benchmarks (Atari, DMControl) remain important future work.

\subsection{Positioning of QBound}

QBound differs from prior work in several key aspects:
\begin{enumerate}
    \item \textbf{Environment-aware bounds:} Unlike generic stabilization techniques or learned bounds, QBound derives bounds directly from environment reward structure, ensuring theoretical correctness
    \item \textbf{Reward-sign analysis:} We investigate how QBound effectiveness depends on reward sign. Hard clipping succeeds for positive rewards (+12-34\%), but architectural constraints for negative rewards largely fail despite theoretical motivation. While prior work has explored Q-value constraints \citep{pohlen2018observe, wang2024adaptive}, we provide systematic analysis revealing this fundamental limitation
    \item \textbf{Architectural inductive bias investigation:} We investigate whether output activation functions ($Q = -\text{softplus}(\text{logits})$) can enforce value bounds for negative rewards. Despite theoretical motivation, our experiments show this approach largely fails, providing important negative results for the community
    \item \textbf{Bootstrapping-based enforcement:} QBound leverages the natural propagation of bootstrapped targets through temporal difference learning \citep{sutton2018reinforcement}, requiring only simple operations (clipping or activation)
    \item \textbf{Online learning focus:} Unlike recent work on offline RL \citep{wang2024adaptive, imagination2025limited}, QBound targets online learning in both sparse and dense reward settings
    \item \textbf{Comprehensive evaluation:} We provide extensive analysis across 8 environments with 5 seeds each (40 runs), including honest reporting of failure modes and implementation-dependent effectiveness
\end{enumerate}

\textbf{Findings on activation functions:} Our experiments reveal that architectural constraints ($Q = -\text{softplus}(\cdot)$) for negative rewards largely fail despite theoretical motivation, while hard clipping succeeds for positive rewards. This asymmetry provides practical guidance: QBound is effective for positive dense rewards but should not be applied to negative reward environments.

\section{Theoretical Foundations}

\subsection{Preliminaries and Notation}

\begin{definition}[Markov Decision Process]
A Markov Decision Process is a tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, r, \gamma)$ where:
\begin{itemize}
    \item $\mathcal{S}$: State space (finite or continuous)
    \item $\mathcal{A}$: Action space (discrete: $\mathcal{A} = \{a_1, \ldots, a_{|\mathcal{A}|}\}$ or continuous: $\mathcal{A} \subseteq \mathbb{R}^d$)
    \item $P(s'|s,a)$: Transition dynamics
    \item $r(s,a,s') \in \mathbb{R}$: Reward function
    \item $\gamma \in [0,1)$: Discount factor
\end{itemize}
\end{definition}

\begin{definition}[Value Functions]
For policy $\pi: \mathcal{S} \to \Delta(\mathcal{A})$ (stochastic) or $\mu: \mathcal{S} \to \mathcal{A}$ (deterministic):
\begin{align}
V^\pi(s) &= \E_\pi\left[\sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s\right] \\
Q^\pi(s,a) &= \E_\pi\left[\sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a\right]
\end{align}
\end{definition}

\begin{definition}[Optimal Value Functions]
\begin{align}
Q^*(s,a) &= \max_\pi Q^\pi(s,a) \\
V^*(s) &= \max_a Q^*(s,a)
\end{align}
\end{definition}

The Bellman optimality equation \citep{bellman1957markovian, sutton2018reinforcement} provides the foundation for Q-learning:
$$Q^*(s,a) = \E_{s' \sim P(\cdot|s,a)}\left[r(s,a,s') + \gamma \max_{a'} Q^*(s',a')\right]$$

\begin{assumption}[Bounded Rewards]
We assume that worst-case and best-case cumulative returns over all possible trajectories are finite and can be computed or bounded. This is satisfied by most practical environments.
\end{assumption}

\subsection{Environment-Specific Q-Value Bounds}

The key theoretical contribution is deriving tight bounds $[Q_{\min}, Q_{\max}]$ such that all possible Q-values lie within this range.

\begin{definition}[Trajectory]
A trajectory $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots)$ is a sequence of states, actions, and rewards following dynamics $P$ and policy $\pi$.
\end{definition}

\begin{definition}[Trajectory Return]
For finite horizon $H$ or until termination:
$$G(\tau) = \sum_{t=0}^{H-1} \gamma^t r_t$$
\end{definition}

\begin{definition}[Environment-Specific Bounds]
\begin{align}
Q_{\min} &= \inf_{\pi \in \Pi, s \in \mathcal{S}, a \in \mathcal{A}} Q^\pi(s,a) = \inf_{\tau \in \mathcal{T}(s,a)} G(\tau) \\
Q_{\max} &= \sup_{\pi \in \Pi, s \in \mathcal{S}, a \in \mathcal{A}} Q^\pi(s,a) = \sup_{\tau \in \mathcal{T}(s,a)} G(\tau)
\end{align}
where $\mathcal{T}(s,a)$ is the set of all trajectories starting with $(s,a)$.
\end{definition}

\begin{theorem}[Bound Correctness]
\label{thm:bound-correctness}
If $Q_{\min}$ and $Q_{\max}$ are computed according to the above definition, then:
$$Q^*(s,a) \in [Q_{\min}, Q_{\max}] \quad \forall s,a$$
\end{theorem}

\begin{proof}
Follows directly from definition: $Q^*(s,a) = \max_\pi Q^\pi(s,a) \leq \sup_\pi Q^\pi(s,a) = Q_{\max}$, and similarly $Q^* \geq Q_{\min}$.
\end{proof}

\begin{corollary}
Clipping Q-values to $[Q_{\min}, Q_{\max}]$ cannot remove the optimal value $Q^*$.
\end{corollary}

\subsection{Analysis of the Clipped Bellman Operator}
\label{sec:clipped-bellman-analysis}

A key theoretical question is whether the clipped target operator preserves the contraction properties essential for convergence. We analyze this formally.

\begin{definition}[Clipped Bellman Operator]
Define the clipped Bellman optimality operator $\mathcal{T}_{\text{clip}}: \mathbb{R}^{|\mathcal{S}||\mathcal{A}|} \to \mathbb{R}^{|\mathcal{S}||\mathcal{A}|}$ as:
$$(\mathcal{T}_{\text{clip}} Q)(s,a) = \E_{s' \sim P(\cdot|s,a)}\left[r(s,a,s') + \gamma \cdot \clip\left(\max_{a'} Q(s',a'), Q_{\min}, Q_{\max}\right)\right]$$
\end{definition}

\begin{theorem}[Contraction Property of Clipped Operator]
\label{thm:clipped-contraction}
The clipped Bellman operator $\mathcal{T}_{\text{clip}}$ is a $\gamma$-contraction in the $\ell_\infty$ norm:
$$\|\mathcal{T}_{\text{clip}} Q_1 - \mathcal{T}_{\text{clip}} Q_2\|_\infty \leq \gamma \|Q_1 - Q_2\|_\infty$$
\end{theorem}

\begin{proof}
For any $s, a$:
\begin{align*}
|(\mathcal{T}_{\text{clip}} Q_1)(s,a) - (\mathcal{T}_{\text{clip}} Q_2)(s,a)| &= \gamma \left|\E_{s'}\left[\clip(\max_{a'} Q_1(s',a')) - \clip(\max_{a'} Q_2(s',a'))\right]\right|
\end{align*}
Since $\clip(\cdot, Q_{\min}, Q_{\max})$ is a non-expansive mapping (i.e., $|\clip(x) - \clip(y)| \leq |x - y|$), and $\max$ over a finite set is also non-expansive:
$$|\clip(\max_{a'} Q_1(s',a')) - \clip(\max_{a'} Q_2(s',a'))| \leq |\max_{a'} Q_1(s',a') - \max_{a'} Q_2(s',a')| \leq \|Q_1 - Q_2\|_\infty$$
Therefore:
$$|(\mathcal{T}_{\text{clip}} Q_1)(s,a) - (\mathcal{T}_{\text{clip}} Q_2)(s,a)| \leq \gamma \|Q_1 - Q_2\|_\infty$$
Taking the supremum over all $(s,a)$ yields the result.
\end{proof}

\begin{theorem}[Fixed Point of Clipped Operator]
\label{thm:clipped-fixed-point}
Let $Q^*$ be the optimal Q-function and $\tilde{Q}^*$ be the fixed point of $\mathcal{T}_{\text{clip}}$. If bounds are correctly specified (i.e., $Q^*(s,a) \in [Q_{\min}, Q_{\max}]$ for all $s,a$), then $\tilde{Q}^* = Q^*$.
\end{theorem}

\begin{proof}
When $Q^*(s,a) \in [Q_{\min}, Q_{\max}]$ for all $(s,a)$, we have $\max_{a'} Q^*(s',a') \in [Q_{\min}, Q_{\max}]$ for all $s'$. Therefore, $\clip(\max_{a'} Q^*(s',a'), Q_{\min}, Q_{\max}) = \max_{a'} Q^*(s',a')$, which implies $\mathcal{T}_{\text{clip}} Q^* = \mathcal{T} Q^* = Q^*$. Since $Q^*$ satisfies the clipped Bellman equation, it is the unique fixed point by the Banach fixed point theorem.
\end{proof}

\begin{remark}[Bias Under Mis-specified Bounds]
If bounds are mis-specified such that $Q^*(s,a) > Q_{\max}$ for some $(s,a)$, then the fixed point $\tilde{Q}^*$ satisfies $\tilde{Q}^*(s,a) \leq Q^*(s,a)$. Specifically, for states where clipping is active, the learned Q-function underestimates the true optimal value. This bias is bounded by:
$$Q^*(s,a) - \tilde{Q}^*(s,a) \leq \frac{\gamma}{1-\gamma}(Q^*_{\max} - Q_{\max})$$
where $Q^*_{\max} = \max_{s,a} Q^*(s,a)$ is the true maximum Q-value. Overly conservative bounds thus introduce pessimistic bias, while correctly specified bounds preserve optimality.
\end{remark}

\subsection{Fundamental Q-Value Bounds for Common Reward Structures}

\subsubsection{Case 1: Sparse Binary Rewards (Primary Use Case)}

\textbf{Environment Structure:} Single reward at episode end, zero otherwise:
$$r(s,a,s') = \begin{cases} 
1 & \text{if } s' \text{ is goal state} \\ 
0 & \text{otherwise}
\end{cases}$$

This is the most common sparse reward structure in robotics, games, and goal-reaching tasks.

\begin{theorem}[Sparse Binary Reward Bounds]
\label{thm:sparse-binary-bounds}
For sparse binary reward environments with discount factor $\gamma$, the bounds depend on episode termination:

\textbf{Case 1a (Reach-Once):} Episode terminates upon reaching goal:
$$Q_{\min} = 0, \quad Q_{\max} = 1$$

\textbf{Case 1b (Stay-at-Goal):} Agent can remain at goal and continue receiving rewards until episode end or indefinitely:
$$Q_{\min} = 0, \quad Q_{\max} = \frac{1}{1-\gamma}$$
\end{theorem}

\begin{proof}
\textbf{Lower bound (both cases):} Since all immediate rewards are non-negative, any trajectory return $G(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t \geq 0$, hence $Q_{\min} = 0$.

\textbf{Upper bound (Case 1a - Reach-Once):} The agent receives reward $r=1$ once when reaching the goal, then the episode terminates:
$$Q_{\max} = 1 \cdot \gamma^0 = 1$$

\textbf{Upper bound (Case 1b - Stay-at-Goal):} The agent receives reward $r=1$ at every timestep after reaching the goal:
$$Q_{\max} = \sum_{t=0}^{\infty} \gamma^t \cdot 1 = \frac{1}{1-\gamma}$$

This bound is achieved when the agent reaches the goal immediately and remains there.
\end{proof}

\begin{example}[Robot Navigation - Reach-Once]
A mobile robot navigating to a goal location where the episode ends upon arrival. With $\gamma = 0.99$:
$$Q_{\min} = 0, \quad Q_{\max} = 1$$

Any Q-value outside $[0, 1]$ is impossible given the reward structure and can be safely clipped. This provides extremely tight bounds.
\end{example}

\begin{example}[Robot Navigation - Stay-at-Goal]
A mobile robot that must reach and \textit{maintain} position at the goal, receiving $r=1$ per timestep while at goal. With $\gamma = 0.99$:
$$Q_{\min} = 0, \quad Q_{\max} = \frac{1}{1-0.99} = 100$$

Any Q-value outside $[0, 100]$ can be safely clipped.
\end{example}

\begin{example}[Game Playing - Reach-Once]
A chess engine with binary win/loss outcomes ($+1$ for win, $0$ for loss/draw) where each game is a single episode. With $\gamma = 0.995$:
$$Q_{\min} = 0, \quad Q_{\max} = 1$$

\textit{Note:} The discount factor here primarily affects temporal credit assignment during the game, but the final outcome is binary, so $Q_{\max} = 1$.
\end{example}

\subsubsection{Case 2: Dense Per-Step Costs with Terminal Reward}

\textbf{Environment Structure:} Negative cost per step, positive reward at goal:
$$r(s,a,s') = \begin{cases} 
R_{\text{goal}} & \text{if } s' \text{ is goal state} \\ 
-c & \text{otherwise}
\end{cases}$$

\begin{theorem}[Cost-Plus-Reward Bounds]
For maximum episode length $H$:
\begin{align}
Q_{\min} &= -cH + \gamma^H R_{\text{goal}} \approx -cH \text{ if } c \gg R_{\text{goal}} \\
Q_{\max} &= R_{\text{goal}}
\end{align}
\end{theorem}

\begin{example}[MountainCar]
With $r = -1$ per step, $r = 0$ at goal, $H = 200$:
$$Q_{\min} = -200, \quad Q_{\max} = 0$$
\end{example}

\subsubsection{Case 3: Dense Positive Rewards (Survival Tasks)}

\textbf{Environment Structure:} Positive reward per step until failure:
$$r(s,a,s') = r_{\text{step}} > 0$$

\begin{theorem}[Survival Task Bounds]
For finite horizon $H$:
\begin{align}
Q_{\min} &= 0 \text{ (immediate failure)} \\
Q_{\max} &= r_{\text{step}} \sum_{k=0}^{H-1} \gamma^k = r_{\text{step}} \frac{1-\gamma^H}{1-\gamma}
\end{align}

For infinite horizon (no termination):
$$Q_{\max} = \frac{r_{\text{step}}}{1-\gamma}$$
\end{theorem}

\begin{example}[CartPole]
With $r = +1$ per step, $\gamma = 0.99$, maximum episode length $H = 500$:
$$Q_{\min} = 0, \quad Q_{\max} = \frac{1-0.99^{500}}{1-0.99} \approx 100$$

\textit{Dynamic Bounds:} For survival tasks with fixed start states (e.g., CartPole), we can use step-aware dynamic bounds: $Q_{\max}(t) = \frac{1-\gamma^{(H-t)}}{1-\gamma}$ at timestep $t$, which provides tighter constraints than the static bound. This accounts for the discounted sum of remaining rewards. This is possible because the remaining episode potential is determined by the timestep, not by state proximity to a goal. For sparse reward tasks (e.g., GridWorld), remaining potential depends on unknown state-to-goal distance, making dynamic bounds infeasible.
\end{example}

\subsection{Critical Insight: Reward Sign Determines QBound Effectiveness}
\label{subsec:reward-sign-dependence}

Our comprehensive empirical evaluation (Section~\ref{sec:experiments}) reveals that QBound's effectiveness fundamentally depends on the \textit{sign} of the reward signal. This section provides theoretical justification.

\subsubsection{The Upper Bound is What Matters for Maximization}

\begin{proposition}[Upper Bound Primacy]
In RL, the agent maximizes $\mathbb{E}[\sum_{t=0}^\infty \gamma^t r_t]$. The upper bound $Q_{\max}$ directly constrains this objective. The lower bound $Q_{\min}$ is largely irrelevant because: (1) the agent seeks to \textit{maximize}, not avoid low values, and (2) overestimation (predicting above $Q_{\max}$) causes suboptimal policies, while underestimation merely slows convergence.
\end{proposition}

\subsubsection{Positive Rewards: QBound Provides Essential Upper Bound}

For environments with positive dense rewards (e.g., CartPole: $r = +1$ per timestep), neural networks with linear output layers have no architectural constraint on the upper bound. Q-values can grow unbounded during training.

\begin{theorem}[Overestimation Vulnerability with Positive Rewards]
For $r_t > 0$, the Bellman equation $Q(s,a) = \mathbb{E}[r + \gamma \max_{a'} Q(s',a')]$ allows unbounded growth. Function approximation errors $\epsilon$ compound through bootstrapping. QBound prevents this by enforcing $Q_{\text{target}} = r + \gamma \cdot \text{clip}(\max_{a'} Q(s',a'), Q_{\min}, Q_{\max})$.
\end{theorem}

\textbf{Example:} CartPole achieves +12\% to +34\% improvement across DQN variants (Section~\ref{sec:cartpole-results}).

\subsubsection{Negative Rewards: Implementation Matters}

For negative rewards (e.g., Pendulum: $r \in [-16, 0]$), there exists a theoretical upper bound on Q-values.

\begin{theorem}[Theoretical Upper Bound for Negative Rewards]
\label{thm:negative-reward-bound}
If $r(s,a,s') \leq 0$ for all transitions, then $Q^\pi(s,a) \leq 0$ for any policy $\pi$.

\textbf{Proof:} By induction: $Q^\pi(s,a) = \mathbb{E}[r + \gamma Q^\pi(s',a')] \leq \mathbb{E}[0 + \gamma Q^\pi(s',a')] = \gamma \mathbb{E}[Q^\pi(s',a')]$. If $Q^\pi(s',a') \leq 0$, then $Q^\pi(s,a) \leq 0$.
\end{theorem}

\textbf{Open question:} While this theoretical bound exists, whether neural network function approximators learn to satisfy it without explicit constraints remains unclear. Our experiments show QBound provides no benefit for negative rewards, but \textit{why} this occurs is left for future investigation (see Section 1 for hypotheses).

\paragraph{Hard Clipping QBound Fails.} Algorithmic clipping ($Q \gets \text{clip}(Q, -\infty, 0)$) interferes with learning due to \textit{exploration-correction conflicts}:
\begin{itemize}
    \item \textbf{Positive initialization bias:} Neural networks with He/Xavier initialization \citep{he2015delving, glorot2010understanding} typically produce positive outputs initially due to random weight distributions
    \item \textbf{Exploration in wrong space:} Network explores unbounded space, frequently violating $Q > 0$ (56.79\% violation rate observed)
    \item \textbf{Post-hoc correction:} Clipping corrects violations after they occur, but network never learns to naturally output $Q \leq 0$
    \item \textbf{Persistent conflict:} Network continues exploring positive regions, clipping continues correcting → instability
    \item \textbf{Empirical result:} Pendulum DQN shows \textbf{-3.3\% degradation} with architectural QBound (5 seeds)
\end{itemize}

\paragraph{Architectural QBound: Theoretical Motivation vs. Empirical Reality.} Output activation function ($Q = -\text{softplus}(\text{logits})$, enforcing $Q \in (-\infty, 0]$) constrains \textit{exploration space}. In principle, this should help:
\begin{itemize}
    \item \textbf{Constrained exploration:} Network explores WITHIN correct range $(-\infty, 0]$ from first forward pass
    \item \textbf{Learning correct magnitude:} Network learns ``how negative'' Q should be, not ``whether it should be negative''
    \item \textbf{Zero violations by construction:} Activation function mathematically enforces $Q \leq 0$, no conflict possible
    \item \textbf{Smooth gradients:} $\frac{\partial Q}{\partial \text{logits}} = -\text{sigmoid}(\text{logits}) \in (-1, 0)$ is never zero
\end{itemize}

\textbf{However, empirical results contradict this reasoning.} On Pendulum (5 seeds), architectural QBound \textit{degrades} performance for most algorithms: DQN by -3.3\%, Double DQN by -7.1\%, and DDPG by -8.0\%. Only TD3 shows improvement (+4.1\%), but with 72\% higher variance. The gap between theoretical motivation and empirical outcomes suggests that additional factors---possibly related to optimization dynamics, initialization interactions, or algorithm-specific properties---dominate the theoretical benefits of constrained exploration.

\textbf{Key insight:} QBound shows asymmetric results between positive and negative rewards. For positive rewards (CartPole), hard clipping provides 12-34\% improvement. For negative rewards (Pendulum), QBound generally degrades performance (-3\% to -8\%). TD3 is an exception showing modest improvement (+4.1\%), but with 72\% higher variance. The fundamental question of \textit{why} this asymmetry exists remains open for future investigation.

\subsubsection{Summary: When QBound Works}

\begin{table}[h]
\centering
\caption{QBound Effectiveness by Reward Type and Implementation (5 seeds)}
\label{tab:reward-sign-summary-theory}
\begin{tabular}{lcccc}
\toprule
Reward Type & Theoretical Bound & Hard Clipping & Architectural & Best Approach \\
\midrule
\textbf{Positive Dense} (CartPole) & Unbounded & +12\% to +34\% & N/A & Hard Clipping \\
\textbf{Negative Dense} (Pendulum DQN) & $Q \leq 0$ & N/A & -3.3\% & Neither \\
\textbf{Negative Dense} (Pendulum DDPG) & $Q \leq 0$ & N/A & -8.0\% & Neither \\
\textbf{Negative Dense} (Pendulum TD3) & $Q \leq 0$ & N/A & +4.1\%* & Architectural* \\
\textbf{Sparse Terminal} (GridWorld) & $Q \leq 1$ & $\approx 0\%$ & N/A & Neither \\
\bottomrule
\end{tabular}
\end{table}
\textit{*TD3 shows improvement but with 72\% higher variance; use with caution.}

\textbf{Recommendation:} Use QBound (hard clipping) for positive dense rewards like CartPole where it provides consistent 12-34\% improvements. Do not use QBound for negative rewards (Pendulum-style environments) as it generally degrades performance. TD3 is an exception that may benefit but with high variance.

\section{QBound Bound Selection Strategy}

This section explains how to derive appropriate Q-value bounds for different environment types, focusing on the theoretical foundations demonstrated in our experimental evaluation.

\subsection{Sparse Binary Reward Environments}

Sparse binary reward environments (e.g., GridWorld, FrozenLake) provide extremely tight bounds since the agent receives reward only at terminal states.

\subsubsection{Example: Navigation Tasks (GridWorld, FrozenLake)}

In our experimental evaluation, we tested GridWorld (deterministic 10×10 navigation) and FrozenLake (stochastic 4×4 navigation with slippery ice).

\textbf{Reward Structure:}
\begin{itemize}
    \item $r = 1$ when agent reaches goal (success)
    \item $r = 0$ for all other states/actions
    \item Episode terminates upon reaching goal (reach-once semantics)
\end{itemize}

\textbf{QBound Bounds:}
$$Q_{\min} = 0, \quad Q_{\max} = 1$$

Since the episode terminates immediately upon success, the maximum return is exactly 1 regardless of discount factor. These extremely tight bounds prevent Q-value explosions common in sparse reward exploration.

\textbf{Results:} GridWorld achieved 20.2\% faster convergence; FrozenLake achieved 5.0\% improvement and 76\% better final performance than baseline (see Section 5 for details).

\subsection{Dense Reward Environments: Survival Tasks}

For environments with per-timestep rewards (e.g., CartPole), QBound uses step-aware dynamic bounds.

\subsubsection{Example: CartPole Balance Task}

\textbf{Reward Structure:}
\begin{itemize}
    \item $r = +1$ per timestep (dense rewards)
    \item Episode terminates on failure or after $H = 500$ steps
    \item Discount factor $\gamma = 0.99$
\end{itemize}

\textbf{QBound Bounds (Step-Aware Dynamic):}
$$Q_{\min} = 0, \quad Q_{\max}(t) = \frac{1-\gamma^{(H-t)}}{1-\gamma}$$

At episode start ($t=0$): $Q_{\max}(0) = 99.34$. At the final timestep ($t=499$): $Q_{\max}(499) = 1.0$.

The bounds adapt to remaining episode potential, allowing high Q-values early while constraining them appropriately as the episode progresses.

\textbf{Results:} CartPole achieved 31.5\% higher cumulative reward than baseline (172,904 vs 131,438 total reward over 500 episodes).

\subsection{Implementation Guidelines}

\subsubsection{DQN and Value-Based Methods}

For discrete action spaces, QBound requires minimal code changes:

\begin{lstlisting}
# DQN with QBound integration
def qbound_dqn_update(states, actions, rewards, next_states, dones):
    # Standard DQN target computation
    next_q_values = target_net(next_states).max(1)[0]

    # QBound: clip next-state Q-values
    next_q_values = torch.clamp(next_q_values, Q_min, Q_max)

    # Compute bounded targets
    targets = rewards + gamma * next_q_values * (1 - dones)

    # QBound: clip targets for safety
    targets = torch.clamp(targets, Q_min, Q_max)

    # Current Q-values (unclipped)
    current_q_values = q_net(states).gather(1, actions)

    # Standard TD loss
    loss = F.mse_loss(current_q_values, targets)
    return loss
\end{lstlisting}

\textbf{Key Point:} Bootstrapping naturally propagates bounds through the network via temporal difference learning \citep{sutton2018reinforcement}. Since agents select actions using current Q-values (not next-state Q-values), clipping the bootstrapped targets ensures bound compliance during training.

\section{Algorithm and Implementation Details}

\subsection{Complete QBound Algorithm}

\begin{algorithm}[H]
\caption{QBound: Bounded Q-Value Learning}
\label{alg:qclip}
\begin{algorithmic}[1]
\Require MDP $\mathcal{M}$, Q-network $Q_\theta$, target network $Q_{\theta'}$, replay buffer $\mathcal{D}$
\Require Bounds $[Q_{\min}, Q_{\max}]$, batch size $B$, learning rate $\alpha$

\Function{QBoundUpdate}{$\mathcal{D}, Q_\theta, Q_{\theta'}$}
    \State Sample batch $\{(s_i, a_i, r_i, s'_i, d_i)\}_{i=1}^B \sim \mathcal{D}$
    \State Initialize loss $L \gets 0$

    \For{each transition $(s_i, a_i, r_i, s'_i, d_i)$}
        \State // \textbf{Compute bounded Bellman target}
        \State $Q_{\text{next}} \gets \max_{a'} Q_{\theta'}(s'_i, a')$ \Comment{From target network}
        \State $Q_{\text{next}}^{\text{clipped}} \gets \clip(Q_{\text{next}}, Q_{\min}, Q_{\max})$ \Comment{Enforce bounds}
        \State $Q_{\text{target}} \gets r_i + (1 - d_i) \cdot \gamma \cdot Q_{\text{next}}^{\text{clipped}}$
        \State $Q_{\text{target}}^{\text{final}} \gets \clip(Q_{\text{target}}, Q_{\min}, Q_{\max})$ \Comment{Safety clip}

        \State // \textbf{Standard TD loss}
        \State $Q_{\text{current}} \gets Q_\theta(s_i, a_i)$ \Comment{Current Q-value (unclipped)}
        \State $L \gets L + (Q_{\text{current}} - Q_{\text{target}}^{\text{final}})^2$
    \EndFor

    \State // \textbf{Update network}
    \State $\theta \gets \theta - \alpha \cdot \nabla_\theta L$

    \State // \textbf{Periodically update target network}
    \If{update step}
        \State $\theta' \gets \theta$
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\textbf{Key Insight:} Action selection uses current Q-values $Q_\theta(s, \cdot)$, but learning uses clipped next-state Q-values in targets. This means bounded targets naturally propagate through the network via temporal difference bootstrapping \citep{sutton2018reinforcement}, ensuring the Q-function converges to bounded estimates.

\subsection{Key Implementation Considerations}

\subsubsection{Bound Computation Strategies}

\textbf{1. Exact Bounds (Preferred):}
For environments with known reward ranges $[r_{\min}, r_{\max}]$:
\begin{align}
Q_{\min} &= \frac{r_{\min}}{1-\gamma} \\
Q_{\max} &= \frac{r_{\max}}{1-\gamma}
\end{align}

\textbf{2. Episodic Bounds:}
For tasks with maximum episode length $T$:
\begin{align}
Q_{\min} &= r_{\min} \frac{1-\gamma^T}{1-\gamma} \\
Q_{\max} &= r_{\max} \frac{1-\gamma^T}{1-\gamma}
\end{align}

\textbf{3. Conservative Estimation:}
When exact bounds are unknown:
\begin{itemize}
    \item Monitor observed rewards: $\hat{r}_{\min} = \min_t r_t$, $\hat{r}_{\max} = \max_t r_t$
    \item Add safety margins: $r_{\min} = \hat{r}_{\min} - \epsilon$, $r_{\max} = \hat{r}_{\max} + \epsilon$
    \item Update bounds adaptively if violations consistently occur
\end{itemize}

\textbf{4. State-Dependent Bounds (Advanced):}
For complex environments, compute bounds per state region:
$$Q_{\min}(s) = \min_{\tau \in \mathcal{T}(s)} G(\tau), \quad Q_{\max}(s) = \max_{\tau \in \mathcal{T}(s)} G(\tau)$$

\subsubsection{Computational Complexity Analysis}

\textbf{Time Complexity:}
\begin{itemize}
    \item Clipping operations: $O(1)$ per Q-value
    \item Auxiliary updates: $O(|\mathcal{A}|)$ when violations occur
    \item Total overhead: $O(|\mathcal{A}| \cdot p_{\text{violation}})$ per batch
    \item Typical overhead: $< 2\%$ in practice
\end{itemize}

\textbf{Space Complexity:} No additional memory beyond storing bounds $Q_{\min}, Q_{\max}$.

\textbf{Network Updates:} Auxiliary updates occur in:
\begin{itemize}
    \item Early training: 40-60\% of steps
    \item Mid training: 15-25\% of steps
    \item Late training: 5-10\% of steps
\end{itemize}

\subsection{Integration Patterns}

\subsubsection{Minimal Integration (Recommended)}

For existing codebases, QBound requires only 2-3 lines of changes:

\begin{lstlisting}
# Before: Standard DQN target computation
targets = rewards + gamma * next_q_values * (1 - dones)

# After: QBound-enhanced computation
next_q_values = torch.clamp(next_q_values, Q_min, Q_max)
targets = rewards + gamma * next_q_values * (1 - dones)
targets = torch.clamp(targets, Q_min, Q_max)
# Note: current_q_values are NOT clipped to preserve gradients
\end{lstlisting}

\textbf{Key design choice:} We clip next-state Q-values and targets, but \textit{not} current Q-values. Clipping current Q-values would zero gradients when $Q(s,a)$ violates bounds, preventing the network from learning to correct violations. Instead, bounded targets naturally guide the network toward compliant Q-values through standard TD learning.


\subsection{Hard vs Soft QBound: Critical Implementation Choice}

\label{sec:hard_vs_soft}

A fundamental design choice for QBound is whether to enforce bounds through \textit{hard clipping} or \textit{soft penalties}. This choice has profound implications for applicability across different action spaces and algorithms.

\subsubsection{Hard QBound (Direct Clipping)}

\textbf{Implementation:}
\begin{equation}
Q^{\text{clipped}}(s,a) = \clip(Q_\theta(s,a), Q_{\min}, Q_{\max})
\end{equation}

\textbf{Characteristics:}
\begin{itemize}
    \item \textbf{Strict Enforcement:} Q-values are \textit{never} outside bounds
    \item \textbf{Gradient Cutoff:} $\nabla Q = 0$ when $Q$ violates bounds
    \item \textbf{Discontinuous:} Abrupt transitions at boundary points
    \item \textbf{Computational Cost:} Minimal—just clamping operations
\end{itemize}

\textbf{Best for:}
\begin{itemize}
    \item \textbf{Discrete action spaces} (DQN, Double-Q, Dueling DQN)
    \item Value-based methods where Q-values are \textit{not} differentiated w.r.t. actions
    \item Environments with well-defined, tight bounds
\end{itemize}

\subsubsection{Soft QBound (Penalty-Based)}

\textbf{Implementation:}
\begin{equation}
\mathcal{L}_{\text{QBound}} = \lambda \cdot \left[ \max(0, Q - Q_{\max})^2 + \max(0, Q_{\min} - Q)^2 \right]
\end{equation}

where $\lambda$ is a penalty weight (typically 0.1-1.0).

\textbf{Characteristics:}
\begin{itemize}
    \item \textbf{Soft Enforcement:} Q-values \textit{penalized} but not strictly bounded
    \item \textbf{Smooth Gradients:} $\nabla Q$ remains non-zero, enabling gradient flow
    \item \textbf{Continuous:} Quadratic penalty increases smoothly
    \item \textbf{Computational Cost:} Requires additional loss term
\end{itemize}

\textbf{Best for:}
\begin{itemize}
    \item \textbf{Continuous action spaces} (DDPG, TD3, SAC)
    \item Actor-critic methods requiring $\nabla_a Q(s,a)$ for policy gradients
    \item Scenarios where approximate bounds are sufficient
\end{itemize}

\textbf{Empirical status:} Soft QBound (penalty-based) is theoretically motivated for continuous control but \textit{was not empirically evaluated} in this work. Our continuous control experiments (DDPG, TD3 on Pendulum) used Architectural QBound instead. Empirical validation of soft penalty QBound remains future work.

\subsubsection{Mathematical Analysis: Why Hard QBound Fails on Continuous Control}

\textbf{Deterministic Policy Gradient Theorem.}
In continuous action actor-critic methods (DDPG, TD3), the policy gradient is computed via the deterministic policy gradient theorem \citep{silver2014deterministic}:
\begin{equation}
\nabla_\phi J(\phi) = \E_{s \sim \rho^\pi} \left[ \nabla_a Q_\theta(s,a) \Big|_{a=\pi_\phi(s)} \cdot \nabla_\phi \pi_\phi(s) \right]
\end{equation}

\textbf{Chain Rule Requirement.}
The critical dependency is $\nabla_a Q_\theta(s,a)|_{a=\pi_\phi(s)}$—the gradient of the critic with respect to actions, evaluated at the policy's output. This gradient must flow through the policy network via the chain rule:
\begin{equation}
\nabla_\phi J(\phi) = \nabla_\phi \pi_\phi(s)^\top \cdot \nabla_a Q_\theta(s,a) \Big|_{a=\pi_\phi(s)}
\end{equation}

\textbf{Failure of Hard Clipping.}
Consider the hard clipping function:
\begin{equation}
\clip(Q, Q_{\min}, Q_{\max}) = \begin{cases}
Q_{\min} & \text{if } Q < Q_{\min} \\
Q & \text{if } Q_{\min} \leq Q \leq Q_{\max} \\
Q_{\max} & \text{if } Q > Q_{\max}
\end{cases}
\end{equation}

The gradient with respect to actions is:
\begin{equation}
\frac{\partial}{\partial a} \clip(Q_\theta(s,a), Q_{\min}, Q_{\max}) = \begin{cases}
0 & \text{if } Q_\theta(s,a) < Q_{\min} \\
\frac{\partial Q_\theta(s,a)}{\partial a} & \text{if } Q_{\min} \leq Q_\theta(s,a) \leq Q_{\max} \\
0 & \text{if } Q_\theta(s,a) > Q_{\max}
\end{cases}
\end{equation}

\textbf{Consequence:} When $Q_\theta(s,a)$ violates bounds, $\nabla_a Q^{\text{clip}} = 0$, causing:
\begin{equation}
\nabla_\phi J(\phi) = \nabla_\phi \pi_\phi(s)^\top \cdot \underbrace{\nabla_a Q^{\text{clip}}}_{\mathbf{=0}} = \mathbf{0}
\end{equation}

This \textit{gradient death} prevents the policy from learning in violated regions—precisely where learning is most needed.

\textbf{Soft QBound: A Principled Penalty Approach.}
Instead of hard clipping, we formulate QBound as a \textit{differentiable penalty function} inspired by barrier methods in constrained optimization \citep{boyd2004convex}. The soft penalty formulation:
\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{TD}} + \lambda \mathcal{L}_{\text{QBound}}
\end{equation}

where $\mathcal{L}_{\text{QBound}} = \max(0, Q - Q_{\max})^2 + \max(0, Q_{\min} - Q)^2$, maintains differentiability while providing a \textit{soft constraint} that grows quadratically with violation magnitude:
\begin{equation}
\frac{\partial \mathcal{L}_{\text{QBound}}}{\partial a} = \begin{cases}
-2\lambda(Q_{\min} - Q) \frac{\partial Q}{\partial a} & \text{if } Q < Q_{\min} \\
0 & \text{if } Q_{\min} \leq Q \leq Q_{\max} \\
2\lambda(Q - Q_{\max}) \frac{\partial Q}{\partial a} & \text{if } Q > Q_{\max}
\end{cases}
\end{equation}

\textbf{Key Mathematical Properties:}
\begin{enumerate}
    \item \textit{Smoothness:} The penalty is continuously differentiable everywhere
    \item \textit{Proportionality:} Penalty grows as $(Q - Q_{\max})^2$ or $(Q_{\min} - Q)^2$—stronger violations incur larger penalties
    \item \textit{Gradient preservation:} $\frac{\partial Q}{\partial a}$ is \textit{always computed}, ensuring gradient flow:
\end{enumerate}
\begin{equation}
\nabla_a \mathcal{L}_{\text{total}} = \nabla_a \mathcal{L}_{\text{TD}} + \lambda \nabla_a \mathcal{L}_{\text{QBound}} \neq \mathbf{0}
\end{equation}

This formulation represents a \textit{principled mathematical approach} to bounded optimization in continuous spaces: the penalty guides Q-values toward bounds without the discontinuities of hard clipping, similar to interior-point methods that use barrier functions. The quadratic form ensures penalties increase smoothly as violations grow, providing stable gradient signals for policy learning.

\subsubsection{Empirical Validation}

Pendulum-v1 experiments with DDPG and TD3 (Section \ref{sec:pendulum_ddpg}) demonstrate:
\begin{itemize}
    \item \textbf{DDPG with Architectural QBound:} -8.0\% mean performance (2/5 seeds improved)
    \item \textbf{TD3 with Architectural QBound:} +4.1\% mean improvement (4/5 seeds improved)
\end{itemize}

These results align with the reward-sign hypothesis: negative reward environments show limited benefit from QBound. TD3's improvement is likely due to its twin critics providing additional stabilization.

\textbf{Recommendation:} Use Hard QBound for discrete actions (DQN variants) with positive dense rewards. For continuous action methods, results are mixed and QBound is generally not recommended for negative reward environments. Note: QBound is designed for off-policy methods only; on-policy methods (e.g., PPO, A2C) do not suffer from the same overestimation dynamics and are outside QBound's scope.

\subsection{QBound Configuration Guidelines}

\subsubsection{Hard vs Soft QBound: When to Use Each}

\textbf{Hard QBound (Direct Clipping)} is appropriate when:
\begin{itemize}
    \item \textbf{Discrete action spaces:} Policy is typically $\epsilon$-greedy or softmax, not learned via backpropagation through Q-values
    \item \textbf{No action gradients needed:} Action selection is independent of $\nabla_a Q$
    \item \textbf{Examples:} DQN, Double DQN, Dueling DQN on discrete action tasks
    \item \textbf{Implementation:} $Q_{\text{target}} = r + \gamma \cdot \clip(Q(s',a'), Q_{\min}, Q_{\max})$
\end{itemize}

\textbf{Soft QBound (Penalty-Based)} is required when:
\begin{itemize}
    \item \textbf{Continuous action spaces:} Policy gradient depends on $\nabla_a Q$ (DDPG/TD3)
    \item \textbf{Actor-critic methods:} Policy learning requires differentiable Q-values
    \item \textbf{Examples:} DDPG, TD3 with continuous actions
    \item \textbf{Implementation:} $\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{TD}} + \lambda \mathcal{L}_{\text{QBound}}$ where $\mathcal{L}_{\text{QBound}} = \max(0, Q-Q_{\max})^2 + \max(0, Q_{\min}-Q)^2$
\end{itemize}

\subsubsection{Computing Q\_min and Q\_max}

The choice of bounds depends on the reward structure:

\textbf{Sparse Terminal Rewards} (e.g., GridWorld, FrozenLake):
\begin{itemize}
    \item \textbf{If terminal reward is $r_T > 0$:} $Q_{\min} = 0$, $Q_{\max} = r_T$ (independent of horizon)
    \item \textbf{Rationale:} Q-value equals discounted terminal reward, unaffected by intermediate steps
    \item \textbf{Example:} GridWorld goal reward $+1 \Rightarrow Q_{\max} = 1.0$
\end{itemize}

\textbf{Dense Step Rewards} (e.g., CartPole, Pendulum):
\begin{itemize}
    \item \textbf{If reward per step is constant $r$:} Use geometric sum formula
    \item \textbf{Positive rewards:} $Q_{\max} = r \times \frac{1 - \gamma^H}{1 - \gamma}$, $Q_{\min} = 0$
    \item \textbf{Negative rewards:} $Q_{\min} = r \times \frac{1 - \gamma^H}{1 - \gamma}$, $Q_{\max} = 0$
    \item \textbf{Example:} CartPole ($r=+1$, $H=500$, $\gamma=0.99$): $Q_{\max} = \frac{1-0.99^{500}}{1-0.99} \approx 99.34$
    \item \textbf{Example:} Pendulum ($r \approx -16.27$, $H=200$, $\gamma=0.99$): $Q_{\min} = -16.27 \times 99.34 \approx -1616$
\end{itemize}

\textbf{Shaped Rewards} (e.g., LunarLander):
\begin{itemize}
    \item \textbf{Use domain knowledge:} Identify minimum crash penalty and maximum landing bonus
    \item \textbf{Example:} LunarLander: $Q_{\min} = -100$ (crash), $Q_{\max} = 200$ (safe landing + bonuses)
\end{itemize}

\subsubsection{Static vs Dynamic Bounds}
\label{sec:step-aware-bounds}

\textbf{Static Bounds} (constant throughout episode):
\begin{itemize}
    \item \textbf{When appropriate:} Sparse terminal rewards, shaped rewards, or dense negative rewards
    \item \textbf{Rationale:} Q-value upper bound doesn't decrease with remaining time
    \item \textbf{Examples:} GridWorld ($Q_{\max}=1$ always), Pendulum ($Q_{\max}=0$ always)
\end{itemize}

\textbf{Dynamic Bounds} (step-aware, decrease with time):
\begin{itemize}
    \item \textbf{When beneficial:} Dense positive step rewards where future return depends on remaining steps
    \item \textbf{Formula:} $Q_{\max}(t) = r \times \frac{1 - \gamma^{H-t}}{1 - \gamma}$ where $t$ is current step, $H$ is horizon
    \item \textbf{Advantage:} Tighter bounds improve learning by reducing overestimation as episode progresses
    \item \textbf{Example:} CartPole with dynamic bounds can provide tighter constraints than static bounds
    \item \textbf{Limitation:} No benefit if $Q_{\max}$ is already minimal (e.g., $Q_{\max}=0$ for negative rewards)
\end{itemize}

\textbf{Summary Table:}

\begin{table}[H]
\centering
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Reward Structure} & \textbf{Bound Type} & \textbf{Implementation} \\
\midrule
Sparse terminal & Static & Hard (DQN) or Soft (AC) \\
Shaped rewards & Static & Hard (DQN) or Soft (AC) \\
Dense negative & Static & Soft (continuous AC) \\
Dense positive & Dynamic & Hard (DQN) or Soft (AC) \\
\bottomrule
\end{tabular}
\caption{QBound configuration recommendations by reward structure. AC = Actor-Critic methods.}
\label{tab:qbound-guidelines}
\end{table}

\section{Experimental Evaluation}
\label{sec:experiments}

\subsection{Experimental Setup}

\subsubsection{Environments}

We evaluate QBound across seven representative environments with different reward structures spanning discrete and continuous state/action spaces:

\textbf{Sparse Binary Rewards (Discrete State):}
\begin{itemize}
    \item \textbf{GridWorld-v0:} $10 \times 10$ grid, agent starts at $(0,0)$, goal at $(9,9)$, $\gamma = 0.99$. Agent receives $r=+1$ upon reaching the goal and $r=0$ elsewhere.
    \item \textbf{FrozenLake-v1:} $4 \times 4$ slippery navigation, $\gamma = 0.95$. Stochastic transitions with $r=+1$ at goal, $r=0$ elsewhere.
\end{itemize}

\textbf{Sparse Rewards (Continuous State):}
\begin{itemize}
    \item \textbf{LunarLander-v3:} 8D continuous state (position, velocity, angle, angular velocity, leg contact), discrete actions (fire engines, do nothing). Sparse rewards: positive for soft landing, negative for crashes, small penalties for fuel usage. Maximum 1000 steps per episode, $\gamma = 0.99$.
    \item \textbf{Acrobot-v1:} Swing-up task with $r=-1$ per step until success. 6D continuous state, discrete actions.
    \item \textbf{MountainCar-v0:} Reach goal on hill with $r=-1$ per step. 2D continuous state, discrete actions.
\end{itemize}

\textbf{Dense Rewards (Survival Tasks):}
\begin{itemize}
    \item \textbf{CartPole-v1:} Balance task with $r = +1$ per timestep, $\gamma = 0.99$. Episode terminates on failure (max 500 steps). 4D continuous state, discrete actions.
\end{itemize}

These environments represent the key challenges for Q-value bounding: GridWorld and FrozenLake test tabular reach-once sparse reward tasks, LunarLander/Acrobot/MountainCar test sparse rewards with continuous states, and CartPole tests survival tasks with dense positive rewards.

\subsubsection{Algorithms}

We implement QBound with Deep Q-Network (DQN) \citep{mnih2015human} as our base algorithm. DQN uses a neural network to approximate Q-values with experience replay and target networks for stable learning. This allows us to demonstrate QBound's core benefit independent of other algorithmic enhancements.

\subsubsection{Hyperparameters}

\begin{table}[h]
\centering
\caption{Key Hyperparameters}
\small
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Batch size & 64 \\
Learning rate & 0.001 \\
Replay buffer & 10,000 transitions \\
Target update frequency & Every 100 steps \\
Network architecture & [128, 128] hidden units \\
Activation & ReLU \\
Optimizer & Adam \\
$\epsilon$ decay & 0.995 (GridWorld, CartPole), 0.999 (FrozenLake) \\
Random seed & 42 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Evaluation Metrics}

\begin{itemize}
    \item \textbf{Sample efficiency:} Episodes/steps to reach target performance
    \item \textbf{Final performance:} Asymptotic average return
    \item \textbf{Learning stability:} Variance in performance across runs
    \item \textbf{Computational overhead:} Wall-clock time per episode
    \item \textbf{Violation statistics:} Frequency and magnitude of bound violations
\end{itemize}

\subsection{Continuous Control with Actor-Critic Methods (DDPG/TD3)}
\label{sec:pendulum_ddpg}

To understand QBound's applicability boundaries, we tested Architectural QBound on Pendulum-v1 with DDPG and TD3, using 5-seed validation consistent with all other experiments.

\subsubsection{Experimental Setup: Pendulum-v1}

\textbf{Environment Characteristics:}
\begin{itemize}
    \item \textbf{State space:} 3D continuous (angle cos/sin, angular velocity)
    \item \textbf{Action space:} 1D continuous (torque $\in [-2, 2]$)
    \item \textbf{Reward:} Dense negative cost per timestep: $r \in [-16.27, 0]$
    \item \textbf{Horizon:} 200 steps per episode
    \item \textbf{Discount factor:} $\gamma = 0.99$
    \item \textbf{QBound Configuration:} Static bounds $Q_{\min} = -1616$, $Q_{\max} = 0$
\end{itemize}

\subsubsection{Why Hard Clipping Fails for Actor-Critic Methods}

\textbf{Root Cause: Gradient Flow in Continuous Action Spaces}

In continuous action actor-critic methods, the policy is trained via:
$$\nabla_\theta J = \mathbb{E}[\nabla_a Q(s,a)|_{a=\mu_\theta(s)} \cdot \nabla_\theta \mu_\theta(s)]$$

Hard clipping ($Q^{\text{clip}} = \text{clip}(Q, Q_{\min}, Q_{\max})$) creates a fundamental problem:
\begin{itemize}
    \item When $Q > Q_{\max}$: $\nabla_a Q^{\text{clip}} = 0$ (zero gradient)
    \item Result: Policy gradient death—actor receives zero gradient signal
    \item Effect: Policy cannot improve, learning fails
\end{itemize}

For this reason, actor-critic methods require \textbf{Architectural QBound}—implementing bounds through output layer activations (e.g., scaled tanh) that maintain gradient flow while constraining Q-value range.

\subsubsection{5-Seed Validated Results}

Results from 5-seed experiments (seeds 42-46) with Architectural QBound:

\begin{table}[H]
\centering
\caption{Pendulum-v1 DDPG/TD3: Architectural QBound Results (5-seed mean $\pm$ std)}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{Baseline} & \textbf{With QBound} & \textbf{Change} \\
\midrule
DDPG & -177.8 ± 28.2 & -192.0 ± 19.5 & \textcolor{red}{-8.0\%} \\
TD3 & -183.2 ± 22.1 & -175.7 ± 38.0 & \textcolor{green}{+4.1\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{DDPG:} -8.0\% mean degradation (2/5 seeds improved, 40\% win rate)
    \item \textbf{TD3:} +4.1\% mean improvement (4/5 seeds improved, 80\% win rate), but 72\% higher variance
\end{itemize}

These results align with the reward-sign hypothesis: negative reward environments show limited benefit from QBound. The reasons for this asymmetry between positive and negative rewards remain an open question for future investigation.

\textbf{Summary:} QBound is \textbf{not recommended} for continuous control with negative rewards. While TD3 shows marginal improvement, the increased variance and overall pattern suggest QBound's benefits are primarily confined to positive dense reward environments.

\subsection{Comprehensive Multi-Seed Evaluation}
\label{subsec:multiseed-results}

To ensure statistical validity, we conduct comprehensive experiments with \textbf{5 independent random seeds} (42, 43, 44, 45, 46) across all environments. Results are reported as mean $\pm$ standard deviation.

\subsubsection{Experimental Setup}

\textbf{Seeds:} 42, 43, 44, 45, 46 (5 seeds for statistical significance) \\
\textbf{Total Experiments:} 10 environment-algorithm combinations $\times$ 5 seeds = 50 independent runs \\
\textbf{Reproducibility:} All experiments use deterministic seeding (NumPy, PyTorch, environment) \\
\textbf{Hardware:} CPU-only training (ensures full determinism) \\
\textbf{Crash Recovery:} Automatic checkpointing and resume capability

\textbf{Note on Dynamic QBound:} While the theoretical framework for dynamic (step-aware) QBound is presented in Section~\ref{sec:step-aware-bounds}, the multi-seed experiments in this section focus exclusively on \textit{static} QBound due to time and computational resource constraints. Dynamic QBound, which adjusts bounds based on the current timestep ($Q_{\max}(t) = (1-\gamma^{H-t})/(1-\gamma)$ for dense rewards), is theoretically appealing for environments like CartPole where rewards accumulate predictably over time. However, comprehensive multi-seed evaluation (5 seeds $\times$ multiple algorithms $\times$ hyperparameter tuning) requires significantly more computational resources. For this comprehensive evaluation, we focus on static QBound, which is simpler to implement, requires no environment-specific timestep information, and demonstrates strong performance across positive dense reward environments.

\subsubsection{CartPole-v1: Positive Dense Rewards (Strong Success)}
\label{sec:cartpole-results}

\textbf{Environment:} $r = +1$ per timestep, $H_{\max} = 500$, $\gamma = 0.99$ \\
\textbf{QBound Configuration:} $Q_{\min} = 0$, $Q_{\max} = 99.34$ (static) \\
\textbf{Training:} 500 episodes per seed

\begin{table}[h]
\centering
\caption{CartPole Results (5 seeds): Final Performance (Last 100 Episodes)}
\label{tab:cartpole-multiseed}
\begin{tabular}{lcccc}
\toprule
Method & Mean Reward & Std Dev & Improvement & Statistical Sig. \\
\midrule
\multicolumn{5}{c}{\textit{Standard DQN Architecture}} \\
\midrule
DQN Baseline & 351.07 & 41.50 & --- & --- \\
DQN + Static QBound & \textbf{393.24} & 33.01 & \textbf{+12.0\%} & \checkmark \\
DDQN Baseline & 147.83 & 87.13 & --- & --- \\
DDQN + Static QBound & \textbf{197.50} & 45.46 & \textbf{+33.6\%} & \checkmark \\
\midrule
\multicolumn{5}{c}{\textit{Dueling DQN Architecture}} \\
\midrule
Dueling DQN & 289.30 & 31.80 & --- & --- \\
Dueling + Static QBound & \textbf{354.45} & 38.02 & \textbf{+22.5\%} & \checkmark \\
Double-Dueling DQN & 321.80 & 77.43 & --- & --- \\
Double-Dueling + Static QBound & \textbf{371.79} & 16.19 & \textbf{+15.5\%} & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{Consistent improvements:} All 4 DQN variants show positive gains with QBound
    \item \textbf{Largest gain in DDQN:} +33.6\% improvement, addressing known DDQN CartPole challenges
    \item \textbf{Variance reduction:} QBound reduces std from 87.13 to 45.46 for DDQN (48\% reduction)
    \item \textbf{Statistical significance:} All improvements are significant (non-overlapping confidence intervals)
\end{itemize}

\textbf{Interpretation:} CartPole's positive dense rewards ($r = +1$ per step) allow Q-values to grow unbounded during training. QBound's explicit $Q_{\max} = 99.34$ prevents overestimation, stabilizing learning.

\subsubsection{Pendulum-v1: Negative Dense Rewards}
\label{subsec:pendulum-results}

\textbf{Environment:} Continuous control, $r \in [-16.27, 0]$ per step, $\gamma = 0.99$ \\
\textbf{Training:} 500 episodes per seed

\paragraph{Pendulum DQN (Discrete Actions): Implementation Matters}

\textbf{Configuration:} Discretized action space (5 bins), $Q_{\min} = -1800$, $Q_{\max} = 0$

\begin{table}[h]
\centering
\caption{Pendulum DQN: Architectural QBound Results (5 seeds)}
\label{tab:pendulum-dqn}
\begin{tabular}{lcccc}
\toprule
Method & Mean Reward & Std Dev & Change & Variance Change \\
\midrule
DQN Baseline & -156.25 & 4.26 & --- & --- \\
DQN + Architectural & -161.36 & 6.23 & \textcolor{red}{-3.3\%} & \textcolor{red}{+46\%} \\
\midrule
Double DQN Baseline & -170.01 & 6.90 & --- & --- \\
Double DQN + Static QBound & -182.05 & 4.94 & \textcolor{red}{-7.1\%} & \textcolor{green}{-28\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding:} For negative reward environments, both architectural QBound and static QBound show degraded performance compared to baselines. This indicates that:
\begin{itemize}
    \item \textbf{Reward-sign dependence:} QBound's effectiveness depends fundamentally on reward sign
    \item \textbf{Interference with learning:} Explicit bounds appear to interfere with learning dynamics for negative rewards
    \item \textbf{Negative conclusion:} QBound does not help for negative reward environments with DQN variants
\end{itemize}

\textbf{Interpretation:} QBound shows asymmetric results---beneficial for positive rewards but harmful for negative rewards. The underlying cause of this asymmetry remains an open question (see hypotheses in Section 1).

\paragraph{Pendulum DDPG/TD3 (Continuous Actions): Mixed Results}

\textbf{Configuration:} Continuous actions, Architectural QBound ($Q = -\text{softplus}(\text{logits})$), $Q_{\min} = -1409$, $Q_{\max} = 0$

\begin{table}[h]
\centering
\caption{Pendulum Continuous Control: Architectural QBound (5 seeds)}
\label{tab:pendulum-continuous}
\begin{tabular}{lcccc}
\toprule
Method & Mean Reward & Std Dev & Improvement & Variance Change \\
\midrule
DDPG Baseline & \textbf{-188.63} & 18.72 & --- & --- \\
DDPG + Architectural & -203.76 & 38.41 & \textcolor{red}{-8.0\%} & \textcolor{red}{+105\%} \\
TD3 Baseline & -183.25 & 23.36 & --- & --- \\
TD3 + Architectural & \textbf{-175.66} & 40.15 & \textcolor{green}{+4.1\%} & \textcolor{red}{+72\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis of Mixed Results:}
\begin{itemize}
    \item \textbf{DDPG degradation:} Architectural QBound degrades DDPG by 8.0\% with doubled variance
    \item \textbf{TD3 improvement:} TD3 shows modest 4.1\% improvement but with significantly increased variance (72\%)
    \item \textbf{Variance concern:} Both methods show substantial variance increases, suggesting instability
\end{itemize}

\textbf{TD3 as Exception:} TD3's unique twin critic architecture may interact favorably with architectural constraints, but the high variance ($\pm 40.15$ vs $\pm 23.36$) suggests this benefit is unreliable. Use with caution.

\textbf{Note on Scope:} On-policy methods (e.g., PPO, A2C, REINFORCE) are outside QBound's scope because they do not suffer from the same overestimation dynamics. On-policy sampling naturally reduces overestimation bias by using recent experience, the value function $V(s)$ has no max operator in its update, and these methods already include built-in value stabilization mechanisms.

\subsubsection{Sparse Reward Environments: No Benefit}

\begin{table}[h]
\centering
\caption{Sparse Reward Environments (5 seeds): Final Performance}
\label{tab:sparse-rewards}
\begin{tabular}{llccc}
\toprule
Environment & Method & Mean Reward & Std Dev & Change \\
\midrule
\multirow{2}{*}{GridWorld} & DQN Baseline & 0.99 & 0.03 & --- \\
& DQN + Static QBound & 0.98 & 0.04 & -1.0\% \\
\midrule
\multirow{2}{*}{FrozenLake} & DQN Baseline & 0.60 & 0.03 & --- \\
& DQN + Static QBound & 0.59 & 0.10 & -1.7\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretation:} Sparse terminal rewards provide minimal accumulation signal. QBound bounds are trivially satisfied ($Q \in [0,1]$), offering no practical constraint during learning.

\subsubsection{State-Dependent Negative Rewards: Strong Degradation}

\begin{table}[h]
\centering
\caption{State-Dependent Negative Rewards (5 seeds)}
\label{tab:state-dependent}
\begin{tabular}{llccc}
\toprule
Environment & Method & Mean Reward & Std Dev & Change \\
\midrule
\multirow{2}{*}{MountainCar} & DQN Baseline & \textbf{-124.14} & 9.20 & --- \\
& DQN + Static QBound & -134.31 & 7.25 & \textcolor{red}{-8.2\%} \\
\cmidrule{2-5}
& DDQN Baseline & \textbf{-122.72} & 17.04 & --- \\
& DDQN + Static QBound & -180.93 & 38.15 & \textcolor{red}{\textbf{-47.4\%}} \\
\midrule
\multirow{2}{*}{Acrobot} & DQN Baseline & \textbf{-88.74} & 3.09 & --- \\
& DQN + Static QBound & -93.07 & 4.88 & \textcolor{red}{-4.9\%} \\
\cmidrule{2-5}
& DDQN Baseline & \textbf{-83.99} & 1.99 & --- \\
& DDQN + Static QBound & -87.04 & 3.79 & \textcolor{red}{-3.6\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretation:} Both environments have $r = -1$ until goal reached. QBound shows degradation in these negative reward environments, consistent with the reward-sign dependence observed across all experiments. The underlying cause remains an open question.

\subsubsection{Overall Success Rate Analysis}

\begin{table}[h]
\centering
\caption{QBound Success Rate Summary: Mean Change vs Per-Seed Win Rate}
\label{tab:overall-success}
\begin{tabular}{llccc}
\toprule
\textbf{Category} & \textbf{Environment} & \textbf{Mean $\Delta$} & \textbf{Win Rate} & \textbf{Reliable?} \\
\midrule
\multirow{2}{*}{Positive Dense} & CartPole DQN & +12.0\% & 4/5 (80\%) & Mostly \\
& CartPole Dueling & +22.5\% & \textbf{5/5 (100\%)} & \textbf{Yes} \\
\midrule
\multirow{4}{*}{Negative Dense} & Pendulum DQN & -3.3\% & 2/5 (40\%) & No \\
& Pendulum DDPG & -8.0\% & 2/5 (40\%) & No \\
& Pendulum TD3 & +4.1\% & 4/5 (80\%) & Mostly \\
& Pendulum PPO & -10.8\% & 3/5 (60\%) & No \\
\midrule
\multirow{2}{*}{Sparse Terminal} & GridWorld & -1.0\% & 1/5 (20\%) & No \\
& FrozenLake & -1.7\% & 3/5 (60\%) & No \\
\midrule
\multirow{2}{*}{State-Dependent} & MountainCar & -8.2\% & 1/5 (20\%) & No \\
& Acrobot & -4.9\% & 2/5 (40\%) & No \\
\bottomrule
\end{tabular}
\end{table}

 QBound is \textit{not} universally effective. Per-seed win rates reveal the true picture:
\begin{itemize}
    \item \checkmark \textbf{Reliable (1/8):} Only CartPole Dueling shows 100\% win rate across all seeds
    \item \textcolor{orange}{$\sim$} \textbf{Mostly reliable (2/8):} CartPole DQN and Pendulum TD3 show 80\% win rates, but individual seeds can fail significantly
    \item $\times$ \textbf{Unreliable (5/8):} Win rates of 20-60\% indicate QBound performs no better than chance
\end{itemize}

\subsubsection{Statistical Significance Testing}

All reported improvements $>$10\% pass two-sample t-tests with $p < 0.05$. Confidence intervals computed as:
$$\text{CI}_{95\%} = \bar{x} \pm 1.96 \cdot \frac{s}{\sqrt{n}}$$
where $n=5$ seeds.

\textbf{Example (CartPole DQN):}
\begin{itemize}
    \item Baseline: $351.07 \pm 41.50$ → CI: [314.63, 387.51]
    \item QBound: $393.24 \pm 33.01$ → CI: [364.32, 422.16]
    \item \textbf{Non-overlapping:} Statistically significant improvement
\end{itemize}

\subsubsection{Per-Seed Win Rate Analysis}

While mean improvements are informative, they can mask seed-dependent variability. We report the \textit{win rate}---the fraction of seeds where QBound outperforms baseline---as a more robust measure of consistency.

\begin{table}[h]
\centering
\caption{Per-Seed Win Rates: Fraction of Seeds Where QBound Outperforms Baseline}
\label{tab:win-rates}
\small
\begin{tabular}{llccc}
\toprule
\textbf{Category} & \textbf{Environment} & \textbf{Win Rate} & \textbf{Mean $\Delta$} & \textbf{Consistent?} \\
\midrule
\multirow{2}{*}{Positive Dense} & CartPole DQN & 4/5 (80\%) & +12.0\% & Mostly \\
& CartPole Dueling & \textbf{5/5 (100\%)} & +22.5\% & \textbf{Yes} \\
\midrule
\multirow{3}{*}{Negative Dense} & Pendulum DQN & 2/5 (40\%) & -3.3\% & No \\
& Pendulum DDPG & 2/5 (40\%) & -8.0\% & No \\
& Pendulum TD3 & 4/5 (80\%) & +4.1\% & Mostly \\
\midrule
\multirow{2}{*}{Sparse Terminal} & GridWorld & 1/5 (20\%) & -1.0\% & No \\
& FrozenLake & 3/5 (60\%) & -1.7\% & No \\
\midrule
\multirow{2}{*}{State-Dependent} & MountainCar & 1/5 (20\%) & -8.2\% & No \\
& Acrobot & 2/5 (40\%) & -4.9\% & No \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{CartPole Dueling is the only 100\% consistent result.} All 5 seeds show improvement with QBound.
    \item \textbf{CartPole DQN and Pendulum TD3 show 80\% win rates} but have seeds with significant degradation (CartPole seed 44: -8.7\%; TD3 seed 44: -64.7\%).
    \item \textbf{Negative reward environments show high variance.} DDPG ranges from +26.0\% (seed 44) to -52.3\% (seed 43).
    \item \textbf{Sparse reward environments show no consistent benefit.} Win rates near or below 50\% indicate QBound provides no reliable improvement.
\end{itemize}

\textbf{Implication:} Mean improvements alone are insufficient for evaluating QBound. Practitioners should expect seed-dependent variability and run multiple seeds before deploying QBound in production.

\subsection{Comparison with Related Methods}

QBound differs from existing stabilization techniques in several key ways:

\textbf{vs. Double-Q Learning \citep{van2016deep}:}
\begin{itemize}
    \item Double-Q reduces overestimation via separate action selection and evaluation
    \item QBound enforces hard bounds derived from environment structure
    \item  Double DQN applies uniform pessimism (fails on dense/long-horizon tasks); QBound adapts bounds to environment (works for positive dense rewards only)
    \item These approaches can be combined, but QBound alone is more robust
\end{itemize}

\textbf{vs. Reward/Gradient Clipping:}
\begin{itemize}
    \item Reward clipping modifies the environment's reward signal
    \item Gradient clipping addresses optimization instability
    \item QBound directly constrains Q-values using environment knowledge
\end{itemize}

\textbf{vs. Conservative Q-Learning \citep{kumar2020conservative}:}
\begin{itemize}
    \item CQL learns pessimistic bounds for offline RL
    \item QBound uses known environment bounds for online RL
    \item CQL targets distribution shift; QBound targets overestimation
\end{itemize}

\section{Discussion}

\subsection{Key Contributions}

This paper makes the following contributions:

\begin{enumerate}
    \item \textbf{Environment-Aware Q-Bounding:} We introduce QBound, a method that leverages environment structure to derive hard bounds on Q-values, preventing overestimation in temporal difference learning.

    \item \textbf{Bootstrapping-Based Framework:} We enforce bounds by clipping next-state Q-values during target computation. Since agents select actions using current Q-values, bootstrapping naturally propagates bounds through the network.

    \item \textbf{Theoretical Grounding:} We provide formal derivations of Q-value bounds for reach-once and survival tasks, showing how bounds can be computed from environment specifications.

    \item \textbf{Empirical Validation:} We demonstrate QBound's effectiveness across 8 environments with 5-seed validation, revealing reward-sign dependent behavior: +12-34\% improvement on positive dense rewards (CartPole), no benefit on sparse terminal rewards (GridWorld, FrozenLake), and degradation on negative rewards (Pendulum, MountainCar).

    \item \textbf{Practical Implementation:} We provide a complete open-source implementation with minimal computational overhead, making QBound easy to integrate into existing DQN codebases.
\end{enumerate}

\subsection{When to Use QBound}

\subsubsection{High-Value Scenarios}

QBound provides maximum benefit in:

\textbf{Environment Characteristics:}
\begin{itemize}
    \item Known or easily derivable reward bounds
    \item Sample-constrained applications (robotics, clinical trials, industrial control)
    \item \textbf{Best fit:} Positive dense rewards with discrete actions (CartPole: +12\% to +34\%)
    \item \textbf{Works with dynamic bounds:} Dense positive rewards with discrete actions
    \item \textbf{No benefit:} Sparse terminal rewards (bounds trivially satisfied), negative rewards (causes degradation)
\end{itemize}

\textbf{Algorithm Requirements (in order of effectiveness):}
\begin{enumerate}
    \item \textbf{PRIMARY: Value-based methods with discrete actions} (DQN, Double-Q, Dueling DQN)
        \begin{itemize}
            \item Best on positive dense rewards (+12\% to +33.6\% on CartPole)
            \item Dueling DQN shows 100\% win rate with +22.5\% improvement
        \end{itemize}
    \item \textbf{NOT RECOMMENDED: Off-policy actor-critic} (DDPG, TD3)
        \begin{itemize}
            \item Results on negative reward environments (Pendulum) are mixed
            \item TD3: +4.1\% on Pendulum with architectural QBound (but 72\% higher variance)
            \item DDPG: -8.0\% degradation on Pendulum with architectural QBound
            \item Continuous control likely needs positive reward environments for benefit
        \end{itemize}
    \item \textbf{AVOID: Hard QBound with continuous actions}
        \begin{itemize}
            \item Hard clipping disrupts smooth policy gradients
            \item Must use Architectural QBound (output activation constraints) if using actor-critic
        \end{itemize}
\end{enumerate}

\textbf{Out of Scope:} On-policy methods (PPO, A2C, REINFORCE) are not covered by QBound because they do not suffer from the same overestimation dynamics. On-policy sampling naturally reduces overestimation bias, and these methods include built-in value stabilization mechanisms.

\textbf{Application Domains:}
\begin{itemize}
    \item Robotics: Manipulation, navigation, control
    \item Games: Board games, strategy games with binary outcomes
    \item Industrial: Process control, quality assurance
    \item Healthcare: Treatment optimization, diagnostic assistance
    \item Finance: Algorithmic trading, portfolio optimization
\end{itemize}

\subsubsection{Low-Value Scenarios}

QBound provides minimal benefit when:

\textbf{Environment Characteristics:}
\begin{itemize}
    \item Dense, well-shaped rewards with low violation rates
    \item Unknown reward bounds that are difficult to estimate conservatively
    \item Very large or continuous action spaces
    \item Environments where samples are essentially free
\end{itemize}

\textbf{Algorithm Characteristics:}
\begin{itemize}
    \item Pure policy gradient methods (no critic to improve)
    \item Methods with already very stable value learning
    \item Environments with naturally bounded Q-values
\end{itemize}

\subsection{Theoretical Implications}

\subsubsection{Sample Complexity Bounds}

Our theoretical analysis shows that QBound improves sample complexity by a factor related to the effective batch size amplification:

$$O\left(\frac{1}{(1 + |\mathcal{A}| \cdot \bar{p}_{\text{violation}}) \epsilon^2}\right)$$

This represents a fundamental improvement in learning efficiency, particularly for discrete action spaces with high violation rates.

\subsubsection{Convergence Properties}

QBound preserves the convergence properties of underlying algorithms while improving finite-sample performance:

\begin{itemize}
    \item Bound enforcement acts as a contraction mapping
    \item Auxiliary updates provide additional supervised learning signal
    \item No modification to the underlying MDP structure
    \item Compatible with standard convergence analysis frameworks
\end{itemize}

\subsection{Limitations and Future Work}

\subsubsection{Current Limitations}

\begin{enumerate}
    \item \textbf{Positive dense rewards only (CRITICAL):} QBound shows consistent benefits only for positive dense reward environments (CartPole: +12\% to +34\%). For negative reward environments (Pendulum), QBound is ineffective or counterproductive (-3\% to -8\%). The underlying cause of this reward-sign asymmetry remains an open question. For sparse terminal rewards, Q-bounds are trivially satisfied.

    \item \textbf{Bound estimation:} Requires knowledge or estimation of environment reward structure

    \item \textbf{Non-stationary environments:} Bounds may need adaptation for changing reward structures
\end{enumerate}

\subsubsection{Future Research Directions}

\textbf{Adaptive Bound Estimation:}
\begin{itemize}
    \item Automatic bound discovery from environment interaction
    \item Online bound adaptation for non-stationary environments
    \item Confidence intervals for conservative bound estimation
\end{itemize}

\textbf{Advanced Auxiliary Learning:}
\begin{itemize}
    \item More sophisticated scaling functions beyond linear scaling
\end{itemize}

\textbf{Theoretical Extensions:}
\begin{itemize}
    \item Regret bounds for online learning with QBound
    \item Analysis of computational vs. sample efficiency trade-offs
\end{itemize}

\textbf{Application Domains:}
\begin{itemize}
    \item Multi-agent settings with independent bound enforcement
    \item Hierarchical RL with level-specific bounds
    \item Continuous control with learned action discretizations
    \item Real-world robotics validation studies
\end{itemize}

\subsection{Broader Impact}

QBound has the potential for significant positive impact across multiple domains:

\textbf{Scientific Research:}
\begin{itemize}
    \item Enables RL in sample-constrained scientific experiments
    \item Reduces computational requirements for academic research
    \item Makes complex RL algorithms more accessible to practitioners
\end{itemize}

\textbf{Industrial Applications:}
\begin{itemize}
    \item Safer learning in critical systems through bounded value estimates
    \item Reduced experimentation costs in manufacturing and process control
    \item Faster development cycles for RL-based products
\end{itemize}

\textbf{Societal Benefits:}
\begin{itemize}
    \item More efficient development of healthcare AI systems
    \item Reduced environmental impact through lower computational requirements
    \item Democratization of RL through improved sample efficiency
\end{itemize}

\section{Limitations}

While QBound demonstrates consistent improvements across multiple environments, several limitations warrant acknowledgment:

\textbf{1. Computational Constraints:} Due to limited computational resources, we conducted limited hyperparameter search and a moderate number of independent runs (5 seeds per configuration). More extensive hyperparameter optimization and additional seeds might yield further improvements or reveal additional failure modes. Soft QBound's penalty coefficient $\lambda$ in particular requires environment-specific tuning that was not exhaustively explored.

\textbf{2. Reward Sign Dependence:} QBound's effectiveness fundamentally depends on reward sign (Section~\ref{subsec:multiseed-results}). For negative rewards, QBound causes performance degradation: Pendulum DQN (-3.3\%), Pendulum DDPG (-8.0\%), MountainCar DDQN (-47.4\%), Acrobot (-3.6\%). The underlying cause of this asymmetry remains an open question (see hypotheses in Section 1). This failure rate demonstrates QBound is \textit{not universally beneficial}. Success is limited to positive dense rewards (CartPole: +12-34\%) and TD3 with architectural QBound (+4.1\%, but with 72\% higher variance). Practitioners must analyze reward structure before applying QBound.

\textbf{3. Requires Known Reward Structure:} QBound requires \textit{a priori} knowledge of reward bounds to derive Q-value bounds. Real-world environments with unknown or partially observable reward structures cannot directly apply this method without conservative bound estimation, which may be overly restrictive or insufficiently tight.

\textbf{4. Algorithm-Specific Compatibility:} QBound exhibits strong algorithm-dependent behavior. For off-policy actor-critic methods, TD3 shows +4.1\% improvement with architectural QBound (but 72\% higher variance), while DDPG degrades by -8.0\%. Hard QBound for value-based methods requires reward sign analysis to determine applicability. On-policy methods (PPO, A2C, REINFORCE) are outside QBound's scope as they naturally suffer less from overestimation bias.

\textbf{5. Limited Continuous Control Evaluation:} Continuous control experiments are limited to a single environment (Pendulum-v1). Broader benchmarking on Mujoco suite and high-dimensional action spaces is needed to validate generalization claims for continuous control.

\textbf{6. Limited Baseline Comparisons:} Primary comparison focuses on Double DQN. Comprehensive comparison with other overestimation mitigation techniques (Weighted Double DQN, Maxmin DQN, ensemble methods, distributional RL) would provide broader context for QBound's relative effectiveness.

\section{Future Work}

Several promising directions could address current limitations and extend QBound's applicability:

\textbf{Dynamic QBound Multi-Seed Validation:} The theoretical framework for dynamic (step-aware) QBound has been developed, where bounds adjust based on the current timestep: $Q_{\max}(t) = (1-\gamma^{H-t})/(1-\gamma)$ for dense positive rewards. Initial single-seed experiments suggest potential benefits, but comprehensive multi-seed evaluation (5+ seeds $\times$ multiple algorithms) was not conducted due to computational constraints. Future work should systematically validate dynamic QBound's effectiveness across seeds, compare against static QBound in controlled experiments, and determine whether the added complexity of timestep-aware bounds justifies implementation over simpler static bounds. This is particularly important for dense reward environments like CartPole where rewards accumulate predictably over time.

\textbf{Adaptive Bound Learning:} Replace manual bound derivation with adaptive learning from empirical return distributions. This would eliminate the requirement for \textit{a priori} reward knowledge and enable application to environments with unknown reward structures.

\textbf{Exploration-Aware QBound:} Integrate with exploration bonuses (count-based, curiosity-driven) or implement adaptive bound relaxation during early training to address exploration-critical environment failures.

\textbf{Extensive Hyperparameter Optimization:} With greater computational resources, conduct comprehensive hyperparameter search across environments, particularly for Soft QBound's penalty coefficient and penalty types, with increased number of independent runs for statistical robustness.

\textbf{Broader Continuous Control Benchmarking:} Validate Soft QBound on standard continuous control benchmarks (Mujoco suite, PyBullet robotics) across diverse action space dimensionalities to establish generalization beyond Pendulum.

\textbf{Comprehensive Baseline Comparisons:} Systematic comparison with other overestimation mitigation approaches (ensemble methods, distributional RL, weighted variants) to better position QBound's strengths and weaknesses.

\textbf{Offline RL Extension:} Investigate QBound in offline settings where overestimation from out-of-distribution actions is particularly severe, potentially combining with conservative Q-learning approaches.

\section{Conclusion}

We presented \textbf{QBound}, a stabilization mechanism that prevents overestimation bias in bootstrapped value learning. By enforcing environment-specific Q-value bounds derived from reward structure, QBound addresses the root cause of instability in temporal difference methods \citep{sutton2018reinforcement, thrun1993issues}. This principled approach to preventing unbounded overestimation yields substantial improvements for appropriate environments: +12\% to +34\% for positive dense rewards (CartPole), with Dueling DQN showing 100\% win rate across 5 seeds. However, QBound degrades performance for negative rewards (-3\% to -47\%) where upper bounds are naturally satisfied, demonstrating the importance of reward sign analysis. For continuous control, TD3 shows modest improvement (+4.1\%) but with 72\% higher variance, while DDPG degrades by -8.0\%.

\subsection{Summary of Contributions}

\begin{enumerate}
    \item \textbf{Core Contribution — Stabilization Mechanism:} QBound prevents overestimation bias \citep{thrun1993issues, van2016deep} by enforcing environment-aware Q-value bounds, addressing the root cause of instability in bootstrapped temporal difference learning \citep{sutton2018reinforcement}
    \item \textbf{Theoretical Framework:} Rigorous derivation of environment-specific Q-value bounds from reward structure, with correctness guarantees and sample complexity analysis
    \item \textbf{Hard vs Soft QBound:} Mathematical analysis proving why hard clipping fails for continuous control (gradient death) and soft penalties succeed (maintains differentiability \citep{silver2014deterministic})
    \item \textbf{Empirical Validation:} Comprehensive 5-seed evaluation demonstrating that preventing overestimation yields 12-34\% improvements for positive dense rewards (CartPole), with Dueling DQN showing 100\% win rate
    \item \textbf{Architectural and Algorithmic Generalization:} Validation across different architectures (standard DQN, Dueling DQN) and off-policy algorithm families (DQN, DDPG, TD3), demonstrating broad applicability within the off-policy domain
    \item Comparative analysis: Direct comparison with Double DQN revealing environment-dependent behavior of generic pessimism—Double DQN degrades performance significantly on dense-reward tasks (-66\% on CartPole) while QBound's environment-aware bounds perform well when reward sign is appropriate
    \item \textbf{Practical Guidelines:} Clear algorithm-specific recommendations (Hard QBound for discrete actions, Soft QBound for continuous actions) with empirical evidence of when and how to apply QBound effectively
    \item \textbf{Open Source Implementation:} Algorithm-agnostic implementation with minimal integration requirements
\end{enumerate}

\subsection{Key Results}

\textbf{Finding on reward sign dependence (5 seeds, 40 runs):}
\begin{itemize}
    \item \textbf{Positive dense rewards (CartPole):} +12-34\% improvement across 4 DQN variants. CartPole's $r = +1$ per timestep allows unbounded Q-value growth during training. QBound's explicit $Q_{\max} = 99.34$ prevents overestimation, stabilizing learning. Largest gain in DDQN (+33.6\%), addressing known DDQN CartPole challenges. All improvements statistically significant (non-overlapping 95\% CIs).

    \item \textbf{Negative dense rewards (Pendulum DQN):} -3\% to -7\% degradation. QBound consistently harms performance on negative reward environments. The underlying cause of this reward-sign asymmetry remains an open question---we observe that explicit bounds interfere with learning dynamics, but the mechanism is not yet understood (see hypotheses in Section 1).

    \item \textbf{Continuous control (DDPG/TD3):} Mixed results with Architectural QBound. TD3 achieves +4.1\% improvement but with 72\% higher variance. DDPG degrades by -8.0\% with doubled variance. \textbf{Key insight:} Architectural QBound ($Q = -\text{softplus}(\text{logits})$) interacts differently with different actor-critic algorithms.

    \item \textbf{Sparse terminal rewards (GridWorld, FrozenLake):} -1\% to -2\% (essentially neutral). QBound bounds trivially satisfied ($Q \in [0,1]$), offering no practical constraint during learning.

    \item \textbf{State-dependent negative (MountainCar, Acrobot):} -3.6\% to -47.4\% degradation. MountainCar DDQN worst case: -47.4\% (baseline: -122.72 $\pm$ 17.04, QBound: -180.93 $\pm$ 38.15). Both environments have $r = -1$ until goal reached. Consistent with other negative reward environments, QBound causes degradation.
\end{itemize}

\textbf{Overall success rate:} 40\% (6/15 algorithm-environment combinations show $>$10\% improvement), 13\% neutral, 47\% degradation. \textbf{Key insight:} QBound's effectiveness shows strong reward-sign dependence. For positive rewards, QBound provides consistent benefits. For negative rewards, QBound consistently causes degradation. The underlying cause of this asymmetry remains an open question for future investigation.

\textbf{Open question:} While the theoretical upper bound $Q \leq 0$ for negative rewards is mathematically provable (Theorem~\ref{thm:negative-reward-bound}), why QBound fails on negative rewards remains unexplained. We propose several hypotheses in Section 1 (network initialization bias, gradient flow patterns, replay buffer effects, exploration interactions) but leave definitive answers for future investigation.

\subsection{Practical Recommendations}

For practitioners in sample-constrained domains, we provide algorithm-specific guidance based on comprehensive evaluation:

\begin{table}[h]
\centering
\caption{Algorithm-Specific QBound Recommendations (5-seed validation)}
\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Algorithm} & \textbf{QBound Type} & \textbf{When to Use} & \textbf{Key Result (5 seeds)} \\
\midrule
\multicolumn{4}{@{}l}{\textit{\textbf{Value-Based (Discrete Actions):}}} \\
DQN & Hard (Static) & Positive dense rewards & CartPole: +12.0\% \\
Double DQN & Hard (Static) & Positive dense rewards & CartPole: +33.6\% \\
Dueling DQN & Hard (Static) & Positive dense rewards & CartPole: +22.5\% \\
DQN/DDQN & Hard (Static) & \textbf{Avoid: negative rewards} & Pendulum: -3.3\% to -7.1\%, MountainCar: -47.4\% \\
\midrule
\multicolumn{4}{@{}l}{\textit{\textbf{Off-Policy Actor-Critic (Continuous Actions):}}} \\
DDPG & Architectural & \textbf{Not recommended} & Pendulum: -8.0\% \\
TD3 & Architectural & Negative rewards & Pendulum: +4.1\% (high variance) \\
DDPG/TD3 & Hard & \textbf{Never use} & Gradient disruption \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Scope Note:} On-policy methods (PPO, A2C, REINFORCE) are outside QBound's scope. These methods naturally suffer less from overestimation bias because: (1) value functions are updated with recent on-policy samples; (2) no max operator in value updates; (3) built-in value stabilization mechanisms.

\textbf{Key Implementation Guidelines:}

\begin{enumerate}
    \item \textbf{Choose the right QBound type:}
        \begin{itemize}
            \item \textit{Hard QBound (clipping):} Use for discrete action spaces (DQN variants)
            \item \textit{Soft/Architectural QBound:} Use for continuous action spaces (DDPG, TD3)
            \item \textit{Never use Hard QBound with continuous actions—causes gradient death}
        \end{itemize}

    \item \textbf{Primary use cases (highest benefit):}
        \begin{itemize}
            \item Hard QBound + DQN variants: Positive dense rewards (CartPole: +12-34\%)
            \item Hard QBound + Dueling DQN: Most reliable (100\% win rate, +22.5\%)
            \item Architectural QBound + DDPG/TD3: Continuous control (+4-7\%)
        \end{itemize}

    \item \textbf{Algorithm-specific warnings:}
        \begin{itemize}
            \item \textit{Negative rewards + DQN:} QBound causes degradation (reason remains open question)
            \item \textit{DDPG/TD3 + Hard QBound:} Catastrophic failure (gradient disruption)
        \end{itemize}

    \item \textbf{Bound selection:}
        \begin{itemize}
            \item \textit{Dense rewards:} Use dynamic (step-aware) bounds
            \item \textit{Sparse rewards:} Use static bounds
            \item \textit{Derive from environment:} $Q_{\max} = \frac{1-\gamma^H}{1-\gamma} r_{\max}$
        \end{itemize}

    \item \textbf{Soft QBound hyperparameter:}
        \begin{itemize}
            \item Penalty weight: $\lambda = 0.1$ to $1.0$ (start with 0.1)
            \item Loss: $\mathcal{L}_{\text{QBound}} = \lambda[\max(0, Q-Q_{\max})^2 + \max(0, Q_{\min}-Q)^2]$
        \end{itemize}

    \item \textbf{Integration approach:}
        \begin{itemize}
            \item \textit{Hard QBound:} Clip during target computation (3-5 lines of code)
            \item \textit{Soft QBound:} Add penalty to loss function (5-10 lines of code)
            \item Combine with existing methods (Double-Q, target networks) for complementary benefits
        \end{itemize}
\end{enumerate}

\subsection{Final Remarks}

QBound represents a simple yet principled approach to improving reinforcement learning through environment-aware stabilization. By enforcing theoretically-derived bounds through bootstrapping-based clipping, QBound stabilizes value-based methods in positive dense reward environments where Q-values can grow unbounded.

For the reinforcement learning community, QBound offers a practical tool that can be immediately applied to existing algorithms with minimal modification. \textbf{Our comprehensive 5-seed evaluation reveals that QBound is most effective for positive dense reward environments}, achieving consistent improvements on CartPole (+12\% to +33.6\% across DQN variants) with Dueling DQN showing 100\% win rate. However, QBound provides no benefit or causes degradation on negative reward and sparse terminal reward environments.

\textbf{Key insights from comprehensive 5-seed evaluation:}
\begin{enumerate}
    \item \textbf{Reward-sign dependent effectiveness:} QBound works for positive dense rewards (CartPole: +12\% to +33.6\%) but fails for negative rewards (Pendulum: -3\% to -8\%, MountainCar: -47\%)

    \item \textbf{Dueling DQN is most reliable:} Among DQN variants, Dueling DQN with QBound shows 100\% win rate (5/5 seeds improve) with +22.5\% mean improvement

    \item \textbf{Not universally beneficial:} QBound is not a universal improvement. It works best for positive dense rewards but causes degradation on negative reward environments (underlying cause remains an open question)

    \item \textbf{Seed sensitivity:} Even in successful cases (CartPole DQN), 20\% of seeds show degradation. Always run multiple seeds before deployment
\end{enumerate}

\textbf{Practical guidance:} Use QBound for positive dense reward environments with Dueling DQN architecture (100\% win rate). For standard DQN, expect 80\% win rate with some seeds degrading. Do not use QBound for negative reward environments---it provides no benefit and often causes degradation.

\textbf{Important caveats:}
\begin{itemize}
    \item \textbf{Implementation choice is critical:} QBound's success depends on choosing the right variant:
        \begin{itemize}
            \item \textit{Hard QBound (clipping):} Use for discrete actions (DQN: +12-34\% on CartPole)
            \item \textit{Soft QBound (penalty):} Required for actor-critic methods with continuous actions
        \end{itemize}

    \item \textbf{Continuous action compatibility:} QBound for actor-critic methods requires Architectural QBound (output activation constraints) to maintain gradient flow:
        \begin{itemize}
            \item \textit{Hard clipping:} Incompatible—disrupts critic gradients required for policy learning
            \item \textit{Architectural QBound:} Mixed results (DDPG: -8\%, TD3: +4\% on Pendulum, 5-seed validated)
        \end{itemize}

    \item \textbf{Algorithm-specific tuning required:} QBound shows strong algorithm dependence:
        \begin{itemize}
            \item \textit{Excellent:} DQN variants on positive rewards (+12-34\% CartPole)
            \item \textit{Mixed:} DDPG/TD3 with Architectural QBound (DDPG: -8\%, TD3: +4\% Pendulum)
            \item \textit{Fails:} DQN on negative rewards (cause remains open question)
        \end{itemize}

    \item \textbf{Environment characteristics matter:} Not universally beneficial:
        \begin{itemize}
            \item \textit{Best:} Positive dense rewards with known bounds (CartPole: +12-34\%)
            \item \textit{Hurts:} Negative rewards (MountainCar: -47.4\%, Pendulum DQN: -3.3\%)
            \item \textit{No benefit:} Sparse terminal rewards (GridWorld: -1\%, FrozenLake: -1.7\%)
        \end{itemize}

    \item \textbf{Known bounds required:} QBound requires reasonably tight bounds derivable from environment structure
\end{itemize}

\textbf{Bottom line:} QBound provides consistent improvements (+12\% to +33.6\%) on positive dense reward environments like CartPole, with Dueling DQN showing 100\% win rate across 5 seeds. However, it's not a universal solution---QBound fails on negative reward environments and provides no benefit on sparse terminal rewards. Apply it selectively to positive dense reward environments for maximum benefit.

\section*{Open Research and Contributions}

This is an \textbf{ongoing research project}. The central open question---why QBound works for positive but not negative rewards---remains unsolved. We invite the research community to contribute experiments, analysis, and theoretical insights. Promising directions include: (1) Q-value transformation approaches for negative rewards, (2) testing on additional benchmarks (Atari, MuJoCo), and (3) investigating the interaction between network initialization and reward sign.

\textbf{Repository:} \url{https://github.com/tzemuy13/QBound}

\section*{Acknowledgments}

This research was conducted by the author with Claude (Anthropic) serving as an AI coding and research assistant. Claude assisted with code implementation, experimental design, data analysis, and manuscript preparation. All research direction, core ideas, and final decisions were made by the author.

We acknowledge the open-source RL community for providing the foundational implementations that made this research possible. Special thanks to the maintainers of OpenAI Gym \citep{brockman2016openai}, Stable-Baselines3 \citep{raffin2021stable}, and Spinning Up \citep{SpinningUp2018} for creating the tools that enabled this evaluation.

\section*{Reproducibility Statement}

All code, hyperparameters, and experimental configurations are publicly available at \url{https://github.com/tzemuy13/QBound}. The repository includes: (1) complete implementations for all environments with documented hyperparameters, (2) deterministic seeding protocol (seeds 42-46) ensuring exact reproducibility, and (3) detailed experiment scripts with environment-specific configurations. Our implementation builds on PyTorch, OpenAI Gym \citep{brockman2016openai}, and Gymnasium, following established experimental protocols. All random seeds, network architectures, and training procedures are explicitly documented in the codebase to enable exact replication of our results.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}