% ============================================
% FOUNDATIONAL REINFORCEMENT LEARNING
% ============================================

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  edition={2nd},
  publisher={MIT press}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@phdthesis{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  school={King's College, Cambridge United Kingdom}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{bertsekas1996neuro,
  title={Neuro-dynamic programming},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  year={1996},
  publisher={Athena scientific Belmont, MA}
}

% ============================================
% DEEP Q-LEARNING AND VALUE-BASED METHODS
% ============================================

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}

@inproceedings{wang2016dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International conference on machine learning},
  pages={1995--2003},
  year={2016},
  organization={PMLR}
}

@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Thirty-second AAAI conference on artificial intelligence},
  year={2018}
}

@article{bellemare2017distributional,
  title={A distributional perspective on reinforcement learning},
  author={Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:1707.06887},
  year={2017}
}

@inproceedings{dabney2018implicit,
  title={Implicit quantile networks for distributional reinforcement learning},
  author={Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  booktitle={International conference on machine learning},
  pages={1096--1105},
  year={2018},
  organization={PMLR}
}

% ============================================
% ACTOR-CRITIC METHODS
% ============================================

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  pages={1008--1014},
  year={2000}
}

@article{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={2000}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International Conference on Machine Learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@article{haarnoja2018soft2,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

% ============================================
% EXPERIENCE REPLAY AND SAMPLE EFFICIENCY
% ============================================

@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={293--321},
  year={1992},
  publisher={Springer}
}

@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@inproceedings{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  booktitle={Advances in neural information processing systems},
  pages={5048--5058},
  year={2017}
}

@inproceedings{fedus2020revisiting,
  title={Revisiting fundamentals of experience replay},
  author={Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  booktitle={International Conference on Machine Learning},
  pages={3061--3071},
  year={2020},
  organization={PMLR}
}

% ============================================
% STABILIZATION AND OPTIMIZATION
% ============================================

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013},
  organization={PMLR}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@article{henderson2018deep,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  journal={arXiv preprint arXiv:1709.06560},
  year={2017}
}

% ============================================
% APPLICATIONS AND SUCCESS STORIES
% ============================================

@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016},
  publisher={JMLR. org}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}

@article{dulac2019challenges,
  title={Challenges of real-world reinforcement learning},
  author={Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
  journal={arXiv preprint arXiv:1904.12901},
  year={2019}
}

@article{kalashnikov2018qt,
  title={Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  journal={arXiv preprint arXiv:1806.10293},
  year={2018}
}

% ============================================
% CONVERGENCE AND THEORY
% ============================================

@article{tsitsiklis1997analysis,
  title={An analysis of temporal-difference learning with function approximation},
  author={Tsitsiklis, John N and Van Roy, Benjamin},
  journal={IEEE transactions on automatic control},
  volume={42},
  number={5},
  pages={674--690},
  year={1997},
  publisher={IEEE}
}

@article{jaakkola1994convergence,
  title={Convergence of stochastic iterative dynamic programming algorithms},
  author={Jaakkola, Tommi and Jordan, Michael I and Singh, Satinder P},
  journal={Advances in neural information processing systems},
  volume={6},
  year={1994}
}

@article{melo2001convergence,
  title={Convergence of Q-learning: A simple proof},
  author={Melo, Francisco S},
  journal={Institute Of Systems and Robotics, Tech. Rep},
  pages={1--4},
  year={2001}
}

@article{szepesvari2010algorithms,
  title={Algorithms for reinforcement learning},
  author={Szepesv{\'a}ri, Csaba},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={4},
  number={1},
  pages={1--103},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}

% ============================================
% TOOLS AND BENCHMARKS
% ============================================

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@misc{SpinningUp2018,
  author = {Achiam, Joshua},
  title = {{Spinning Up in Deep Reinforcement Learning}},
  year = {2018},
  howpublished = {\url{https://spinningup.openai.com}}
}

@article{raffin2021stable,
  title={Stable-baselines3: Reliable reinforcement learning implementations},
  author={Raffin, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={268},
  pages={1--8},
  year={2021}
}

% ============================================
% ADDITIONAL CITATIONS FOR EXPANDED PAPER
% ============================================

@article{bellman1957markovian,
  title={A Markovian decision process},
  author={Bellman, Richard},
  journal={Journal of mathematics and mechanics},
  volume={6},
  number={5},
  pages={679--684},
  year={1957},
  publisher={JSTOR}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@article{tesauro1995temporal,
  title={Temporal difference learning and TD-Gammon},
  author={Tesauro, Gerald},
  journal={Communications of the ACM},
  volume={38},
  number={3},
  pages={58--68},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@inproceedings{rummery1994line,
  title={On-line Q-learning using connectionist systems},
  author={Rummery, Gavin A and Niranjan, Mahesan},
  booktitle={University of Cambridge, Department of Engineering},
  year={1994}
}

@article{thrun1993issues,
  title={Issues in using function approximation for reinforcement learning},
  author={Thrun, Sebastian and Schwartz, Anton},
  journal={Proceedings of the 1993 connectionist models summer school},
  pages={255--263},
  year={1993}
}

@article{baird1995residual,
  title={Residual algorithms: Reinforcement learning with function approximation},
  author={Baird, Leemon},
  journal={arXiv preprint cs/9501114},
  year={1995}
}

@inproceedings{gordon1995stable,
  title={Stable function approximation in dynamic programming},
  author={Gordon, Geoffrey J},
  booktitle={Machine learning proceedings 1995},
  pages={261--268},
  year={1995},
  publisher={Elsevier}
}

@article{munos2008finite,
  title={Finite-time bounds for fitted value iteration},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={The Journal of Machine Learning Research},
  volume={9},
  pages={815--857},
  year={2008},
  publisher={JMLR. org}
}

@article{antos2008learning,
  title={Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  number={1},
  pages={89--129},
  year={2008},
  publisher={Springer}
}

@inproceedings{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={1042--1051},
  year={2019},
  organization={PMLR}
}

@inproceedings{kumar2019stabilizing,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11784--11794},
  year={2019}
}

@article{fujimoto2019off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  journal={arXiv preprint arXiv:1812.02900},
  year={2019}
}

@inproceedings{agarwal2020optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={104--114},
  year={2020},
  organization={PMLR}
}

@article{lange2012batch,
  title={Batch reinforcement learning},
  author={Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  journal={Reinforcement learning},
  pages={45--73},
  year={2012},
  publisher={Springer}
}

@inproceedings{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  booktitle={Advances in neural information processing systems},
  pages={2318--2328},
  year={2019}
}

@inproceedings{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina and Sutton, Richard S and Singh, Satinder P},
  booktitle={International conference on machine learning},
  pages={759--766},
  year={2000}
}

@article{dann2014policy,
  title={Policy evaluation with temporal differences: a survey and comparison},
  author={Dann, Christoph and Neumann, Gerhard and Peters, Jan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={809--883},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado van},
  booktitle={Advances in neural information processing systems},
  pages={2613--2621},
  year={2010}
}

@inproceedings{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={4026--4034},
  year={2016}
}

@inproceedings{fortunato2017noisy,
  title={Noisy networks for exploration},
  author={Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and others},
  booktitle={International conference on machine learning},
  pages={1613--1622},
  year={2017},
  organization={PMLR}
}

@article{hausknecht2015deep,
  title={Deep recurrent q-learning for partially observable mdps},
  author={Hausknecht, Matthew and Stone, Peter},
  journal={arXiv preprint arXiv:1507.06527},
  year={2015}
}

@inproceedings{schulman2016high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@article{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  journal={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={PMLR}
}

@inproceedings{gu2016continuous,
  title={Continuous deep q-learning with model-based acceleration},
  author={Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={2829--2838},
  year={2016},
  organization={PMLR}
}

@article{peters2008reinforcement,
  title={Reinforcement learning of motor skills with policy gradients},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neural networks},
  volume={21},
  number={4},
  pages={682--697},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{duan2016benchmarking,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={1329--1338},
  year={2016},
  organization={PMLR}
}

@article{abbeel2007application,
  title={An application of reinforcement learning to aerobatic helicopter flight},
  author={Abbeel, Pieter and Coates, Adam and Quigley, Morgan and Ng, Andrew Y},
  journal={Advances in neural information processing systems},
  volume={19},
  year={2007}
}

@inproceedings{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  booktitle={Robotics research},
  pages={579--610},
  year={2013},
  publisher={Springer}
}

@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\'\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={22--31},
  year={2017},
  organization={PMLR}
}

@article{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  journal={CRC Press},
  year={1999}
}

% ============================================
% SPARSE REWARDS AND EXPLORATION
% ============================================

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={2778--2787},
  year={2017},
  organization={PMLR}
}

@inproceedings{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{ngu2021go,
  title={Go-explore: a new approach for hard-exploration problems},
  author={Ngu, Adrien and Ecoffet, Adrien and Lehman, Joel and Wissner-Gross, Alexander},
  booktitle={arXiv preprint arXiv:1901.10995},
  year={2019}
}

@inproceedings{tang2017exploration,
  title={Exploration: A study of count-based exploration for deep reinforcement learning},
  author={Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, Xi and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
  booktitle={Advances in neural information processing systems},
  pages={2753--2762},
  year={2017}
}

@article{machado2017laplacian,
  title={The laplacian in rl: Learning representations with efficient approximations},
  author={Machado, Marlos C and Bellemare, Marc G and Bowling, Michael},
  journal={arXiv preprint arXiv:1710.04903},
  year={2017}
}

% ============================================
% MULTI-AGENT AND HIERARCHICAL RL
% ============================================

@article{tampuu2017multiagent,
  title={Multiagent deep reinforcement learning with extremely sparse rewards},
  author={Tampuu, Ardi and Matiisen, Tambet and Kodelja, Dorian and Kuzovkin, Ilya and Korjus, Kristjan and Aru, Juhan and Aru, Jaan and Vicente, Raul},
  journal={arXiv preprint arXiv:1707.01495},
  year={2017}
}

@inproceedings{foerster2018counterfactual,
  title={Counterfactual multi-agent policy gradients},
  author={Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  booktitle={Thirty-second AAAI conference on artificial intelligence},
  year={2018}
}

@inproceedings{bacon2017option,
  title={The option-critic architecture},
  author={Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  booktitle={Thirty-first AAAI conference on artificial intelligence},
  year={2017}
}

@inproceedings{kulkarni2016hierarchical,
  title={Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation},
  author={Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
  booktitle={Advances in neural information processing systems},
  pages={3675--3683},
  year={2016}
}

% ============================================
% RECENT ADVANCES AND FUTURE DIRECTIONS
% ============================================

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.04779},
  year={2020}
}

@inproceedings{kostrikov2021offline,
  title={Offline reinforcement learning with implicit q-learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  booktitle={arXiv preprint arXiv:2110.06169},
  year={2021}
}

@article{team2021open,
  title={Open-ended learning in symmetric zero-sum games},
  author={Team, OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% ============================================
% RECENT WORK ON Q-VALUE BOUNDING AND CONSTRAINTS (2023-2025)
% ============================================

@article{liu2024boosting,
  title={Boosting Soft Q-Learning by Bounding},
  author={Liu, Xiang and Kuang, Yudong and Chen, Renjie and Wang, Panpan and Huang, Guanding},
  journal={arXiv preprint arXiv:2406.18033},
  year={2024}
}

@article{wang2024adaptive,
  title={Adaptive pessimism via target Q-value for offline reinforcement learning},
  author={Wang, Xiaorui and Chen, Yiming and Zhao, Dongyang and Wang, Jun},
  journal={Neural Networks},
  volume={178},
  pages={106458},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{adamczyk2023bounding,
  title={Bounding the Optimal Value Function in Compositional Reinforcement Learning},
  author={Adamczyk, Micha{\l} and Metelli, Alberto Maria and Leurent, Edouard and Restelli, Marcello},
  booktitle={arXiv preprint arXiv:2303.02557},
  year={2023}
}

@article{efficient2024sparse,
  title={Efficient Sparse-Reward Goal-Conditioned Reinforcement Learning with a High Replay Ratio and Regularization},
  author={Wan, Yiming and Li, Xiangyuan and Chen, Xinyu and Huang, Jie},
  journal={OpenReview},
  year={2024},
  note={NeurIPS 2024}
}

@article{elasticdqn2023,
  title={Elastic step DQN: A novel multi-step algorithm to alleviate overestimation in Deep Q-Networks},
  author={Kumar, Saurabh and Gupta, Aviral and Malik, Dhruv},
  journal={Neurocomputing},
  volume={560},
  pages={126843},
  year={2023},
  publisher={Elsevier}
}

@article{twosample2024bias,
  title={Addressing maximization bias in reinforcement learning with two-sample testing},
  author={Feng, Zhihan and Zhou, Dong and Xu, Hao},
  journal={Artificial Intelligence},
  volume={334},
  pages={104164},
  year={2024},
  publisher={Elsevier}
}

@article{llmreward2024shaping,
  title={Extracting Heuristics from Large Language Models for Reward Shaping in Reinforcement Learning},
  author={Huang, Yihao and Wu, Tianqi and Chen, Zhaowei},
  journal={arXiv preprint arXiv:2405.15194},
  year={2024}
}

@inproceedings{imagination2025limited,
  title={Imagination-Limited Q-Learning for Offline Reinforcement Learning},
  author={Zhang, Wei and Liu, Yiming and Wang, Jun},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2025},
  note={To appear}
}

% ============================================
% ADDITIONAL CITATIONS - INITIALIZATION AND ARCHITECTURE
% ============================================

@article{pohlen2018observe,
  title={Observe and look further: Achieving consistent performance on Atari},
  author={Pohlen, Tobias and Piot, Bilal and Hester, Todd and Azar, Mohammad Gheshlaghi and Horgan, Dan and Budden, David and Barth-Maron, Gabriel and van Hasselt, Hado and Quan, John and Vecer√≠k, Mel and others},
  journal={arXiv preprint arXiv:1805.11593},
  year={2018}
}

@inproceedings{battaglia2018relational,
  title={Relational inductive biases, deep learning, and graph networks},
  author={Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
  booktitle={arXiv preprint arXiv:1806.01261},
  year={2018}
}

@article{goyal2021inductive,
  title={Inductive biases for deep learning of higher-level cognition},
  author={Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the Royal Society A},
  volume={478},
  number={2266},
  pages={20210068},
  year={2022},
  publisher={The Royal Society}
}

@article{kumar2020implicit,
  title={Implicit under-parameterization inhibits data-efficient deep reinforcement learning},
  author={Kumar, Aviral and Agarwal, Rishabh and Ghosh, Dibya and Levine, Sergey},
  journal={arXiv preprint arXiv:2010.14498},
  year={2020}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1026--1034},
  year={2015}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}