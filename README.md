# QBound - Q-Value Bounding for Deep Reinforcement Learning

A research project implementing QBound, a technique for bounding Q-values in Deep Q-Networks (DQN) to improve learning in sparse reward environments.

## ğŸ“ Project Structure

```
QBound/
â”œâ”€â”€ src/                          # Core implementation
â”‚   â”œâ”€â”€ dqn_agent.py             # DQN agent with QBound
â”‚   â””â”€â”€ environment.py           # GridWorld environment
â”‚
â”œâ”€â”€ experiments/                  # Experiment scripts
â”‚   â”œâ”€â”€ gridworld/               # GridWorld experiments
â”‚   â”‚   â””â”€â”€ train_gridworld.py   # Train on GridWorld (10x10 grid)
â”‚   â”œâ”€â”€ frozenlake/              # FrozenLake experiments
â”‚   â”‚   â””â”€â”€ train_frozenlake.py  # Train on FrozenLake (4x4, slippery)
â”‚   â”œâ”€â”€ cartpole/                # CartPole experiments
â”‚   â”‚   â””â”€â”€ train_cartpole.py    # Train on CartPole (balance task)
â”‚   â””â”€â”€ combined/                # Run all experiments
â”‚       â””â”€â”€ run_all_experiments.py  # Run all 3 environments
â”‚
â”œâ”€â”€ analysis/                     # Analysis and visualization
â”‚   â”œâ”€â”€ analyze_results.py       # Analyze experiment results
â”‚   â”œâ”€â”€ comprehensive_analysis.py # Full analysis with plots
â”‚   â”œâ”€â”€ qbound_summary_table.py  # Generate summary tables
â”‚   â”œâ”€â”€ show_qbound_config.py    # Show configuration analysis
â”‚   â”œâ”€â”€ track_q_values.py        # Track Q-value statistics
â”‚   â”œâ”€â”€ generate_plot.py         # Generate plots for paper
â”‚   â””â”€â”€ update_paper_with_results.py  # Update paper with results
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ ANALYSIS_SUMMARY.md      # Full analysis of results
â”‚   â”œâ”€â”€ CHANGES.md               # Code change log
â”‚   â””â”€â”€ explain_aux_weight.md    # Explanation of aux_weight parameter
â”‚
â”œâ”€â”€ results/                      # Experiment results
â”‚   â”œâ”€â”€ gridworld/               # GridWorld results
â”‚   â”œâ”€â”€ frozenlake/              # FrozenLake results
â”‚   â”œâ”€â”€ cartpole/                # CartPole results
â”‚   â”œâ”€â”€ combined/                # Combined results
â”‚   â””â”€â”€ plots/                   # Generated plots
â”‚
â”œâ”€â”€ CLAUDE.md                     # Project instructions for Claude
â””â”€â”€ README.md                     # This file
```

## ğŸ¯ Environments

### 1. GridWorld (10x10)
- **File:** `experiments/gridworld/train_gridworld.py`
- **Environment:** Custom 10x10 grid, start at (0,0), goal at (9,9)
- **Reward:** +1 for reaching goal, 0 otherwise
- **QBound Config:** Q_min=0.0, Q_max=1.0, Î³=0.99
- **Episodes:** 500
- **Status:** âŒ QBound underperforms (-22.1%)

### 2. FrozenLake (4x4, Slippery)
- **File:** `experiments/frozenlake/train_frozenlake.py`
- **Environment:** Gymnasium FrozenLake-v1 (stochastic)
- **Reward:** +1 for reaching goal, 0 otherwise
- **QBound Config:** Q_min=0.0, Q_max=1.0, Î³=0.95
- **Episodes:** 2000
- **Status:** âœ… QBound works! (+19.4% faster convergence)

### 3. CartPole
- **File:** `experiments/cartpole/train_cartpole.py`
- **Environment:** Gymnasium CartPole-v1 (balance pole)
- **Reward:** +1 per timestep survived (max 500)
- **QBound Config:** Q_min=0.0, Q_max=100.0, Î³=0.99
- **Episodes:** 500
- **Status:** âŒ QBound severely underperforms (-41.4%)

## ğŸš€ Quick Start

### Run Individual Experiments

```bash
# GridWorld
cd /root/projects/QBound
python experiments/gridworld/train_gridworld.py

# FrozenLake
python experiments/frozenlake/train_frozenlake.py

# CartPole
python experiments/cartpole/train_cartpole.py
```

### Run All Experiments

```bash
python experiments/combined/run_all_experiments.py
```

### Analyze Results

```bash
# Quick summary
python analysis/qbound_summary_table.py

# Detailed analysis
python analysis/analyze_results.py

# Full analysis with plots
python analysis/comprehensive_analysis.py

# Show Q-value configuration
python analysis/show_qbound_config.py
```

## ğŸ“Š Key Results

| Environment | QBound Episodes | Baseline Episodes | Performance |
|------------|----------------|------------------|-------------|
| GridWorld  | 326            | 267              | -22.1% âŒ   |
| FrozenLake | 203            | 252              | +19.4% âœ…   |
| CartPole   | N/A            | N/A              | -41.4% âŒ   |

### Key Findings

1. **QBound works well in stochastic environments** (FrozenLake âœ…)
2. **QBound struggles with high discount factors** (GridWorld âŒ)
3. **QBound fails when Q_max is too restrictive** (CartPole âŒ)

## ğŸ”§ Core Components

### DQN Agent (`src/dqn_agent.py`)

Implements DQN with optional QBound using dual-loss training:

**Primary Loss:** Standard TD loss for learning optimal Q-values
```python
primary_loss = MSE(Q(s,a), r + Î³ * max_a' Q(s',a'))
```

**Auxiliary Loss:** Penalizes only Q-values that violate [Q_min, Q_max]
```python
violation_mask = (Q < Q_min) | (Q > Q_max)
aux_loss = MSE(Q[violation_mask], clip(Q[violation_mask]))
```

**Combined Loss:**
```python
total_loss = primary_loss + aux_weight * aux_loss
```

### Key Parameters

- `use_qclip`: Enable/disable QBound (True/False)
- `qclip_min`: Lower bound for Q-values
- `qclip_max`: Upper bound for Q-values
- `aux_weight`: Weight for auxiliary loss (default: 0.5)
- `gamma`: Discount factor

## ğŸ“ˆ Recent Changes

### v2.0 - Fixed Auxiliary Loss (2025-10-25)

**Changed:** Auxiliary loss now clips only violating Q-values instead of scaling all actions

**Before:**
- When one action violated bounds, ALL actions were scaled proportionally
- Problem: Punished good learners for one bad action

**After:**
- Only Q-values that violate bounds are clipped
- Benefit: Well-behaved actions remain unchanged

See `docs/CHANGES.md` for details.

## ğŸ“ Documentation

- **docs/ANALYSIS_SUMMARY.md** - Comprehensive analysis of all experiments
- **docs/CHANGES.md** - Code change history
- **docs/explain_aux_weight.md** - Detailed explanation of aux_weight parameter

## âš ï¸ Known Issues

1. **Q_max values are incorrectly set** - Based on step rewards instead of episode returns
2. **CartPole severely limited** - Q_max=100 but optimal return â‰ˆ500
3. **GridWorld value propagation** - Q_max=1.0 prevents proper learning

## ğŸ”® Future Work

1. Fix Q_max values based on maximum episode returns
2. Experiment with different aux_weight values (0.0 to 1.0)
3. Test with various discount factors
4. Add more environments (Atari, MuJoCo)
5. Implement adaptive Q_max bounds

## ğŸ“„ Citation

```bibtex
@article{qbound2025,
  title={QBound: Q-Value Bounding for Deep Reinforcement Learning},
  author={...},
  year={2025}
}
```
