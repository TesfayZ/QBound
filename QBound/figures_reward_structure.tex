% FIGURES: Reward Structure Analysis
% Insert after the theoretical section on reward sign differentiation

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{figures/reward_structure_analysis.pdf}
\caption{\textbf{Reward Structure Analysis: Why QBound Effectiveness Depends on Reward Type.}
\textbf{Top row:} Reward patterns over time. (a) Sparse terminal rewards (GridWorld/FrozenLake) provide single reward at episode end. (b) Dense positive rewards (CartPole) provide $r=+1$ per timestep. (c) Dense negative rewards (Pendulum) provide $r \in [-16, 0]$ per timestep.
\textbf{Middle row:} Theoretical Q-value bounds. (d) Sparse rewards have \textit{constant} $Q_{\max} = 1$ (no time dependence). (e) Dense positive rewards have \textit{decreasing} $Q_{\max}(t)$ as remaining potential diminishes. (f) Dense negative rewards have \textit{constant} $Q_{\max} = 0$ (naturally enforced by Bellman equation with $r \leq 0$).
\textbf{Bottom row:} Empirical validation. (g) Overestimation risk: Dense positive rewards have highest risk (85\%) due to unbounded accumulation. (h) Violation rates without QBound: CartPole shows 12.5\% violations of theoretical bounds, while Pendulum shows 0.0\% violations of $Q > 0$ (upper bound naturally satisfied). (i) QBound effectiveness: Strong improvement (+22.5\%) for dense positive, no benefit or degradation for sparse ($-1.0\%$) and dense negative ($-7.0\%$). Results averaged over 5 seeds.}
\label{fig:reward-structure-analysis}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{figures/q_bound_theory_comparison.pdf}
\caption{\textbf{Theoretical Q-Value Bound Computation and Comparison.}
\textbf{(Top-left):} Dense positive rewards: $Q_{\max}(t) = \frac{1-\gamma^{H-t}}{1-\gamma}$ decreases with timestep $t$ as remaining episode potential diminishes. Higher $\gamma$ values lead to higher Q-values. For CartPole with $\gamma=0.99$, $H=500$: $Q_{\max}(0) \approx 100$ at start, $Q_{\max}(499) = 1$ at end.
\textbf{(Top-right):} Sparse terminal rewards: $Q_{\max} = 1$ remains constant (episode terminates upon reaching goal). No time dependence because reward structure is state-based, not time-based.
\textbf{(Bottom-left):} Dense negative rewards: $Q_{\max} = 0$ remains constant. The Bellman equation with $r \leq 0$ naturally constrains $Q(s,a) = r + \gamma \max Q(s',a') \leq 0$ through recursive bootstrapping. Network learns this constraint through statistical learning (0.0000 empirical violations).
\textbf{(Bottom-right):} Summary table showing reward type determines QBound applicability. Dense positive rewards have \textit{decreasing} bounds and high overestimation risk (85\%), making QBound highly effective (+12-34\% improvement). Sparse and dense negative rewards have \textit{constant} bounds with low risk, making QBound ineffective or harmful.}
\label{fig:q-bound-theory-comparison}
\end{figure*}

% Additional figure: Multi-seed learning curves (reference existing plots)
\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{figures/cartpole_dqn_full_qbound_5seed.pdf}
    \caption{CartPole DQN (Dense Positive): QBound helps}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{figures/pendulum_dqn_full_qbound_5seed.pdf}
    \caption{Pendulum DQN (Dense Negative): QBound hurts}
\end{subfigure}
\caption{\textbf{Learning Curves Demonstrating Reward Sign Effect (5 seeds, mean $\pm$ std).} (a) CartPole with positive rewards (+1/step): Static QBound consistently improves both DQN and DDQN by preventing Q-value overestimation. (b) Pendulum with negative rewards ($r \in [-16,0]$): Static QBound degrades performance for both DQN and DDQN because the upper bound $Q \leq 0$ is naturally satisfied by the Bellman equation, making explicit bounding redundant.}
\label{fig:learning-curves-reward-sign}
\end{figure*}
