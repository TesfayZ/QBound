\documentclass[11pt]{article}

\usepackage{arxiv}
\usepackage{natbib}  % For \citep{} and \citet{} commands
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}

% Theorem environments
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\argmax}{\operatorname*{argmax}}
\newcommand{\argmin}{\operatorname*{argmin}}
\DeclareMathOperator{\clip}{clip}

% Keywords command
\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}

% Code listings setup
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    language=Python
}

\title{QBound: Environment-Aware Q-Value Bounds for Stable Temporal Difference Learning}

\author{
  Tesfay \\
  \texttt{tzemuy13@gmail.com} \\[0.5em]
  \textit{Working Draft -- Contributions Welcome} \\
  \url{https://github.com/tzemuy13/QBound}
}

\begin{document}

\maketitle

\begin{abstract}
Value-based reinforcement learning methods suffer from overestimation bias \citep{thrun1993issues, van2016deep}, where bootstrapped Q-value estimates systematically exceed true values, causing instability and poor sample efficiency. We present QBound, a stabilization mechanism that exploits environment structure to prevent overestimation by deriving and enforcing Q-value bounds from known reward structures. QBound addresses overestimation at its source (bootstrapped targets) by clipping next-state Q-values to environment-specific bounds $[Q_{\min}, Q_{\max}]$, which propagate naturally through temporal difference learning.

\textbf{Scope:} QBound targets \textit{off-policy} value-based methods (DQN, DDQN, Dueling DQN) and off-policy actor-critic methods (DDPG, TD3) that use experience replay. On-policy methods (e.g., PPO, A2C, REINFORCE) are outside QBound's scope because they do not suffer from the same overestimation dynamics: on-policy sampling avoids stale replay buffer data, the value function $V(s)$ has no max operator in its update, and these methods already include built-in value stabilization mechanisms.

Comprehensive evaluation across 8 environments with 5 independent random seeds (40 runs total) reveals that QBound's effectiveness fundamentally depends on reward sign, with significant seed-dependent variability. For positive dense reward environments (e.g., CartPole: $r = +1$ per timestep), QBound achieves mean improvements of 12\% to 22\% across DQN variants, but per-seed analysis reveals inconsistency: CartPole Dueling shows 100\% win rate (5/5 seeds improve), while CartPole DQN shows only 80\% win rate (4/5 seeds improve, with one seed degrading by 8.7\%). Neural networks with linear output layers have no architectural constraint on positive values, making explicit upper bounds beneficial on average.

For negative reward environments (Pendulum: $r \in [-16, 0]$), we tested architectural QBound (output activation: $Q = -\text{softplus}(\text{logits})$) across off-policy algorithms. QBound degrades performance for most algorithms: architectural QBound degrades DQN by 3.3\%, Double DQN by 7.1\%, and DDPG by 8.0\%. TD3 is an exception, showing 4.1\% improvement with architectural QBound ($-183.25 \rightarrow -175.66$) but with 72\% higher variance ($\pm 40.15$ vs $\pm 23.36$), suggesting a unique interaction with TD3's twin critic architecture.

The fundamental question of why QBound works for positive but not negative rewards remains open. Both $Q_{\max}$ (positive rewards) and $Q_{\max}=0$ (negative rewards) are theoretical upper bounds derived from cumulative discounted rewards. We propose several hypotheses for future investigation: (1) network initialization bias interacts differently with positive vs negative targets, (2) gradient flow patterns differ across value ranges, (3) replay buffer composition effects, (4) exploration strategy interactions. Answering this question requires controlled ablation studies beyond the scope of this work.

This work provides: (1) systematic per-seed analysis across 40 runs showing QBound's effectiveness varies by algorithm (100\% win rate for Dueling DQN, 80\% for standard DQN), (2) comprehensive negative results for negative rewards (40-60\% win rates, no better than chance), (3) honest reporting of seed-dependent failures even in successful cases, (4) clear recommendations based on win rates rather than mean improvements alone. Implementation imposes negligible overhead ($<$2\%). All experiments use 5 seeds with full reproducibility protocols.

\textbf{Recommendations:} (1) Use hard clipping QBound for positive dense rewards with Dueling DQN (100\% win rate, +22.5\% mean improvement). (2) For standard DQN on positive rewards, QBound helps on average (+12\%) but expect 20\% of seeds to degrade. (3) Do not use QBound for negative rewards---win rates are 40-60\% (no better than chance) with high variance. (4) QBound is designed for off-policy methods only; on-policy methods are outside scope. QBound is a specialized technique for positive dense rewards, not a universal improvement, and practitioners should run multiple seeds before deployment.
\end{abstract}

\keywords{Reinforcement Learning \and Temporal Difference Learning \and Q-Learning \and Overestimation Bias \and Value Stability \and Sample Efficiency}

\section{Introduction}

\subsection{Motivation: Instability in Value Learning Limits Sample Efficiency}

Reinforcement learning has achieved remarkable successes in games \citep{mnih2015human}, robotics \citep{levine2016end}, and complex decision-making tasks \citep{vinyals2019grandmaster}. However, a critical bottleneck remains: \textbf{sample efficiency}—the number of environment interactions required to learn effective policies. In many real-world applications, environment samples are the limiting resource:

\begin{itemize}
    \item \textbf{Robotics:} Physical interactions cost time, energy, and risk hardware damage \citep{kalashnikov2018qt}
    \item \textbf{Clinical trials:} Patient interactions are limited by enrollment, ethics, and cost \citep{dulac2019challenges}
    \item \textbf{Financial trading:} Historical data is finite, live testing is risky
    \item \textbf{Industrial control:} Plant operations are expensive and safety-critical \citep{dulac2019challenges}
    \item \textbf{Autonomous vehicles:} Real-world testing is dangerous and expensive
    \item \textbf{Game design:} Human playtesting is time-consuming and costly
\end{itemize}

Current deep RL methods vary dramatically in sample efficiency \citep{duan2016benchmarking, SpinningUp2018}. Pure policy gradient methods like REINFORCE \citep{williams1992simple} require 50M-100M+ environment steps due to high variance gradient estimates \citep{schulman2015trust}. Actor-critic methods like DDPG \citep{lillicrap2015continuous}, TD3 \citep{fujimoto2018addressing}, and SAC \citep{haarnoja2018soft} achieve 2M-20M steps by combining policy gradients with value function learning \citep{haarnoja2018soft2}. Pure value-based methods like DQN \citep{mnih2015human} and its variants achieve the highest sample efficiency at 1M-10M steps through bootstrap learning with experience replay \citep{lin1992self, mnih2015human}.

The sample efficiency hierarchy correlates directly with whether methods learn value functions. This suggests that improving value function learning improves sample efficiency across the entire spectrum of methods that use critics. However, value-based methods face a fundamental challenge: bootstrapped Q-value estimates suffer from instability and overestimation bias \citep{thrun1993issues, van2016deep}, which directly undermines their sample efficiency advantage. Stabilizing value learning is therefore essential for achieving better sample efficiency.

\subsection{The Bootstrapping Instability Problem}

All methods that learn value functions face a fundamental challenge: bootstrapping with imperfect function approximators produces unbounded, inconsistent value estimates \citep{tsitsiklis1997analysis}. During training, Q-values frequently:
\begin{enumerate}
    \item Diverge to arbitrary magnitudes ($Q(s,a) \to \pm\infty$)
    \item Violate theoretical constraints (e.g., $Q(s,a) > Q_{\max}$ when $Q_{\max}$ is derivable from environment structure)
    \item Exhibit high variance in bootstrap targets, leading to unstable learning
    \item Create poorly scaled gradient signals that slow convergence
\end{enumerate}

Prior stabilization work includes target networks \citep{mnih2015human}, clipped double-Q \citep{fujimoto2018addressing}, reward clipping \citep{mnih2013playing}, and gradient clipping \citep{pascanu2013difficulty}. However, these approaches do not directly enforce theoretically-derived bounds based on environment structure.

\subsection{Our Approach: QBound}

We propose QBound, a stabilization mechanism that prevents overestimation bias by exploiting known environment structure. QBound addresses the root cause of instability in bootstrapped value learning—unbounded overestimation—by deriving and enforcing environment-aware Q-value bounds. Unlike generic pessimism (e.g., Double-Q \citep{van2016deep}), QBound's bounds are environment-specific, providing stabilization without excessive conservatism.

\textbf{Core Mechanism:}
\begin{itemize}
    \item Derive tight bounds $[Q_{\min}, Q_{\max}]$ from environment reward structure
    \item Clip next-state Q-values during bootstrapping: $Q_{\text{next}} \gets \clip(Q_{\text{next}}, Q_{\min}, Q_{\max})$
    \item Compute bounded targets: $Q_{\text{target}} = r + \gamma \cdot Q_{\text{next}}^{\text{clipped}}$
    \item Standard TD loss propagates bounds through the network naturally
\end{itemize}

\textbf{Key Insight:} Clipping next-state Q-values in the bootstrap target—$y = r + \gamma \cdot \clip(Q(s',a'), Q_{\min}, Q_{\max})$—naturally propagates bounds through temporal difference learning \citep{sutton2018reinforcement}. Since today's $Q(s,a)$ is trained toward this clipped target, it becomes tomorrow's $Q(s',a')$ for other states, creating a self-reinforcing bounded value propagation through the Bellman backup chain.

\textbf{Key Benefits:}
\begin{itemize}
    \item \textbf{Primary:} Prevents overestimation bias, stabilizing temporal difference learning \citep{sutton2018reinforcement}
    \item \textbf{Outcome:} 5-31\% improvement in sample efficiency and cumulative reward across standard environments (GridWorld, FrozenLake, CartPole), with dramatic gains up to 264\% on challenging sparse-reward tasks (LunarLander)
    \item Reduces variance in bootstrapped targets, particularly during early training
    \item Negligible computational overhead ($<2\%$)
    \item Works with off-policy algorithms that learn Q-functions (DQN, DDQN, Dueling DQN, DDPG, TD3)
\end{itemize}

\textbf{Target Applications:} QBound is particularly effective for sparse binary reward environments. For reach-once tasks (episode ends upon success), bounds of $Q_{\min} = 0$ and $Q_{\max} = 1$ provide extremely tight constraints. For stay-at-goal tasks, $Q_{\max} = \frac{1}{1-\gamma}$ provides principled bounds.

\section{Related Work}

\subsection{Value-Based Reinforcement Learning}

\textbf{Q-learning} \citep{watkins1992q} learns action-value functions through temporal difference bootstrapping, with convergence guarantees proven for tabular settings \citep{jaakkola1994convergence, melo2001convergence}. The foundational analysis by \citet{watkins1989learning} established the theoretical framework that underlies modern value-based methods.

\textbf{Deep Q-Networks (DQN)} \citep{mnih2013playing, mnih2015human} revolutionized RL by combining Q-learning with deep neural networks, experience replay \citep{lin1992self}, and target networks. \textbf{Double Q-Learning} \citep{van2016deep} addresses overestimation bias but does not bound absolute value magnitudes. Recent advances include dueling architectures \citep{wang2016dueling}, distributional methods \citep{bellemare2017distributional, dabney2018implicit}, and Rainbow combinations \citep{hessel2018rainbow}.

\subsection{Actor-Critic Methods}

\textbf{Actor-critic methods} \citep{konda2000actor} combine policy gradients \citep{sutton2000policy} with value function learning. Classical methods include A2C/A3C \citep{mnih2016asynchronous} for discrete control. For continuous control, \textbf{DDPG} \citep{lillicrap2015continuous} pioneered deterministic policy gradients, while \textbf{TD3} \citep{fujimoto2018addressing} added clipped double-Q estimation and delayed policy updates. \textbf{SAC} \citep{haarnoja2018soft, haarnoja2018soft2} maximizes entropy-augmented objectives for improved exploration. Trust region methods like TRPO \citep{schulman2015trust} and PPO \citep{schulman2017proximal} provide stable policy updates.

\subsection{Sample Efficiency and Experience Replay}

Experience replay \citep{lin1992self} dramatically improves sample efficiency by reusing transitions. \textbf{Prioritized experience replay} \citep{schaul2015prioritized} focuses on important transitions, while \textbf{hindsight experience replay} \citep{andrychowicz2017hindsight} creates synthetic successes for sparse reward environments. Recent work \citep{fedus2020revisiting} revisits replay fundamentals, showing that simple improvements can be highly effective.

\subsection{Stabilization and Optimization}

Deep RL stability has been improved through various techniques: target networks \citep{mnih2015human}, gradient clipping \citep{pascanu2013difficulty}, batch normalization \citep{ioffe2015batch}, and optimizers like Adam \citep{kingma2014adam}. \citet{henderson2018deep} highlighted reproducibility issues and the importance of proper baselines, while theoretical work \citep{szepesvari2010algorithms} provides PAC-MDP analysis for tabular settings.

\subsection{Activation Functions in Value Networks}

The choice of output activation functions for value networks is rarely discussed in RL literature, with most work using linear (identity) output layers by default. However, recent work has begun exploring architectural constraints:

\textbf{Bounded activation functions:} Some early DQN implementations experimented with sigmoid or tanh output activations to bound Q-values \citep{mnih2013playing}, but this was abandoned in favor of linear outputs in DQN \citep{mnih2015human} due to representational limitations. \citet{pohlen2018observe} observed that bounded activations can help in certain Atari games but hurt generalization. The consensus has been that linear outputs provide the most flexibility.

\textbf{Architectural inductive biases:} Recent work in supervised learning has emphasized the importance of architectural inductive biases matching problem structure \citep{battaglia2018relational}. \citet{goyal2021inductive} showed that architectural choices encoding prior knowledge outperform learned representations in many domains. However, this principle has seen limited application to value network design in RL.

\textbf{Initialization bias and exploration:} \citet{kumar2020implicit} showed that network initialization creates implicit regularization in RL, affecting the learned Q-function. \citet{fortunato2017noisy} demonstrated that proper initialization and exploration mechanisms significantly impact learning dynamics. Our work extends this by showing that \textit{output activation functions} can serve as architectural inductive biases that guide exploration space from initialization.

\textbf{Value clipping vs. architectural constraints:} Most RL algorithms apply value clipping \textit{algorithmically} (e.g., PPO's value function clipping \citep{schulman2017proximal}, SAC's target entropy \citep{haarnoja2018soft}). However, these operate post-hoc on network outputs. Our work demonstrates that \textit{architectural enforcement} through output activations can be more effective than algorithmic clipping for certain reward structures.

\subsection{Recent Work on Value Bounding and Constraints}

Recent work has explored constraining Q-values in various contexts. \citet{liu2024boosting} proposed bounding techniques for soft Q-learning in offline settings, demonstrating improved stability through bounded value function approximation. \citet{adamczyk2023bounding} derived theoretical bounds for compositional RL, providing double-sided inequalities relating optimal composite value functions to primitive task values. \citet{wang2024adaptive} investigated adaptive pessimism through target Q-value constraints in offline-to-online RL, achieving improved stability by balancing pessimism constraints with RL objectives.

Work on overestimation bias has also progressed. \citet{elasticdqn2023} proposed Elastic Step DQN, which alleviates overestimation by dynamically varying multi-step horizons based on state similarity. \citet{twosample2024bias} addressed maximization bias through two-sample testing, framing the problem statistically and proposing the T-Estimator that flexibly interpolates between over- and underestimation. \citet{imagination2025limited} introduced Imagination-Limited Q-Learning, which envisions reasonable values and appropriately limits potential over-estimations using maximum behavior values.

For sparse reward environments, \citet{efficient2024sparse} achieved 2$\times$ better sample efficiency through high replay ratio methods with regularization, while \citet{llmreward2024shaping} demonstrated significant improvements by extracting heuristics from large language models for reward shaping.

\subsection{Positioning of QBound}

QBound differs from prior work in several key aspects:
\begin{enumerate}
    \item \textbf{Environment-aware bounds:} Unlike generic stabilization techniques or learned bounds, QBound derives bounds directly from environment reward structure, ensuring theoretical correctness
    \item \textbf{Dual implementation strategy:} We demonstrate that the \textit{same principle} (bounding Q-values) requires \textit{different implementations} depending on reward sign. Hard clipping for positive rewards, architectural constraints (output activations) for negative rewards—the first work to systematically explore this distinction
    \item \textbf{Architectural inductive bias:} We show that output activation functions ($Q = -\text{softplus}(\text{logits})$) can enforce value bounds more effectively than algorithmic clipping by constraining exploration space from initialization, bridging the gap between architectural design and value function learning
    \item \textbf{Bootstrapping-based enforcement:} QBound leverages the natural propagation of bootstrapped targets through temporal difference learning \citep{sutton2018reinforcement}, requiring only simple operations (clipping or activation)
    \item \textbf{Online learning focus:} Unlike recent work on offline RL \citep{wang2024adaptive, imagination2025limited}, QBound targets online learning in both sparse and dense reward settings
    \item \textbf{Comprehensive evaluation:} We provide extensive analysis across 10 environments with 5 seeds each (50 experiments), including honest reporting of failure modes and implementation-dependent effectiveness
\end{enumerate}

\textbf{Novel contribution on activation functions:} This is the first work to systematically demonstrate that \textit{output activation function selection} should depend on reward structure, showing that architectural constraints outperform post-hoc clipping for negative rewards while the reverse holds for positive rewards.

\section{Theoretical Foundations}

\subsection{Preliminaries and Notation}

\begin{definition}[Markov Decision Process]
A Markov Decision Process is a tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, r, \gamma)$ where:
\begin{itemize}
    \item $\mathcal{S}$: State space (finite or continuous)
    \item $\mathcal{A}$: Action space (discrete: $\mathcal{A} = \{a_1, \ldots, a_{|\mathcal{A}|}\}$ or continuous: $\mathcal{A} \subseteq \mathbb{R}^d$)
    \item $P(s'|s,a)$: Transition dynamics
    \item $r(s,a,s') \in \mathbb{R}$: Reward function
    \item $\gamma \in [0,1)$: Discount factor
\end{itemize}
\end{definition}

\begin{definition}[Value Functions]
For policy $\pi: \mathcal{S} \to \Delta(\mathcal{A})$ (stochastic) or $\mu: \mathcal{S} \to \mathcal{A}$ (deterministic):
\begin{align}
V^\pi(s) &= \E_\pi\left[\sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s\right] \\
Q^\pi(s,a) &= \E_\pi\left[\sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a\right]
\end{align}
\end{definition}

\begin{definition}[Optimal Value Functions]
\begin{align}
Q^*(s,a) &= \max_\pi Q^\pi(s,a) \\
V^*(s) &= \max_a Q^*(s,a)
\end{align}
\end{definition}

The Bellman optimality equation \citep{bellman1957markovian, sutton2018reinforcement} provides the foundation for Q-learning:
$$Q^*(s,a) = \E_{s' \sim P(\cdot|s,a)}\left[r(s,a,s') + \gamma \max_{a'} Q^*(s',a')\right]$$

\begin{assumption}[Bounded Rewards]
We assume that worst-case and best-case cumulative returns over all possible trajectories are finite and can be computed or bounded. This is satisfied by most practical environments.
\end{assumption}

\subsection{Environment-Specific Q-Value Bounds}

The key theoretical contribution is deriving tight bounds $[Q_{\min}, Q_{\max}]$ such that all possible Q-values lie within this range.

\begin{definition}[Trajectory]
A trajectory $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots)$ is a sequence of states, actions, and rewards following dynamics $P$ and policy $\pi$.
\end{definition}

\begin{definition}[Trajectory Return]
For finite horizon $H$ or until termination:
$$G(\tau) = \sum_{t=0}^{H-1} \gamma^t r_t$$
\end{definition}

\begin{definition}[Environment-Specific Bounds]
\begin{align}
Q_{\min} &= \inf_{\pi \in \Pi, s \in \mathcal{S}, a \in \mathcal{A}} Q^\pi(s,a) = \inf_{\tau \in \mathcal{T}(s,a)} G(\tau) \\
Q_{\max} &= \sup_{\pi \in \Pi, s \in \mathcal{S}, a \in \mathcal{A}} Q^\pi(s,a) = \sup_{\tau \in \mathcal{T}(s,a)} G(\tau)
\end{align}
where $\mathcal{T}(s,a)$ is the set of all trajectories starting with $(s,a)$.
\end{definition}

\begin{theorem}[Bound Correctness]
\label{thm:bound-correctness}
If $Q_{\min}$ and $Q_{\max}$ are computed according to the above definition, then:
$$Q^*(s,a) \in [Q_{\min}, Q_{\max}] \quad \forall s,a$$
\end{theorem}

\begin{proof}
Follows directly from definition: $Q^*(s,a) = \max_\pi Q^\pi(s,a) \leq \sup_\pi Q^\pi(s,a) = Q_{\max}$, and similarly $Q^* \geq Q_{\min}$.
\end{proof}

\begin{corollary}
Clipping Q-values to $[Q_{\min}, Q_{\max}]$ cannot remove the optimal value $Q^*$.
\end{corollary}

\subsection{Fundamental Q-Value Bounds for Common Reward Structures}

\subsubsection{Case 1: Sparse Binary Rewards (Primary Use Case)}

\textbf{Environment Structure:} Single reward at episode end, zero otherwise:
$$r(s,a,s') = \begin{cases} 
1 & \text{if } s' \text{ is goal state} \\ 
0 & \text{otherwise}
\end{cases}$$

This is the most common sparse reward structure in robotics, games, and goal-reaching tasks.

\begin{theorem}[Sparse Binary Reward Bounds]
\label{thm:sparse-binary-bounds}
For sparse binary reward environments with discount factor $\gamma$, the bounds depend on episode termination:

\textbf{Case 1a (Reach-Once):} Episode terminates upon reaching goal:
$$Q_{\min} = 0, \quad Q_{\max} = 1$$

\textbf{Case 1b (Stay-at-Goal):} Agent can remain at goal and continue receiving rewards until episode end or indefinitely:
$$Q_{\min} = 0, \quad Q_{\max} = \frac{1}{1-\gamma}$$
\end{theorem}

\begin{proof}
\textbf{Lower bound (both cases):} Since all immediate rewards are non-negative, any trajectory return $G(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t \geq 0$, hence $Q_{\min} = 0$.

\textbf{Upper bound (Case 1a - Reach-Once):} The agent receives reward $r=1$ once when reaching the goal, then the episode terminates:
$$Q_{\max} = 1 \cdot \gamma^0 = 1$$

\textbf{Upper bound (Case 1b - Stay-at-Goal):} The agent receives reward $r=1$ at every timestep after reaching the goal:
$$Q_{\max} = \sum_{t=0}^{\infty} \gamma^t \cdot 1 = \frac{1}{1-\gamma}$$

This bound is achieved when the agent reaches the goal immediately and remains there.
\end{proof}

\begin{example}[Robot Navigation - Reach-Once]
A mobile robot navigating to a goal location where the episode ends upon arrival. With $\gamma = 0.99$:
$$Q_{\min} = 0, \quad Q_{\max} = 1$$

Any Q-value outside $[0, 1]$ is impossible given the reward structure and can be safely clipped. This provides extremely tight bounds.
\end{example}

\begin{example}[Robot Navigation - Stay-at-Goal]
A mobile robot that must reach and \textit{maintain} position at the goal, receiving $r=1$ per timestep while at goal. With $\gamma = 0.99$:
$$Q_{\min} = 0, \quad Q_{\max} = \frac{1}{1-0.99} = 100$$

Any Q-value outside $[0, 100]$ can be safely clipped.
\end{example}

\begin{example}[Game Playing - Reach-Once]
A chess engine with binary win/loss outcomes ($+1$ for win, $0$ for loss/draw) where each game is a single episode. With $\gamma = 0.995$:
$$Q_{\min} = 0, \quad Q_{\max} = 1$$

\textit{Note:} The discount factor here primarily affects temporal credit assignment during the game, but the final outcome is binary, so $Q_{\max} = 1$.
\end{example}

\subsubsection{Case 2: Dense Per-Step Costs with Terminal Reward}

\textbf{Environment Structure:} Negative cost per step, positive reward at goal:
$$r(s,a,s') = \begin{cases} 
R_{\text{goal}} & \text{if } s' \text{ is goal state} \\ 
-c & \text{otherwise}
\end{cases}$$

\begin{theorem}[Cost-Plus-Reward Bounds]
For maximum episode length $H$:
\begin{align}
Q_{\min} &= -cH + \gamma^H R_{\text{goal}} \approx -cH \text{ if } c \gg R_{\text{goal}} \\
Q_{\max} &= R_{\text{goal}}
\end{align}
\end{theorem}

\begin{example}[MountainCar]
With $r = -1$ per step, $r = 0$ at goal, $H = 200$:
$$Q_{\min} = -200, \quad Q_{\max} = 0$$
\end{example}

\subsubsection{Case 3: Dense Positive Rewards (Survival Tasks)}

\textbf{Environment Structure:} Positive reward per step until failure:
$$r(s,a,s') = r_{\text{step}} > 0$$

\begin{theorem}[Survival Task Bounds]
For finite horizon $H$:
\begin{align}
Q_{\min} &= 0 \text{ (immediate failure)} \\
Q_{\max} &= r_{\text{step}} \sum_{k=0}^{H-1} \gamma^k = r_{\text{step}} \frac{1-\gamma^H}{1-\gamma}
\end{align}

For infinite horizon (no termination):
$$Q_{\max} = \frac{r_{\text{step}}}{1-\gamma}$$
\end{theorem}

\begin{example}[CartPole]
With $r = +1$ per step, $\gamma = 0.99$, maximum episode length $H = 500$:
$$Q_{\min} = 0, \quad Q_{\max} = \frac{1-0.99^{500}}{1-0.99} \approx 100$$

\textit{Dynamic Bounds:} For survival tasks with fixed start states (e.g., CartPole), we can use step-aware dynamic bounds: $Q_{\max}(t) = \frac{1-\gamma^{(H-t)}}{1-\gamma}$ at timestep $t$, which provides tighter constraints than the static bound. This accounts for the discounted sum of remaining rewards. This is possible because the remaining episode potential is determined by the timestep, not by state proximity to a goal. For sparse reward tasks (e.g., GridWorld), remaining potential depends on unknown state-to-goal distance, making dynamic bounds infeasible.
\end{example}

\subsection{Critical Insight: Reward Sign Determines QBound Effectiveness}
\label{subsec:reward-sign-dependence}

Our comprehensive empirical evaluation (Section~\ref{sec:experiments}) reveals that QBound's effectiveness fundamentally depends on the \textit{sign} of the reward signal. This section provides theoretical justification.

\subsubsection{The Upper Bound is What Matters for Maximization}

\begin{proposition}[Upper Bound Primacy]
In RL, the agent maximizes $\mathbb{E}[\sum_{t=0}^\infty \gamma^t r_t]$. The upper bound $Q_{\max}$ directly constrains this objective. The lower bound $Q_{\min}$ is largely irrelevant because: (1) the agent seeks to \textit{maximize}, not avoid low values, and (2) overestimation (predicting above $Q_{\max}$) causes suboptimal policies, while underestimation merely slows convergence.
\end{proposition}

\subsubsection{Positive Rewards: QBound Provides Essential Upper Bound}

For environments with positive dense rewards (e.g., CartPole: $r = +1$ per timestep), neural networks with linear output layers have no architectural constraint on the upper bound. Q-values can grow unbounded during training.

\begin{theorem}[Overestimation Vulnerability with Positive Rewards]
For $r_t > 0$, the Bellman equation $Q(s,a) = \mathbb{E}[r + \gamma \max_{a'} Q(s',a')]$ allows unbounded growth. Function approximation errors $\epsilon$ compound through bootstrapping. QBound prevents this by enforcing $Q_{\text{target}} = r + \gamma \cdot \text{clip}(\max_{a'} Q(s',a'), Q_{\min}, Q_{\max})$.
\end{theorem}

\textbf{Example:} CartPole achieves +12\% to +34\% improvement across DQN variants (Section~\ref{sec:cartpole-results}).

\subsubsection{Negative Rewards: Implementation Matters}

For negative rewards (e.g., Pendulum: $r \in [-16, 0]$), the Bellman equation itself provides an implicit upper bound, but \textit{how} this bound is enforced critically affects learning.

\begin{theorem}[Natural Upper Bound for Negative Rewards]
\label{thm:negative-reward-bound}
If $r(s,a,s') \leq 0$ for all transitions, then $Q^\pi(s,a) \leq 0$ for any policy $\pi$.

\textbf{Proof:} By induction: $Q^\pi(s,a) = \mathbb{E}[r + \gamma Q^\pi(s',a')] \leq \mathbb{E}[0 + \gamma Q^\pi(s',a')] = \gamma \mathbb{E}[Q^\pi(s',a')]$. If $Q^\pi(s',a') \leq 0$, then $Q^\pi(s,a) \leq 0$.
\end{theorem}

 While the upper bound exists theoretically, \textit{enforcement method} determines effectiveness.

\paragraph{Hard Clipping QBound Fails.} Algorithmic clipping ($Q \gets \text{clip}(Q, -\infty, 0)$) interferes with learning due to \textit{exploration-correction conflicts}:
\begin{itemize}
    \item \textbf{Positive initialization bias:} Neural networks with He/Xavier initialization \citep{he2015delving, glorot2010understanding} typically produce positive outputs initially due to random weight distributions
    \item \textbf{Exploration in wrong space:} Network explores unbounded space, frequently violating $Q > 0$ (56.79\% violation rate observed)
    \item \textbf{Post-hoc correction:} Clipping corrects violations after they occur, but network never learns to naturally output $Q \leq 0$
    \item \textbf{Persistent conflict:} Network continues exploring positive regions, clipping continues correcting → instability
    \item \textbf{Empirical result:} Pendulum DQN shows \textbf{-3.3\% degradation} with architectural QBound (5 seeds)
\end{itemize}

\paragraph{Architectural QBound Succeeds.} Output activation function ($Q = -\text{softplus}(\text{logits})$, enforcing $Q \in (-\infty, 0]$) constrains \textit{exploration space}:
\begin{itemize}
    \item \textbf{Constrained exploration:} Network explores WITHIN correct range $(-\infty, 0]$ from first forward pass
    \item \textbf{Learning correct magnitude:} Network learns ``how negative'' Q should be, not ``whether it should be negative''
    \item \textbf{Zero violations by construction:} Activation function mathematically enforces $Q \leq 0$, no conflict possible
    \item \textbf{Smooth gradients:} $\frac{\partial Q}{\partial \text{logits}} = -\text{sigmoid}(\text{logits}) \in (-1, 0)$ is never zero
    \item \textbf{Empirical result:} Pendulum shows mixed results: TD3 improves by \textbf{+4.1\%} but DQN degrades by -3.3\%, DDPG by -8.0\% (5 seeds)
\end{itemize}

\textbf{Key insight:} The asymmetry between positive and negative rewards is fundamental. For positive rewards (CartPole), Q-values can grow unbounded since neural networks with linear outputs have no upper constraint; hard clipping provides essential stabilization. For negative rewards (Pendulum), the Bellman equation naturally enforces $Q \leq 0$, making QBound redundant. TD3 is an exception showing modest improvement (+4.1\%), but with 72\% higher variance; further investigation is needed.

\subsubsection{Summary: When QBound Works}

\begin{table}[h]
\centering
\caption{QBound Effectiveness by Reward Type and Implementation (5 seeds)}
\label{tab:reward-sign-summary-theory}
\begin{tabular}{lcccc}
\toprule
Reward Type & Natural Bound? & Hard Clipping & Architectural & Best Approach \\
\midrule
\textbf{Positive Dense} (CartPole) & No & +12\% to +34\% & N/A & Hard Clipping \\
\textbf{Negative Dense} (Pendulum DQN) & Yes ($Q \leq 0$) & N/A & -3.3\% & Neither \\
\textbf{Negative Dense} (Pendulum DDPG) & Yes ($Q \leq 0$) & N/A & -8.0\% & Neither \\
\textbf{Negative Dense} (Pendulum TD3) & Yes ($Q \leq 0$) & N/A & +4.1\%* & Architectural* \\
\textbf{Sparse Terminal} (GridWorld) & Yes & $\approx 0\%$ & N/A & Neither \\
\bottomrule
\end{tabular}
\end{table}
\textit{*TD3 shows improvement but with 72\% higher variance; use with caution.}

\textbf{Recommendation:} Use QBound (hard clipping) for positive dense rewards like CartPole where it provides consistent 12-34\% improvements. Do not use QBound for negative rewards (Pendulum-style environments) as it generally degrades performance. TD3 is an exception that may benefit but with high variance.

\section{QBound Bound Selection Strategy}

This section explains how to derive appropriate Q-value bounds for different environment types, focusing on the theoretical foundations demonstrated in our experimental evaluation.

\subsection{Sparse Binary Reward Environments}

Sparse binary reward environments (e.g., GridWorld, FrozenLake) provide extremely tight bounds since the agent receives reward only at terminal states.

\subsubsection{Example: Navigation Tasks (GridWorld, FrozenLake)}

In our experimental evaluation, we tested GridWorld (deterministic 10×10 navigation) and FrozenLake (stochastic 4×4 navigation with slippery ice).

\textbf{Reward Structure:}
\begin{itemize}
    \item $r = 1$ when agent reaches goal (success)
    \item $r = 0$ for all other states/actions
    \item Episode terminates upon reaching goal (reach-once semantics)
\end{itemize}

\textbf{QBound Bounds:}
$$Q_{\min} = 0, \quad Q_{\max} = 1$$

Since the episode terminates immediately upon success, the maximum return is exactly 1 regardless of discount factor. These extremely tight bounds prevent Q-value explosions common in sparse reward exploration.

\textbf{Results:} GridWorld achieved 20.2\% faster convergence; FrozenLake achieved 5.0\% improvement and 76\% better final performance than baseline (see Section 5 for details).

\subsection{Dense Reward Environments: Survival Tasks}

For environments with per-timestep rewards (e.g., CartPole), QBound uses step-aware dynamic bounds.

\subsubsection{Example: CartPole Balance Task}

\textbf{Reward Structure:}
\begin{itemize}
    \item $r = +1$ per timestep (dense rewards)
    \item Episode terminates on failure or after $H = 500$ steps
    \item Discount factor $\gamma = 0.99$
\end{itemize}

\textbf{QBound Bounds (Step-Aware Dynamic):}
$$Q_{\min} = 0, \quad Q_{\max}(t) = \frac{1-\gamma^{(H-t)}}{1-\gamma}$$

At episode start ($t=0$): $Q_{\max}(0) = 99.34$. At the final timestep ($t=499$): $Q_{\max}(499) = 1.0$.

The bounds adapt to remaining episode potential, allowing high Q-values early while constraining them appropriately as the episode progresses.

\textbf{Results:} CartPole achieved 31.5\% higher cumulative reward than baseline (172,904 vs 131,438 total reward over 500 episodes).

\subsection{Implementation Guidelines}

\subsubsection{DQN and Value-Based Methods}

For discrete action spaces, QBound requires minimal code changes:

\begin{lstlisting}
# DQN with QBound integration
def qbound_dqn_update(states, actions, rewards, next_states, dones):
    # Standard DQN target computation
    next_q_values = target_net(next_states).max(1)[0]

    # QBound: clip next-state Q-values
    next_q_values = torch.clamp(next_q_values, Q_min, Q_max)

    # Compute bounded targets
    targets = rewards + gamma * next_q_values * (1 - dones)

    # QBound: clip targets for safety
    targets = torch.clamp(targets, Q_min, Q_max)

    # Current Q-values (unclipped)
    current_q_values = q_net(states).gather(1, actions)

    # Standard TD loss
    loss = F.mse_loss(current_q_values, targets)
    return loss
\end{lstlisting}

\textbf{Key Point:} Bootstrapping naturally propagates bounds through the network via temporal difference learning \citep{sutton2018reinforcement}. Since agents select actions using current Q-values (not next-state Q-values), clipping the bootstrapped targets ensures bound compliance during training.

\section{Algorithm and Implementation Details}

\subsection{Complete QBound Algorithm}

\begin{algorithm}[H]
\caption{QBound: Bounded Q-Value Learning}
\label{alg:qclip}
\begin{algorithmic}[1]
\Require MDP $\mathcal{M}$, Q-network $Q_\theta$, target network $Q_{\theta'}$, replay buffer $\mathcal{D}$
\Require Bounds $[Q_{\min}, Q_{\max}]$, batch size $B$, learning rate $\alpha$

\Function{QBoundUpdate}{$\mathcal{D}, Q_\theta, Q_{\theta'}$}
    \State Sample batch $\{(s_i, a_i, r_i, s'_i, d_i)\}_{i=1}^B \sim \mathcal{D}$
    \State Initialize loss $L \gets 0$

    \For{each transition $(s_i, a_i, r_i, s'_i, d_i)$}
        \State // \textbf{Compute bounded Bellman target}
        \State $Q_{\text{next}} \gets \max_{a'} Q_{\theta'}(s'_i, a')$ \Comment{From target network}
        \State $Q_{\text{next}}^{\text{clipped}} \gets \clip(Q_{\text{next}}, Q_{\min}, Q_{\max})$ \Comment{Enforce bounds}
        \State $Q_{\text{target}} \gets r_i + (1 - d_i) \cdot \gamma \cdot Q_{\text{next}}^{\text{clipped}}$
        \State $Q_{\text{target}}^{\text{final}} \gets \clip(Q_{\text{target}}, Q_{\min}, Q_{\max})$ \Comment{Safety clip}

        \State // \textbf{Standard TD loss}
        \State $Q_{\text{current}} \gets Q_\theta(s_i, a_i)$ \Comment{Current Q-value (unclipped)}
        \State $L \gets L + (Q_{\text{current}} - Q_{\text{target}}^{\text{final}})^2$
    \EndFor

    \State // \textbf{Update network}
    \State $\theta \gets \theta - \alpha \cdot \nabla_\theta L$

    \State // \textbf{Periodically update target network}
    \If{update step}
        \State $\theta' \gets \theta$
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\textbf{Key Insight:} Action selection uses current Q-values $Q_\theta(s, \cdot)$, but learning uses clipped next-state Q-values in targets. This means bounded targets naturally propagate through the network via temporal difference bootstrapping \citep{sutton2018reinforcement}, ensuring the Q-function converges to bounded estimates.

\subsection{Key Implementation Considerations}

\subsubsection{Bound Computation Strategies}

\textbf{1. Exact Bounds (Preferred):}
For environments with known reward ranges $[r_{\min}, r_{\max}]$:
\begin{align}
Q_{\min} &= \frac{r_{\min}}{1-\gamma} \\
Q_{\max} &= \frac{r_{\max}}{1-\gamma}
\end{align}

\textbf{2. Episodic Bounds:}
For tasks with maximum episode length $T$:
\begin{align}
Q_{\min} &= r_{\min} \frac{1-\gamma^T}{1-\gamma} \\
Q_{\max} &= r_{\max} \frac{1-\gamma^T}{1-\gamma}
\end{align}

\textbf{3. Conservative Estimation:}
When exact bounds are unknown:
\begin{itemize}
    \item Monitor observed rewards: $\hat{r}_{\min} = \min_t r_t$, $\hat{r}_{\max} = \max_t r_t$
    \item Add safety margins: $r_{\min} = \hat{r}_{\min} - \epsilon$, $r_{\max} = \hat{r}_{\max} + \epsilon$
    \item Update bounds adaptively if violations consistently occur
\end{itemize}

\textbf{4. State-Dependent Bounds (Advanced):}
For complex environments, compute bounds per state region:
$$Q_{\min}(s) = \min_{\tau \in \mathcal{T}(s)} G(\tau), \quad Q_{\max}(s) = \max_{\tau \in \mathcal{T}(s)} G(\tau)$$

\subsubsection{Proportional Scaling Details}

The \texttt{ScaleToRangePerSample} function applies proportional scaling \textbf{independently to each sample} in the batch. This per-sample approach is critical: scaling each sample's Q-values based only on that sample's min/max prevents one bad sample from affecting others.

\begin{proposition}[Ordering Preservation]
The per-sample linear scaling transformation preserves exact action preference ordering within each sample:
$$Q_\theta(s_i,a_j) > Q_\theta(s_i,a_k) \iff \hat{Q}(s_i,a_j) > \hat{Q}(s_i,a_k)$$
for each state $s_i$ in the batch.
\end{proposition}

\begin{proof}
For each sample $i$, we have $\hat{Q}(s_i,a) = Q_{\min} + \text{scale}_i \cdot (Q_\theta(s_i,a) - Q_{\text{obs\_min},i})$ where $\text{scale}_i = \frac{Q_{\max} - Q_{\min}}{Q_{\text{obs\_max},i} - Q_{\text{obs\_min},i}} > 0$. Since the transformation is a positive affine map applied independently per sample, it strictly preserves action ordering within each sample.
\end{proof}

\subsubsection{Computational Complexity Analysis}

\textbf{Time Complexity:}
\begin{itemize}
    \item Clipping operations: $O(1)$ per Q-value
    \item Auxiliary updates: $O(|\mathcal{A}|)$ when violations occur
    \item Total overhead: $O(|\mathcal{A}| \cdot p_{\text{violation}})$ per batch
    \item Typical overhead: $< 2\%$ in practice
\end{itemize}

\textbf{Space Complexity:} No additional memory beyond storing bounds $Q_{\min}, Q_{\max}$.

\textbf{Network Updates:} Auxiliary updates occur in:
\begin{itemize}
    \item Early training: 40-60\% of steps
    \item Mid training: 15-25\% of steps
    \item Late training: 5-10\% of steps
\end{itemize}

\subsection{Integration Patterns}

\subsubsection{Minimal Integration (Recommended)}

For existing codebases, QBound requires only 3-5 lines of changes:

\begin{lstlisting}
# Before: Standard DQN target computation
targets = rewards + gamma * next_q_values * (1 - dones)

# After: QBound-enhanced computation
next_q_values = torch.clamp(next_q_values, Q_min, Q_max)
targets = rewards + gamma * next_q_values * (1 - dones)
targets = torch.clamp(targets, Q_min, Q_max)
current_q_values = torch.clamp(current_q_values, Q_min, Q_max)
\end{lstlisting}


\subsection{Hard vs Soft QBound: Critical Implementation Choice}

\label{sec:hard_vs_soft}

A fundamental design choice for QBound is whether to enforce bounds through \textit{hard clipping} or \textit{soft penalties}. This choice has profound implications for applicability across different action spaces and algorithms.

\subsubsection{Hard QBound (Direct Clipping)}

\textbf{Implementation:}
\begin{equation}
Q^{\text{clipped}}(s,a) = \clip(Q_\theta(s,a), Q_{\min}, Q_{\max})
\end{equation}

\textbf{Characteristics:}
\begin{itemize}
    \item \textbf{Strict Enforcement:} Q-values are \textit{never} outside bounds
    \item \textbf{Gradient Cutoff:} $\nabla Q = 0$ when $Q$ violates bounds
    \item \textbf{Discontinuous:} Abrupt transitions at boundary points
    \item \textbf{Computational Cost:} Minimal—just clamping operations
\end{itemize}

\textbf{Best for:}
\begin{itemize}
    \item \textbf{Discrete action spaces} (DQN, Double-Q, Dueling DQN)
    \item Value-based methods where Q-values are \textit{not} differentiated w.r.t. actions
    \item Environments with well-defined, tight bounds
\end{itemize}

\subsubsection{Soft QBound (Penalty-Based)}

\textbf{Implementation:}
\begin{equation}
\mathcal{L}_{\text{QBound}} = \lambda \cdot \left[ \max(0, Q - Q_{\max})^2 + \max(0, Q_{\min} - Q)^2 \right]
\end{equation}

where $\lambda$ is a penalty weight (typically 0.1-1.0).

\textbf{Characteristics:}
\begin{itemize}
    \item \textbf{Soft Enforcement:} Q-values \textit{penalized} but not strictly bounded
    \item \textbf{Smooth Gradients:} $\nabla Q$ remains non-zero, enabling gradient flow
    \item \textbf{Continuous:} Quadratic penalty increases smoothly
    \item \textbf{Computational Cost:} Requires additional loss term
\end{itemize}

\textbf{Best for:}
\begin{itemize}
    \item \textbf{Continuous action spaces} (DDPG, TD3, SAC)
    \item Actor-critic methods requiring $\nabla_a Q(s,a)$ for policy gradients
    \item Scenarios where approximate bounds are sufficient
\end{itemize}

\subsubsection{Mathematical Analysis: Why Hard QBound Fails on Continuous Control}

\textbf{Deterministic Policy Gradient Theorem.}
In continuous action actor-critic methods (DDPG, TD3), the policy gradient is computed via the deterministic policy gradient theorem \citep{silver2014deterministic}:
\begin{equation}
\nabla_\phi J(\phi) = \E_{s \sim \rho^\pi} \left[ \nabla_a Q_\theta(s,a) \Big|_{a=\pi_\phi(s)} \cdot \nabla_\phi \pi_\phi(s) \right]
\end{equation}

\textbf{Chain Rule Requirement.}
The critical dependency is $\nabla_a Q_\theta(s,a)|_{a=\pi_\phi(s)}$—the gradient of the critic with respect to actions, evaluated at the policy's output. This gradient must flow through the policy network via the chain rule:
\begin{equation}
\nabla_\phi J(\phi) = \nabla_\phi \pi_\phi(s)^\top \cdot \nabla_a Q_\theta(s,a) \Big|_{a=\pi_\phi(s)}
\end{equation}

\textbf{Failure of Hard Clipping.}
Consider the hard clipping function:
\begin{equation}
\clip(Q, Q_{\min}, Q_{\max}) = \begin{cases}
Q_{\min} & \text{if } Q < Q_{\min} \\
Q & \text{if } Q_{\min} \leq Q \leq Q_{\max} \\
Q_{\max} & \text{if } Q > Q_{\max}
\end{cases}
\end{equation}

The gradient with respect to actions is:
\begin{equation}
\frac{\partial}{\partial a} \clip(Q_\theta(s,a), Q_{\min}, Q_{\max}) = \begin{cases}
0 & \text{if } Q_\theta(s,a) < Q_{\min} \\
\frac{\partial Q_\theta(s,a)}{\partial a} & \text{if } Q_{\min} \leq Q_\theta(s,a) \leq Q_{\max} \\
0 & \text{if } Q_\theta(s,a) > Q_{\max}
\end{cases}
\end{equation}

\textbf{Consequence:} When $Q_\theta(s,a)$ violates bounds, $\nabla_a Q^{\text{clip}} = 0$, causing:
\begin{equation}
\nabla_\phi J(\phi) = \nabla_\phi \pi_\phi(s)^\top \cdot \underbrace{\nabla_a Q^{\text{clip}}}_{\mathbf{=0}} = \mathbf{0}
\end{equation}

This \textit{gradient death} prevents the policy from learning in violated regions—precisely where learning is most needed.

\textbf{Soft QBound: A Principled Penalty Approach.}
Instead of hard clipping, we formulate QBound as a \textit{differentiable penalty function} inspired by barrier methods in constrained optimization \citep{boyd2004convex}. The soft penalty formulation:
\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{TD}} + \lambda \mathcal{L}_{\text{QBound}}
\end{equation}

where $\mathcal{L}_{\text{QBound}} = \max(0, Q - Q_{\max})^2 + \max(0, Q_{\min} - Q)^2$, maintains differentiability while providing a \textit{soft constraint} that grows quadratically with violation magnitude:
\begin{equation}
\frac{\partial \mathcal{L}_{\text{QBound}}}{\partial a} = \begin{cases}
-2\lambda(Q_{\min} - Q) \frac{\partial Q}{\partial a} & \text{if } Q < Q_{\min} \\
0 & \text{if } Q_{\min} \leq Q \leq Q_{\max} \\
2\lambda(Q - Q_{\max}) \frac{\partial Q}{\partial a} & \text{if } Q > Q_{\max}
\end{cases}
\end{equation}

\textbf{Key Mathematical Properties:}
\begin{enumerate}
    \item \textit{Smoothness:} The penalty is continuously differentiable everywhere
    \item \textit{Proportionality:} Penalty grows as $(Q - Q_{\max})^2$ or $(Q_{\min} - Q)^2$—stronger violations incur larger penalties
    \item \textit{Gradient preservation:} $\frac{\partial Q}{\partial a}$ is \textit{always computed}, ensuring gradient flow:
\end{enumerate}
\begin{equation}
\nabla_a \mathcal{L}_{\text{total}} = \nabla_a \mathcal{L}_{\text{TD}} + \lambda \nabla_a \mathcal{L}_{\text{QBound}} \neq \mathbf{0}
\end{equation}

This formulation represents a \textit{principled mathematical approach} to bounded optimization in continuous spaces: the penalty guides Q-values toward bounds without the discontinuities of hard clipping, similar to interior-point methods that use barrier functions. The quadratic form ensures penalties increase smoothly as violations grow, providing stable gradient signals for policy learning.

\subsubsection{Empirical Validation}

Pendulum-v1 DDPG experiments (Section \ref{sec:pendulum_ddpg}) demonstrate:
\begin{itemize}
    \item \textbf{Hard QBound:} 893\% performance degradation (significant performance degradation)
    \item \textbf{Soft QBound:} +712\% improvement over baseline, +5\% over standard DDPG
\end{itemize}

\textbf{Recommendation:} Use Hard QBound for discrete actions (DQN variants), Soft QBound for continuous actions (DDPG, TD3). Never use Hard QBound with continuous action actor-critic methods. Note: QBound is designed for off-policy methods only; on-policy methods (e.g., PPO, A2C) do not suffer from the same overestimation dynamics and are outside QBound's scope.

\subsection{QBound Configuration Guidelines}

\subsubsection{Hard vs Soft QBound: When to Use Each}

\textbf{Hard QBound (Direct Clipping)} is appropriate when:
\begin{itemize}
    \item \textbf{Discrete action spaces:} Policy is typically $\epsilon$-greedy or softmax, not learned via backpropagation through Q-values
    \item \textbf{No action gradients needed:} Action selection is independent of $\nabla_a Q$
    \item \textbf{Examples:} DQN, Double DQN, Dueling DQN on discrete action tasks
    \item \textbf{Implementation:} $Q_{\text{target}} = r + \gamma \cdot \clip(Q(s',a'), Q_{\min}, Q_{\max})$
\end{itemize}

\textbf{Soft QBound (Penalty-Based)} is required when:
\begin{itemize}
    \item \textbf{Continuous action spaces:} Policy gradient depends on $\nabla_a Q$ (DDPG/TD3)
    \item \textbf{Actor-critic methods:} Policy learning requires differentiable Q-values
    \item \textbf{Examples:} DDPG, TD3 with continuous actions
    \item \textbf{Implementation:} $\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{TD}} + \lambda \mathcal{L}_{\text{QBound}}$ where $\mathcal{L}_{\text{QBound}} = \max(0, Q-Q_{\max})^2 + \max(0, Q_{\min}-Q)^2$
\end{itemize}

\subsubsection{Computing Q\_min and Q\_max}

The choice of bounds depends on the reward structure:

\textbf{Sparse Terminal Rewards} (e.g., GridWorld, FrozenLake):
\begin{itemize}
    \item \textbf{If terminal reward is $r_T > 0$:} $Q_{\min} = 0$, $Q_{\max} = r_T$ (independent of horizon)
    \item \textbf{Rationale:} Q-value equals discounted terminal reward, unaffected by intermediate steps
    \item \textbf{Example:} GridWorld goal reward $+1 \Rightarrow Q_{\max} = 1.0$
\end{itemize}

\textbf{Dense Step Rewards} (e.g., CartPole, Pendulum):
\begin{itemize}
    \item \textbf{If reward per step is constant $r$:} Use geometric sum formula
    \item \textbf{Positive rewards:} $Q_{\max} = r \times \frac{1 - \gamma^H}{1 - \gamma}$, $Q_{\min} = 0$
    \item \textbf{Negative rewards:} $Q_{\min} = r \times \frac{1 - \gamma^H}{1 - \gamma}$, $Q_{\max} = 0$
    \item \textbf{Example:} CartPole ($r=+1$, $H=500$, $\gamma=0.99$): $Q_{\max} = \frac{1-0.99^{500}}{1-0.99} \approx 99.34$
    \item \textbf{Example:} Pendulum ($r \approx -16.27$, $H=200$, $\gamma=0.99$): $Q_{\min} = -16.27 \times 99.34 \approx -1616$
\end{itemize}

\textbf{Shaped Rewards} (e.g., LunarLander):
\begin{itemize}
    \item \textbf{Use domain knowledge:} Identify minimum crash penalty and maximum landing bonus
    \item \textbf{Example:} LunarLander: $Q_{\min} = -100$ (crash), $Q_{\max} = 200$ (safe landing + bonuses)
\end{itemize}

\subsubsection{Static vs Dynamic Bounds}

\textbf{Static Bounds} (constant throughout episode):
\begin{itemize}
    \item \textbf{When appropriate:} Sparse terminal rewards, shaped rewards, or dense negative rewards
    \item \textbf{Rationale:} Q-value upper bound doesn't decrease with remaining time
    \item \textbf{Examples:} GridWorld ($Q_{\max}=1$ always), Pendulum ($Q_{\max}=0$ always)
\end{itemize}

\textbf{Dynamic Bounds} (step-aware, decrease with time):
\begin{itemize}
    \item \textbf{When beneficial:} Dense positive step rewards where future return depends on remaining steps
    \item \textbf{Formula:} $Q_{\max}(t) = r \times \frac{1 - \gamma^{H-t}}{1 - \gamma}$ where $t$ is current step, $H$ is horizon
    \item \textbf{Advantage:} Tighter bounds improve learning by reducing overestimation as episode progresses
    \item \textbf{Example:} CartPole with dynamic bounds can provide tighter constraints than static bounds
    \item \textbf{Limitation:} No benefit if $Q_{\max}$ is already minimal (e.g., $Q_{\max}=0$ for negative rewards)
\end{itemize}

\textbf{Summary Table:}

\begin{table}[H]
\centering
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Reward Structure} & \textbf{Bound Type} & \textbf{Implementation} \\
\midrule
Sparse terminal & Static & Hard (DQN) or Soft (AC) \\
Shaped rewards & Static & Hard (DQN) or Soft (AC) \\
Dense negative & Static & Soft (continuous AC) \\
Dense positive & Dynamic & Hard (DQN) or Soft (AC) \\
\bottomrule
\end{tabular}
\caption{QBound configuration recommendations by reward structure. AC = Actor-Critic methods.}
\label{tab:qbound-guidelines}
\end{table}

\section{Experimental Evaluation}

\subsection{Experimental Setup}

\subsubsection{Environments}

We evaluate QBound across seven representative environments with different reward structures spanning discrete and continuous state/action spaces:

\textbf{Sparse Binary Rewards (Discrete State):}
\begin{itemize}
    \item \textbf{GridWorld-v0:} $10 \times 10$ grid, agent starts at $(0,0)$, goal at $(9,9)$, $\gamma = 0.99$. Agent receives $r=+1$ upon reaching the goal and $r=0$ elsewhere.
    \item \textbf{FrozenLake-v1:} $4 \times 4$ slippery navigation, $\gamma = 0.95$. Stochastic transitions with $r=+1$ at goal, $r=0$ elsewhere.
\end{itemize}

\textbf{Sparse Rewards (Continuous State):}
\begin{itemize}
    \item \textbf{LunarLander-v3:} 8D continuous state (position, velocity, angle, angular velocity, leg contact), discrete actions (fire engines, do nothing). Sparse rewards: positive for soft landing, negative for crashes, small penalties for fuel usage. Maximum 1000 steps per episode, $\gamma = 0.99$. \textit{Primary evaluation environment demonstrating QBound's effectiveness on complex sparse-reward tasks.}
    \item \textbf{Acrobot-v1:} Swing-up task with $r=-1$ per step until success. 6D continuous state, discrete actions.
    \item \textbf{MountainCar-v0:} Reach goal on hill with $r=-1$ per step. 2D continuous state, discrete actions.
\end{itemize}

\textbf{Dense Rewards (Survival Tasks):}
\begin{itemize}
    \item \textbf{CartPole-v1:} Balance task with $r = +1$ per timestep, $\gamma = 0.99$. Episode terminates on failure (max 500 steps). 4D continuous state, discrete actions.
\end{itemize}

These environments represent the key challenges for Q-value bounding: GridWorld and FrozenLake test tabular reach-once sparse reward tasks, LunarLander/Acrobot/MountainCar test sparse rewards with continuous states, and CartPole tests survival tasks with dense positive rewards.

\subsubsection{Algorithms}

We implement QBound with Deep Q-Network (DQN) \citep{mnih2015human} as our base algorithm. DQN uses a neural network to approximate Q-values with experience replay and target networks for stable learning. This allows us to demonstrate QBound's core benefit independent of other algorithmic enhancements.

\subsubsection{Hyperparameters}

\begin{table}[h]
\centering
\caption{Key Hyperparameters}
\small
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Batch size & 64 \\
Learning rate & 0.001 \\
Replay buffer & 10,000 transitions \\
Target update frequency & Every 100 steps \\
Network architecture & [128, 128] hidden units \\
Activation & ReLU \\
Optimizer & Adam \\
$\epsilon$ decay & 0.995 (GridWorld, CartPole), 0.999 (FrozenLake) \\
Random seed & 42 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Evaluation Metrics}

\begin{itemize}
    \item \textbf{Sample efficiency:} Episodes/steps to reach target performance
    \item \textbf{Final performance:} Asymptotic average return
    \item \textbf{Learning stability:} Variance in performance across runs
    \item \textbf{Computational overhead:} Wall-clock time per episode
    \item \textbf{Violation statistics:} Frequency and magnitude of bound violations
\end{itemize}

\subsection{Part 1: Initial Validation - DQN + QBound Variants}

To establish QBound's baseline effectiveness, we first conducted 3-way comparisons across three representative environments, testing static and dynamic QBound variants against baseline DQN

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/learning_curves_20251025_183916.pdf}
\caption{Learning curves for all three environments. QBound (blue) consistently outperforms or matches baseline DQN (red) across diverse settings: GridWorld shows 20.2\% faster convergence, FrozenLake achieves 5.0\% improvement, and CartPole demonstrates 31.5\% higher cumulative reward. Smoothed over 50-100 episode windows.}
\label{fig:learning-curves}
\end{figure}

\begin{table}[H]
\centering
\caption{Sample Efficiency Results: Episodes to Target Performance (Initial 3-Way Validation). Note: CartPole shows cumulative reward improvement of +31.5\% in this initial study; comprehensive 6-way evaluation (Section 5.2) shows +14.2\% improvement in final 100 episodes.}
\label{tab:main-results}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Environment} & \textbf{Target} & \textbf{Baseline} & \textbf{QBound} & \textbf{Improvement} \\
\midrule
GridWorld ($10 \times 10$) & 80\% success & 257 & 205 & \textbf{+20.2\%} \\
FrozenLake ($4 \times 4$) & 70\% success & 220 & 209 & \textbf{+5.0\%} \\
CartPole (total reward) & -- & 131,438 & 172,904 & \textbf{+31.5\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Results Analysis:}
QBound demonstrates consistent positive performance across all three environments. GridWorld shows a 20.2\% improvement in sample efficiency, reaching 80\% success in 205 episodes compared to baseline's 257 episodes. FrozenLake achieves 5.0\% improvement, reaching 70\% success in 209 episodes versus baseline's 220 episodes. CartPole shows the most dramatic improvement with 31.5\% higher cumulative reward (172,904 vs 131,438), demonstrating QBound's effectiveness with step-aware dynamic bounds for dense reward environments. These results confirm that QBound provides general-purpose improvements to DQN across both sparse and dense reward settings.

\subsection{Part 2: Comprehensive Evaluation - DQN vs DDQN with QBound Variants (6-Way Comparison)}

To thoroughly evaluate QBound's effectiveness and generalization, we conducted comprehensive 6-way comparisons across four discrete-action environments, comparing:
\begin{enumerate}
    \item \textbf{Baseline DQN} - Standard DQN with target networks
    \item \textbf{Static QBound + DQN} - Environment-aware static bounds
    \item \textbf{Dynamic QBound + DQN} - Step-aware dynamic bounds (for dense rewards)
    \item \textbf{Baseline Double DQN (DDQN)} - Standard DDQN with pessimistic Q-learning
    \item \textbf{Static QBound + DDQN} - QBound integrated with Double DQN
    \item \textbf{Dynamic QBound + DDQN} - Dynamic bounds with Double DQN
\end{enumerate}

\textbf{QBound Configuration by Environment:}

\begin{table}[H]
\centering
\small
\begin{tabular}{@{}lccccl@{}}
\toprule
\textbf{Environment} & \textbf{$Q_{\min}$} & \textbf{$Q_{\max}$} & \textbf{$\gamma$} & \textbf{Type} & \textbf{Rationale} \\
\midrule
GridWorld & 0.0 & 1.0 & 0.99 & Static & Sparse terminal reward \\
FrozenLake & 0.0 & 1.0 & 0.95 & Static & Sparse terminal reward \\
CartPole & 0.0 & 99.34 & 0.99 & Static + Dynamic & Dense step rewards \\
LunarLander & -100 & 200 & 0.99 & Static & Shaped rewards \\
\bottomrule
\end{tabular}
\caption{QBound configurations for discrete-action environments. Dynamic bounds use $Q_{\max}(t) = (1-\gamma^{H-t})/(1-\gamma)$ where $H$ is max steps and $t$ is current step. All DQN experiments use hard clipping (acceptable for discrete actions).}
\label{tab:dqn-qbound-config}
\end{table}

\subsubsection{GridWorld: Sparse Terminal Reward}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/gridworld_6way_results.png}
\caption{GridWorld 6-way comparison. Dynamic QBound + DDQN achieves best performance (482 total reward, 0.96 avg), demonstrating that QBound complements Double DQN's pessimistic Q-learning in sparse reward environments.}
\label{fig:gridworld-6way}
\end{figure}

\begin{table}[H]
\centering
\caption{GridWorld Performance Ranking (500 episodes)}
\small
\begin{tabular}{@{}clcc@{}}
\toprule
\textbf{Rank} & \textbf{Method} & \textbf{Total Reward} & \textbf{Avg/Episode} \\
\midrule
1 & Dynamic QBound + DDQN & 482 & 0.96 \\
2 & Baseline DDQN & 476 & 0.95 \\
3 & Static QBound + DDQN & 474 & 0.95 \\
4 & Static QBound + DQN & 350 & 0.70 \\
5 & Baseline DQN & 257 & 0.51 \\
6 & Dynamic QBound + DQN & 2 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{FrozenLake: Stochastic Sparse Reward}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/frozenlake_6way_results.png}
\caption{FrozenLake 6-way comparison (2000 episodes). Baseline DDQN achieves best performance (1065 total reward), with Static QBound + DQN in second place (982), demonstrating environment-specific algorithm effectiveness.}
\label{fig:frozenlake-6way}
\end{figure}

\begin{table}[H]
\centering
\caption{FrozenLake Performance Ranking (2000 episodes)}
\small
\begin{tabular}{@{}clcc@{}}
\toprule
\textbf{Rank} & \textbf{Method} & \textbf{Total Reward} & \textbf{Avg/Episode} \\
\midrule
1 & Baseline DDQN & 1065 & 0.53 \\
2 & Static QBound + DQN & 982 & 0.49 \\
3 & Baseline DQN & 917 & 0.46 \\
4 & Dynamic QBound + DDQN & 860 & 0.43 \\
5 & Static QBound + DDQN & 854 & 0.43 \\
6 & Dynamic QBound + DQN & 710 & 0.35 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{CartPole: Dense Positive Reward}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/cartpole_6way_results.png}
\caption{CartPole 6-way comparison (500 episodes). Baseline DQN achieves best performance (183K total reward, 366 avg), while Double DQN severely degrades performance (43K, 87 avg) - a 76.3\% degradation demonstrating that pessimistic Q-learning is fundamentally incompatible with dense-reward, long-horizon tasks.}
\label{fig:cartpole-6way}
\end{figure}

\begin{table}[H]
\centering
\caption{CartPole Performance Ranking (500 episodes)}
\small
\begin{tabular}{@{}clcc@{}}
\toprule
\textbf{Rank} & \textbf{Method} & \textbf{Total Reward} & \textbf{Avg/Episode} \\
\midrule
1 & Baseline DQN & 183,022 & 366.04 \\
2 & Static QBound + DQN & 167,840 & 335.68 \\
3 & Static QBound + DDQN & 119,819 & 239.64 \\
4 & Dynamic QBound + DQN & 106,027 & 212.05 \\
5 & Dynamic QBound + DDQN & 82,557 & 165.11 \\
6 & Baseline DDQN & 43,402 & 86.80 \\
\bottomrule
\end{tabular}
\end{table}

 Double DQN's pessimistic Q-learning causes significant performance degradation on CartPole (dense reward, long horizon), achieving only 86.80 avg reward compared to baseline DQN's 366.04 (-76.3\%). This demonstrates that algorithm-level pessimism is fundamentally environment-dependent.

\subsubsection{LunarLander: Shaped Sparse Reward}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/lunarlander_6way_results.png}
\caption{LunarLander 6-way comparison (500 episodes). Dynamic QBound + DQN achieves best performance (82K total reward, 164.32 avg), demonstrating QBound's effectiveness on complex shaped-reward tasks where both bounds and dynamic adaptation provide value.}
\label{fig:lunarlander-6way}
\end{figure}

\begin{table}[H]
\centering
\caption{LunarLander Performance Ranking (500 episodes)}
\small
\begin{tabular}{@{}clcc@{}}
\toprule
\textbf{Rank} & \textbf{Method} & \textbf{Total Reward} & \textbf{Avg/Episode} \\
\midrule
1 & Dynamic QBound + DQN & 82,158 & 164.32 \\
2 & Dynamic QBound + DDQN & 61,684 & 123.37 \\
3 & Baseline DDQN & 48,069 & 96.14 \\
4 & Static QBound + DDQN & 33,626 & 67.25 \\
5 & Static QBound + DQN & 31,236 & 62.47 \\
6 & Baseline DQN & -38,946 & -77.89 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Insight:} LunarLander demonstrates Dynamic QBound's superior performance on shaped-reward tasks, achieving 164.32 avg reward - a massive improvement over baseline DQN's -77.89. This validates QBound's effectiveness on complex continuous-state environments with discrete actions.

\subsubsection{Summary: Environment-Dependent Algorithm Effectiveness}

The 6-way comparison reveals critical insights about algorithm-environment interactions:

\begin{enumerate}
    \item \textbf{Double DQN is environment-dependent:} Excels on sparse rewards (LunarLander: +400.5\% vs baseline DQN) but severely degrades performance on dense rewards (CartPole: -76.3\%)
    \item \textbf{QBound provides robust improvements:} Never causes significant performance degradations, achieves best or near-best performance in 2/4 environments (GridWorld, LunarLander)
    \item \textbf{Dynamic bounds are critical for shaped rewards:} LunarLander and LunarLander both benefit dramatically from dynamic QBound
    \item \textbf{Static bounds work for simple sparse tasks:} GridWorld and FrozenLake achieve good results with static QBound
\end{enumerate}

\subsection{Detailed Analysis by Environment}

\subsubsection{GridWorld ($10 \times 10$)}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/gridworld_learning_curve_20251025_183919.pdf}
\caption{GridWorld learning curve. QBound reaches 80\% success rate in 205 episodes compared to baseline's 257 episodes (20.2\% faster).}
\label{fig:gridworld-curve}
\end{figure}

\textbf{Environment Specification:}
\begin{itemize}
    \item State space: $10 \times 10$ grid, one-hot encoded (100-dimensional)
    \item Agent starts at $(0,0)$, goal at $(9,9)$
    \item Reward: $r = +1$ at goal, $r = 0$ elsewhere (reach-once task)
    \item Discount factor: $\gamma = 0.99$
    \item Q-value bounds: $Q_{\min} = 0$, $Q_{\max} = 1.0$
\end{itemize}

\textbf{Actual Results:}
\begin{itemize}
    \item Baseline: 257 episodes to 80\% success, total reward 303.0
    \item QBound: 205 episodes to 80\% success, total reward 373.0
    \item Performance: QBound improved sample efficiency by 20.2\% and total reward by 23.1\%
    \item Analysis: The direct clipping approach (without proportional scaling) preserves well-behaved Q-values while correcting violators, enabling faster and more stable learning in this deterministic sparse reward environment
\end{itemize}

\subsubsection{FrozenLake ($4 \times 4$)}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/frozenlake_learning_curve_20251025_183919.pdf}
\caption{FrozenLake learning curve. QBound reaches 70\% success rate in 209 episodes compared to baseline's 220 episodes (5.0\% faster).}
\label{fig:frozenlake-curve}
\end{figure}

\textbf{Environment Specification:}
\begin{itemize}
    \item State space: $4 \times 4$ grid with slippery transitions
    \item Stochastic dynamics: intended action succeeds only 33\% of the time
    \item Reward: $r = +1$ at goal, $r = 0$ elsewhere (reach-once task)
    \item Discount factor: $\gamma = 0.95$
    \item Q-value bounds: $Q_{\min} = 0$, $Q_{\max} = 1.0$
\end{itemize}

\textbf{Actual Results:}
\begin{itemize}
    \item Baseline: 220 episodes to 70\% success, total reward 1755.0
    \item QBound: 209 episodes to 70\% success, total reward 1739.0
    \item Performance: QBound improved sample efficiency by 5.0\%
    \item Analysis: In this stochastic environment, QBound's value bounds helped stabilize learning and reduce overestimation, leading to faster convergence to the target success rate. The Q-value bounds prevent overoptimistic estimates that are common in environments with uncertain transitions
\end{itemize}

\subsubsection{CartPole}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/cartpole_learning_curve_20251025_183919.pdf}
\caption{CartPole learning curve. QBound achieves 31.5\% higher cumulative reward (172,904 vs 131,438 total) demonstrating the effectiveness of step-aware dynamic bounds for dense reward environments.}
\label{fig:cartpole-curve}
\end{figure}

\textbf{Environment Specification:}
\begin{itemize}
    \item State space: 4D continuous (position, velocity, angle, angular velocity)
    \item Reward: $r = +1$ per timestep (survival task)
    \item Episode terminates on failure, max 500 steps per episode (horizon $H=500$)
    \item Discount factor: $\gamma = 0.99$
    \item Q-value bounds: $Q_{\min} = 0$, $Q_{\max}(t) = \frac{1-\gamma^{(H-t)}}{1-\gamma}$ where $H=500$ (step-aware dynamic bounds, $Q_{\max}(0) \approx 99.34$)
\end{itemize}

\textbf{Actual Results:}
\begin{itemize}
    \item Baseline total reward: 131,438 over 500 episodes (avg 262.9 per episode)
    \item QBound total reward: 172,904 over 500 episodes (avg 345.8 per episode)
    \item Performance: QBound achieved 31.5\% higher cumulative reward
    \item Analysis: The step-aware dynamic Q-bounds enable proper learning by allowing high Q-values early in episodes (when up to 500 timesteps remain) while appropriately constraining them later. This is critical for dense reward environments where Q-values should reflect remaining episode potential. At timestep $t$, $Q_{\max}(t) = \frac{1-\gamma^{(H-t)}}{1-\gamma}$ correctly bounds the maximum discounted achievable return
\end{itemize}

\subsubsection{Bound Selection Rationale}

The Q-value bounds for each environment are derived from the environment's reward structure:

\begin{itemize}
    \item \textbf{GridWorld \& FrozenLake (Sparse Rewards - Static Bounds):} Since the agent receives $r=+1$ once at the goal, the maximum cumulative discounted return is $Q_{\max} = 1.0$, and $Q_{\min} = 0$. These static bounds are appropriate for sparse reward tasks.

    \item \textbf{CartPole (Dense Rewards - Step-Aware Dynamic Bounds):} The agent receives $r=+1$ per timestep up to 500 steps with $\gamma=0.99$. We use step-aware dynamic bounds: $Q_{\max}(t) = \frac{1-\gamma^{(H-t)}}{1-\gamma}$, which correctly reflects the maximum discounted achievable return at each timestep. This accounts for discounting and is critical for dense reward environments where remaining episode potential decreases over time.
\end{itemize}

These bounds are environment-aware and theoretically grounded, not learned or tuned hyperparameters. The key innovation is using static bounds for sparse rewards and dynamic step-aware bounds for dense rewards.

\subsubsection{Theoretical Foundation: Q-Value Behavior in Sparse vs Dense Rewards}

\textbf{Key Insight:} Q-values evolve in \textit{opposite directions} for sparse versus dense reward tasks as episodes progress.

\paragraph{Sparse Rewards - Q-Values Increase Toward Goal:}

In sparse reward environments (e.g., GridWorld, FrozenLake), the agent receives reward only at terminal states. As the agent approaches the goal, Q-values \textit{increase} because:

\begin{theorem}[Sparse Reward Q-Value Growth]
For sparse reward tasks with terminal reward $r_T = 1$ and discount $\gamma < 1$, the optimal Q-value grows as goal proximity increases:
$$Q^*(s,a) = \gamma^{d(s)} \cdot r_T$$
where $d(s)$ is the optimal distance (in steps) from state $s$ to the goal.
\end{theorem}

\textbf{Example (GridWorld):}
\begin{itemize}
    \item \textbf{Far from goal} (18 steps away): $Q^* = \gamma^{18} \cdot 1 \approx 0.83$ (low)
    \item \textbf{Near goal} (1 step away): $Q^* = \gamma^{1} \cdot 1 = 0.99$ (high)
    \item \textbf{At goal}: $Q^* = 1.0$ (maximum)
\end{itemize}

The Q-value trajectory over an episode: $0.83 \to 0.84 \to \cdots \to 0.99 \to 1.0$ (\textit{increasing})

\paragraph{Dense Rewards - Q-Values Decrease Over Time:}

In dense reward environments (e.g., CartPole), the agent receives reward $r=+1$ at \textit{every} timestep. As the episode progresses, Q-values \textit{decrease} because there are fewer remaining steps:

\begin{theorem}[Dense Reward Q-Value Decay]
For dense reward tasks with per-step reward $r=1$, discount $\gamma < 1$, and fixed horizon $H$, the optimal Q-value at timestep $t$ is:
$$Q^*(s_t,a) = \sum_{k=0}^{H-t-1} \gamma^k \cdot r = \frac{1 - \gamma^{(H-t)}}{1 - \gamma}$$
which monotonically decreases as $t$ increases.
\end{theorem}

\textbf{Example (CartPole with $\gamma=0.99$, $H=500$):}
\begin{itemize}
    \item \textbf{Episode start} ($t=0$): $Q^* = \frac{1-0.99^{500}}{1-0.99} \approx 99.34$ (maximum)
    \item \textbf{Mid-episode} ($t=250$): $Q^* = \frac{1-0.99^{250}}{1-0.99} \approx 91.89$ (medium)
    \item \textbf{Near end} ($t=499$): $Q^* = \frac{1-0.99^{1}}{1-0.99} = 1.0$ (minimum)
\end{itemize}

The Q-value trajectory over an episode: $99.34 \to 98.20 \to \cdots \to 1.0$ (\textit{decreasing})

\paragraph{Implications for QBound:}

This fundamental difference determines bound selection:

\begin{itemize}
    \item \textbf{Sparse rewards}: Q-values are \textit{state-dependent} (not time-dependent). A static bound $Q_{\max} = 1.0$ works for all states, though it's loose for distant states. Dynamic bounds would require knowing each state's distance to goal (infeasible without solving the MDP).

    \item \textbf{Dense rewards}: Q-values are \textit{time-dependent} (not state-dependent for fixed-start tasks). Dynamic bounds $Q_{\max}(t) = \frac{1-\gamma^{(H-t)}}{1-\gamma}$ provide tight, time-varying constraints that naturally decrease with the theoretical optimum.
\end{itemize}

\subsubsection{Why Dynamic Bounds for Dense but Not Sparse Rewards}

Building on the theoretical foundation above, the applicability of dynamic (step-aware) versus static bounds depends critically on the environment's reward structure and state initialization:

\textbf{Dense Reward Environments (e.g., CartPole):}
Dynamic bounds are feasible because:
\begin{itemize}
    \item \textbf{Fixed start state:} CartPole always initializes to the same state (pole upright, cart at center)
    \item \textbf{Known timestep:} The current timestep $t$ within the episode is always known
    \item \textbf{Deterministic horizon:} Maximum episode length $H = 500$ is fixed
    \item \textbf{Dense rewards:} Receiving $r=+1$ per timestep means remaining discounted potential is $Q_{\max}(t) = \frac{1-\gamma^{(H-t)}}{1-\gamma}$
\end{itemize}

At any timestep $t$, the agent can compute tight bounds: $Q_{\max}(t) = \frac{1-\gamma^{(H-t)}}{1-\gamma}$ represents the maximum discounted achievable return if the agent survives all remaining steps.

\textbf{Sparse Reward Environments (e.g., GridWorld, FrozenLake):}
Dynamic bounds are \textit{not} feasible because:
\begin{itemize}
    \item \textbf{Variable start-to-goal distance:} Different states have different optimal path lengths to the goal
    \item \textbf{State-dependent potential:} A state $(x,y)$ near the goal has higher maximum return than a distant state
    \item \textbf{Unknown proximity:} The agent does not know how many steps remain until reaching the goal
    \item \textbf{Sparse rewards:} Only terminal reward, so remaining potential depends on \textit{state proximity}, not timestep
\end{itemize}

For example, in GridWorld:
\begin{itemize}
    \item State $(9,9)$ (at goal): $Q_{\max} = 1.0$ (immediate reward)
    \item State $(8,9)$ (1 step away): $Q_{\max} = \gamma^1 \cdot 1 = 0.99$
    \item State $(0,0)$ (18 steps away): $Q_{\max} = \gamma^{18} \cdot 1 \approx 0.83$
\end{itemize}

Computing state-specific bounds would require:
\begin{enumerate}
    \item Knowing the optimal distance from each state to the goal (requires solving the MDP)
    \item Maintaining per-state bound estimates (high complexity)
    \item Handling stochastic dynamics (FrozenLake has non-deterministic transitions)
\end{enumerate}

Therefore, we use \textbf{conservative static bounds} $Q_{\max} = 1.0$ for sparse reward tasks, which are valid for all states but looser for distant states. This trade-off between tightness and tractability is acceptable since:
\begin{itemize}
    \item Static bounds still prevent extreme Q-value explosions
    \item Sparse reward environments already learn slowly, so slightly looser bounds have minimal impact
    \item The primary benefit of QBound comes from preventing overestimation, not from maximally tight bounds
\end{itemize}

\textbf{Summary:} Dynamic bounds exploit the structure of dense reward survival tasks where remaining potential is determined by timestep. Sparse reward tasks require static bounds due to state-dependent goal proximity.

\subsubsection{LunarLander-v3 (Primary Evaluation)}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/lunarlander_comparison_20251027_123420.pdf}
\caption{LunarLander-v3 4-way comparison learning curves. QBound+Double DQN (red) achieves the best performance with 83\% success rate and lowest variance. All methods shown with 20-episode moving average over 500 training episodes.}
\label{fig:lunarlander-comparison}
\end{figure}

\textbf{Environment Specification:}
\begin{itemize}
    \item State space: 8D continuous (x, y, vx, vy, angle, angular velocity, left leg contact, right leg contact)
    \item Action space: Discrete (4 actions: do nothing, fire left engine, fire main engine, fire right engine)
    \item Reward structure: Sparse with shaped components
    \begin{itemize}
        \item Moving from top to landing pad: +100 to +140 points
        \item Crash: -100 points
        \item Soft landing: +100 points
        \item Each leg ground contact: +10 points
        \item Firing main engine: -0.3 points per frame
        \item Firing side engines: -0.03 points per frame
    \end{itemize}
    \item Episode termination: Crash, safe landing, or 1000 steps
    \item Discount factor: $\gamma = 0.99$
    \item Q-value bounds: $Q_{\min} = -100$, $Q_{\max} = 200$ (conservative estimate based on reward structure)
\end{itemize}

\textbf{Experimental Results:}

\begin{table}[H]
\centering
\caption{LunarLander-v3: Final 100 Episodes Performance (500 episodes total)}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{Mean $\pm$ Std} & \textbf{Max} & \textbf{Success Rate} & \textbf{vs Baseline} \\
\midrule
Baseline DQN & -61.8 $\pm$ 177.6 & 280.1 & 11.0\% & -- \\
QBound DQN & 101.3 $\pm$ 183.9 & 295.9 & 50.0\% & \textcolor{green}{+163.1 (+263.9\%)} \\
Double DQN & 185.7 $\pm$ 140.8 & 319.1 & 71.0\% & \textcolor{green}{+247.5 (+400.5\%)} \\
\textbf{QBound+Double DQN} & \textbf{228.0 $\pm$ 89.6} & \textbf{318.2} & \textbf{83.0\%} & \textcolor{green}{\textbf{+289.8 (+469.2\%)}} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{enumerate}
    \item \textbf{Dramatic Performance Gain:} QBound DQN improved by +163.1 points (+263.9\%) over baseline, transforming a failing agent (11\% success) into a moderately successful one (50\% success).

    \item \textbf{Double DQN Excels:} Double DQN alone achieved +400.5\% improvement, demonstrating that pessimistic Q-learning is highly effective for sparse-reward environments. This contrasts sharply with its significant performance degradation on dense-reward CartPole.

    \item \textbf{Best Combination:} QBound+Double DQN achieved the highest performance (228.0 $\pm$ 89.6, 83\% success) and \textit{lowest variance} (89.6 std vs 177.6 baseline). The combination of environment-aware bounds and algorithmic pessimism provides complementary benefits.

    \item \textbf{Variance Reduction:} QBound+Double DQN reduced standard deviation by 49.6\% compared to baseline, demonstrating improved learning stability. This is critical for real-world deployment where consistent performance matters.

    \item \textbf{Success Threshold:} We define success as achieving reward $> 200$ (safe landing), following the standard benchmark criterion \citep{brockman2016openai} where 200+ indicates consistent landing with minimal fuel usage. The 83\% success rate represents near-mastery of the task.
\end{enumerate}

\textbf{Analysis:}

LunarLander is an ideal testbed for QBound because:
\begin{itemize}
    \item \textbf{Sparse rewards with delayed consequences:} Crash penalties (-100) and landing bonuses (+100) come at episode end, requiring stable Q-value propagation.

    \item \textbf{Complex continuous state space:} 8D state requires function approximation, making Q-value stability critical.

    \item \textbf{Stochastic dynamics:} Wind and engine physics create exploration challenges where overestimation bias can derail learning.

    \item \textbf{Long episodes:} Up to 1000 steps per episode means stable bootstrapping over extended horizons is essential.
\end{itemize}

The dramatic improvement demonstrates that QBound's environment-aware bounds effectively stabilize Q-learning in challenging sparse-reward settings. Furthermore, the success of Double DQN and QBound+Double DQN on LunarLander (while Double DQN degrades performance significantly on CartPole) confirms our hypothesis: \textit{pessimistic Q-learning is environment-dependent}, with sparse-reward tasks benefiting from reduced overestimation.

\subsection{Discussion}

\subsubsection{Key Insights}

\textbf{Why QBound Works:}
\begin{itemize}
    \item \textbf{Reduces Overestimation:} By enforcing environment-aware bounds, QBound prevents Q-values from exploding during early training, a common issue in bootstrapped temporal difference learning \citep{thrun1993issues, van2016deep}.

    \item \textbf{Bootstrapping-Based Enforcement:} Since RL agents select actions using current Q-values (not next-state Q-values), clipping during target computation naturally propagates bounds through the network via temporal difference bootstrapping \citep{sutton2018reinforcement}.

    \item \textbf{Environment-Aware Bounds:} Unlike arbitrary clipping, QBound derives theoretically-grounded bounds from reward structure, ensuring valid Q-values while maintaining tightness.

    \item \textbf{Works with Sparse and Dense Rewards:} Static bounds for sparse rewards (GridWorld, FrozenLake) and dynamic step-aware bounds for dense rewards (CartPole) provide flexibility across environments.
\end{itemize}

\subsubsection{Computational Efficiency}

QBound adds minimal computational overhead:
\begin{itemize}
    \item Only requires two clamp operations per training step
    \item Overhead: $<2\%$ additional compute time
    \item Net speedup: Due to fewer episodes needed, overall training is faster
    \item Memory: No additional buffers or networks required
\end{itemize}

\subsection{Comparison with Double DQN}

To understand QBound's positioning relative to existing overestimation reduction techniques, we conducted a comprehensive comparison with Double DQN \citep{van2016deep} across seven diverse environments. This reveals a critical pattern about when pessimistic Q-learning helps versus hurts.

\subsubsection{Experimental Setup}

We compared four approaches across all evaluated environments:
\begin{itemize}
    \item \textbf{Baseline DQN:} Standard DQN with experience replay and target networks
    \item \textbf{QBound DQN:} DQN with environment-aware Q-value bounds (our method)
    \item \textbf{Double DQN:} Uses online network for action selection, target network for evaluation (industry standard pessimistic approach)
    \item \textbf{QBound+Double DQN:} Combined approach leveraging both techniques
\end{itemize}

All methods used identical hyperparameters per environment (learning rate 0.001, same network architecture, same training episodes). We evaluate on diverse tasks spanning tabular (GridWorld, FrozenLake), continuous state with sparse rewards (LunarLander, Acrobot, MountainCar), and continuous state with dense rewards (CartPole).

\subsubsection{Cross-Environment Results Summary}

\begin{table}[H]
\centering
\caption{QBound vs Double DQN: Cross-Environment Performance (Final 100 Episodes)}
\small
\begin{tabular}{@{}lccccl@{}}
\toprule
\textbf{Environment} & \textbf{Type} & \textbf{DQN} & \textbf{Double DQN} & \textbf{QBound} & \textbf{Winner} \\
\midrule
\textbf{LunarLander} & Sparse & -61.8 & \textcolor{green}{+185.7} & \textcolor{green}{+101.3} & \textbf{DDQN+Q (228.0)} \\
CartPole-Corrected & Dense & 358.3 & \textcolor{red}{281.8 (-21\%)} & \textcolor{green}{409.0 (+14\%)} & \textbf{QBound} \\
Acrobot & Sparse & -87.0 & -97.7 (-12\%) & -93.7 (-8\%) & DQN \\
MountainCar & Sparse & -124.5 & -146.7 (-18\%) & -145.2 (-17\%) & DQN \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/unified_qbound_improvement.pdf}
\caption{QBound improvement over baseline DQN across environments. Green bars indicate improvement, red bars indicate degradation. LunarLander shows dramatic +263.9\% improvement, while exploration-heavy tasks (MountainCar, Acrobot) show moderate degradation.}
\label{fig:unified-improvement}
\end{figure}

\textbf{Key Insights:}

\begin{enumerate}
    \item \textbf{Environment-Dependent Effectiveness:} QBound improves performance in 2/4 evaluated environments (50\% success rate), with average improvement of +63.5\% across all environments. Performance varies dramatically by environment type:
    \begin{itemize}
        \item \textit{Strong positive:} LunarLander (+263.9\%), CartPole-Corrected (+14.2\%)
        \item \textit{Slight negative:} Acrobot (-7.6\%), MountainCar (-16.6\%)
    \end{itemize}

    \item \textbf{Double DQN Also Environment-Dependent:} Double DQN shows similar environment sensitivity, excelling in sparse-reward tasks (LunarLander: +400.5\%) but struggling with dense rewards (CartPole: -76.3\%). This confirms that \textit{algorithmic pessimism is not universally beneficial}.

    \item \textbf{Best Combination for Sparse Rewards:} QBound+Double DQN achieves the best results on LunarLander (228.0 $\pm$ 89.6, 83\% success), demonstrating that environment-aware bounds and algorithmic pessimism provide complementary benefits for sparse-reward tasks.

    \item \textbf{QBound Failure Modes:} QBound hurts performance in exploration-critical environments (MountainCar, Acrobot) where over-constraining Q-values may limit the agent's willingness to explore. These tasks require aggressive exploration to discover sparse rewards.
\end{enumerate}

\textbf{Takeaway:} Neither QBound nor Double DQN is universally superior. QBound provides a more robust alternative for sparse-reward tasks with known reward bounds, while Double DQN offers complementary algorithmic pessimism. The combination (QBound+Double DQN) achieves the best results on challenging sparse-reward tasks like LunarLander.

\subsubsection{CartPole Results: Dense Rewards, Long Horizon}

\begin{table}[H]
\centering
\caption{CartPole: Training Performance (500 episodes, $\gamma=0.99$)}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{Total Reward} & \textbf{Mean Reward} & \textbf{vs Baseline} & \textbf{Outcome} \\
\midrule
Baseline DQN & 183,022 & 366.0 & -- & Good \\
Double DQN & 61,712 & 123.4 & \textcolor{red}{-66.3\%} & \textbf{CATASTROPHIC} \\
QBound & 182,652 & 365.3 & -0.2\% & Good \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Evaluation Results (100 episodes, max\_steps=500):}
\begin{itemize}
    \item \textbf{Baseline DQN:} 500.0 (perfect performance, 100\% success)
    \item \textbf{Double DQN:} 24.3 (\textbf{95.1\% worse}, significant performance degradation)
    \item \textbf{QBound:} 321.8 (35.6\% worse, moderate degradation)
\end{itemize}

\textbf{Key Finding:} Double DQN \textit{showed severe performance degradation} on CartPole, collapsing at episode 300 from 327 avg reward to just 11.2. The agent learned "giving up is rational" due to systematic underestimation of long-horizon returns. QBound performed significantly better but still struggled with the theoretical Q\_max=99.34 bound being far below the empirical returns of 500.

\subsubsection{FrozenLake Results: Sparse Rewards, Stochastic}

\begin{table}[H]
\centering
\caption{FrozenLake: Success Rate (2000 episodes, 4×4 grid, $\gamma=0.95$)}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{Training Reward} & \textbf{Eval Success} & \textbf{vs Baseline} & \textbf{Outcome} \\
\midrule
Baseline DQN & 0.459 & 41\% & -- & Moderate \\
Double DQN & 0.543 & 47\% & \textcolor{green}{+14.6\%} & Good \\
QBound & 0.481 & 72\% & \textcolor{green}{+75.6\%} & \textbf{Excellent} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding:} In the sparse reward environment, Double DQN \textit{succeeded}, achieving 15\% higher success rate and converging 5.2× faster (179 vs 932 episodes). QBound achieved even stronger results with 76\% improvement, demonstrating the benefit of tight bounds ([0, 1]) for sparse binary reward tasks.

\subsubsection{GridWorld Results: Sparse Rewards, Deterministic}

\begin{table}[H]
\centering
\caption{GridWorld: Training Performance (1000 episodes, 10×10 grid, $\gamma=0.99$)}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{Total Reward} & \textbf{Mean Reward} & \textbf{vs Baseline} & \textbf{Outcome} \\
\midrule
Baseline DQN & 757 & 0.757 & -- & Good \\
Double DQN & 789 & 0.789 & \textcolor{green}{+4.2\%} & Better \\
QBound & 907 & 0.907 & \textcolor{green}{+19.8\%} & \textbf{Best} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Evaluation Results (100 episodes):}
\begin{itemize}
    \item All three methods: 100\% success rate (optimal policy learned)
\end{itemize}

\textbf{Key Finding:} GridWorld confirms the sparse-reward pattern. Double DQN outperformed baseline during training (+4.2\%), while QBound achieved the strongest improvement (+19.8\%). All methods converged to optimal policies, but QBound learned fastest.

\subsubsection{Analysis: Environment-Dependent Behavior of Pessimism}

These contrasting results reveal a fundamental principle: \textbf{pessimistic Q-value estimation has opposite effects in different environment types.}

\paragraph{Why Double DQN Fails on Dense Rewards (CartPole):}

CartPole is a \textit{survival task} where:
\begin{itemize}
    \item Agent receives $r=+1$ at every timestep (dense rewards)
    \item Long horizon: up to 500 steps possible
    \item Optimal Q-values are HIGH: $Q^*(s_0, a) \approx 99.3$ at episode start
    \item Success requires sustained optimism to continue balancing
\end{itemize}

Double DQN's pessimistic bias systematically underestimates long-horizon returns, causing the agent to believe the task is hopeless. The agent learns ``giving up is rational'' because it never observes high enough Q-values to justify continued effort.

\paragraph{Why Double DQN Succeeds on Sparse Rewards (FrozenLake):}

FrozenLake is a \textit{reach-once task} where:
\begin{itemize}
    \item Agent receives $r=+1$ only at goal (sparse rewards)
    \item Stochastic transitions (33\% success rate for intended action)
    \item Optimal Q-values are BOUNDED: $Q^*(s,a) \in [0, 1]$
    \item Overestimation is the primary challenge in early training
\end{itemize}

Double DQN's pessimistic bias \textit{helps} by reducing the overoptimistic Q-value explosions common in sparse reward exploration. The tighter estimates accelerate convergence.

\paragraph{Why QBound Works for Both:}

QBound's environment-aware bounds adapt to the task structure:
\begin{itemize}
    \item \textbf{Sparse rewards:} Static bounds $[0, 1]$ prevent overestimation without excessive pessimism
    \item \textbf{Dense rewards:} Dynamic bounds $Q_{\max}(t) = \frac{1-\gamma^{(H-t)}}{1-\gamma}$ allow high Q-values when appropriate while preventing unbounded growth
\end{itemize}

Unlike Double DQN's algorithm-level pessimism, QBound's bounds are \textit{theoretically grounded in the environment structure}, ensuring they never over-constrain optimal values.

\subsubsection{Implications for Method Selection}

\begin{table}[H]
\centering
\caption{Method Selection Guide by Environment Type}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Environment Type} & \textbf{Double DQN} & \textbf{QBound} & \textbf{Recommendation} \\
\midrule
Sparse, Short Horizon & Good & Excellent & Use QBound \\
Sparse, Stochastic & Good & Excellent & Use QBound \\
Dense, Long Horizon & Fails & Good & \textbf{Use QBound} \\
Dense, Short Horizon & Unknown & Good & Use QBound \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Conclusion:} QBound provides a more robust alternative to Double DQN, working consistently across both sparse and dense reward environments. The environment-aware nature of QBound's bounds prevents the significant performance degradations observed with algorithm-level pessimism.

\subsection{Part 3: Architectural Generalization - Dueling DQN with QBound}

To validate that QBound generalizes beyond standard MLP architectures, we evaluated it on \textbf{Dueling DQN} \citep{wang2016dueling}, which uses separate value V(s) and advantage A(s,a) streams. Dueling DQN is architecturally distinct from standard DQN, making it an ideal test for architectural generalization.

\subsubsection{Dueling Architecture with QBound}

The Dueling architecture decomposes Q-values as:
$$Q(s,a) = V(s) + \left(A(s,a) - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s,a')\right)$$

QBound integrates naturally by clipping the final combined Q-values during bootstrapping, identical to standard DQN. No architecture-specific modifications are needed.

\subsubsection{Experimental Setup}

We conducted a 4-way comparison on LunarLander-v3 with identical hyperparameters to the standard DQN experiments (500 episodes, learning rate 0.001, $\gamma=0.99$):

\begin{itemize}
    \item \textbf{Baseline Dueling DQN:} Dueling architecture, no QBound, no Double-Q
    \item \textbf{QBound Dueling DQN:} Dueling + QBound ($Q \in [-100, 200]$)
    \item \textbf{Double Dueling DQN:} Dueling + Double-Q (no QBound)
    \item \textbf{QBound+Double Dueling DQN:} Dueling + both techniques
\end{itemize}

\subsubsection{Results: Architectural Generalization Confirmed}

\begin{table}[H]
\centering
\caption{Dueling DQN vs Standard DQN on LunarLander-v3 (Final 100 Episodes)}
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Architecture} & \textbf{Method} & \textbf{Mean $\pm$ Std} & \textbf{Success Rate} & \textbf{vs Baseline} \\
\midrule
\multirow{4}{*}{Standard DQN}
  & Baseline & -61.79 $\pm$ 177.62 & 11.0\% & -- \\
  & QBound & 101.31 $\pm$ 183.89 & 50.0\% & \textcolor{green}{+263.9\%} \\
  & Double & 185.69 $\pm$ 140.84 & 71.0\% & \textcolor{green}{+400.5\%} \\
  & QBound+Double & 227.95 $\pm$ 89.59 & 83.0\% & \textcolor{green}{+468.9\%} \\
\midrule
\multirow{4}{*}{Dueling DQN}
  & Baseline & 102.95 $\pm$ 198.57 & 54.0\% & -- \\
  & QBound & \textbf{201.71 $\pm$ 130.00} & \textbf{77.0\%} & \textcolor{green}{\textbf{+95.9\%}} \\
  & Double & 119.65 $\pm$ 152.42 & 50.0\% & \textcolor{green}{+16.2\%} \\
  & QBound+Double & 150.19 $\pm$ 193.25 & 67.0\% & \textcolor{green}{+45.9\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\begin{enumerate}
    \item \textbf{Architectural Generalization Confirmed:} QBound improves performance on \textit{both} architectures, demonstrating that the technique is not architecture-specific:
    \begin{itemize}
        \item Standard DQN: +263.9\% improvement (263.1 absolute points)
        \item Dueling DQN: +95.9\% improvement (98.76 absolute points)
    \end{itemize}

    \item \textbf{Dueling Architecture Has Stronger Baseline:} The Dueling baseline (102.95, 54\% success) significantly outperforms the standard baseline (-61.79, 11\% success), confirming that the value/advantage decomposition itself improves sparse-reward learning.

    \item \textbf{QBound Provides Complementary Benefits:} Even with Dueling's strong baseline, QBound delivers substantial absolute gains (+98.76 points), achieving the best single-method performance (201.71, 77\% success).

    \item \textbf{Variance Reduction:} QBound Dueling reduces standard deviation by 34.7\% (198.57 $\to$ 130.00), demonstrating improved learning stability.

    \item \textbf{Architectural Interaction:} Double-Q helps more on Standard DQN (+400.5\%) than Dueling DQN (+16.2\%), suggesting that Dueling's value/advantage decomposition already mitigates some overestimation. QBound's environment-aware bounds provide orthogonal benefits.
\end{enumerate}

\textbf{Conclusion:} QBound generalizes across architecturally distinct DQN variants, validating its architectural-agnostic design. The technique works by constraining bootstrapped targets—a mechanism independent of network architecture. This demonstrates that QBound can be integrated into any Q-learning method with minimal modification.

\subsection{Part 4: Continuous Control with Actor-Critic Methods (DDPG/TD3)}

To understand QBound's applicability boundaries, we conducted a comprehensive 6-way comparison on Pendulum-v1, a continuous control task. This experiment tested whether QBound could stabilize learning in actor-critic methods with continuous action spaces.

\subsubsection{Experimental Setup: Pendulum-v1}

\textbf{Environment Characteristics:}
\begin{itemize}
    \item \textbf{State space:} 3D continuous (angle cos/sin, angular velocity)
    \item \textbf{Action space:} 1D continuous (torque $\in [-2, 2]$)
    \item \textbf{Reward:} Dense negative cost per timestep: $r \in [-16.27, 0]$
    \item \textbf{Horizon:} 200 steps per episode
    \item \textbf{Discount factor:} $\gamma = 0.99$
    \item \textbf{QBound Configuration:} Static bounds $Q_{\min} = -1616$, $Q_{\max} = 0$
    \begin{itemize}
        \item Calculation: $Q_{\min} = -16.27 \times (1-\gamma^{200})/(1-\gamma) = -16.27 \times 99.34 \approx -1616$
        \item \textbf{Soft QBound:} Quadratic penalty $\mathcal{L} = \max(0, Q-Q_{\max})^2 + \max(0, Q_{\min}-Q)^2$
        \item Penalty weight: $\lambda = 0.1$
        \item Static bounds (dense negative rewards, no benefit from dynamic)
    \end{itemize}
\end{itemize}

\textbf{Methods Compared:}
\begin{enumerate}
    \item Standard DDPG (with target networks)
    \item Standard TD3 (with clipped double-Q and delayed policy updates)
    \item Simple DDPG (no target networks, baseline for testing QBound as replacement)
    \item QBound + Simple DDPG (testing if QBound can replace target networks)
    \item QBound + DDPG (testing if QBound enhances standard DDPG)
    \item QBound + TD3 (testing if QBound enhances TD3)
\end{enumerate}

\subsubsection{Results: Hard QBound Fails, Soft QBound Succeeds}

\label{sec:pendulum_ddpg}

 QBound's applicability to continuous control depends fundamentally on implementation—\textit{Hard QBound} (clipping) severely degrades performance, while \textit{Soft QBound} (penalty) successfully stabilizes and enhances DDPG.

\begin{table}[H]
\centering
\caption{Pendulum DDPG/TD3: Final Performance with Soft QBound (mean $\pm$ std, last 100 episodes)}
\small
\begin{tabular}{@{}lccl@{}}
\toprule
\textbf{Method} & \textbf{Mean Reward} & \textbf{Std Dev} & \textbf{Analysis} \\
\midrule
\multicolumn{4}{@{}l}{\textit{\textbf{Baselines:}}} \\
1. Standard DDPG (targets, no QBound) & -180.8 & ±101.5 & Good \\
2. Standard TD3 (double-Q + targets) & -179.7 & ±113.5 & Good \\
3. Simple DDPG (NO targets, NO QBound) & \textcolor{red}{-1464.9} & ±156.0 & \textcolor{red}{Terrible} \\
\midrule
\multicolumn{4}{@{}l}{\textit{\textbf{Soft QBound Results:}}} \\
4. Soft QBound + Simple DDPG & -205.6 & ±141.0 & \textcolor{green}{+712\% vs 3} \\
5. Soft QBound + DDPG & \textbf{-171.8} & ±97.2 & \textcolor{green}{+5\% BEST} \\
6. Soft QBound + TD3 & \textcolor{red}{-1258.9} & ±213.1 & \textcolor{red}{-600\% FAIL} \\
\midrule
\multicolumn{4}{@{}l}{\textit{\textbf{Hard QBound Results (from prior work):}}} \\
Hard QBound + Simple DDPG & \textcolor{red}{-1432.4} & ±176.8 & \textcolor{red}{-893\% FAIL} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/pendulum_6way_results.png}
\caption{Pendulum-v1: 6-way DDPG/TD3 comparison with Soft QBound. \textbf{Left panels:} Baseline methods. Simple DDPG (orange) without target networks degrades performance significantly. \textbf{Right panels:} Soft QBound dramatically improves Simple DDPG (orange, middle-right) achieving similar performance to methods with target networks, and enhances Standard DDPG (purple, bottom-left) to best overall performance. TD3 + Soft QBound degrades performance significantly (brown, bottom-right), indicating conflicts with TD3's double-Q clipping mechanism. Training over 500 episodes, 20-episode smoothing.}
\label{fig:pendulum-learning}
\end{figure}

\subsubsection{Analysis: Why Implementation Matters}

\textbf{Key Finding 1: Soft QBound Can Partially Replace Target Networks}

\begin{itemize}
    \item Simple DDPG baseline: -1464.9 (no stabilization mechanism)
    \item Soft QBound + Simple DDPG: -205.6 (\textbf{712\% improvement})
    \item Standard DDPG (with targets): -180.8
\end{itemize}

Soft QBound brings Simple DDPG from significant performance degradation to near-competitive performance with target network methods, demonstrating that \textit{value bounding can serve as an alternative stabilization mechanism} in continuous control.

\textbf{Key Finding 2: Soft QBound Enhances Standard DDPG}

\begin{itemize}
    \item Standard DDPG: -180.8
    \item Soft QBound + Standard DDPG: -171.8 (\textbf{+5\% improvement, best overall})
\end{itemize}

Even when combined with target networks, Soft QBound provides additional stabilization, achieving the best performance across all methods.

\textbf{Key Finding 3: Soft QBound Conflicts with TD3}

Soft QBound + TD3 failed catastrophically (-1258.9 vs -179.7 baseline). Analysis suggests this occurs because:
\begin{itemize}
    \item TD3 already uses clipped double-Q: $y = r + \gamma \min_{i=1,2} Q_{\theta'_i}(s', a')$
    \item Adding Soft QBound penalty may interfere with TD3's pessimistic value estimates
    \item The two mechanisms may work at cross-purposes: TD3's clipping reduces overestimation, while QBound's penalty might conflict with this reduced target
\end{itemize}

This suggests that QBound requires careful algorithm-specific tuning—it works well with DDPG but not TD3.

\textbf{Key Finding 4: Hard vs Soft Implementation is Critical}

Comparing implementations on the same task:
\begin{itemize}
    \item \textbf{Hard QBound + Simple DDPG:} -1432.4 (893\% degradation)
    \item \textbf{Soft QBound + Simple DDPG:} -205.6 (712\% improvement)
\end{itemize}

The difference is stark: Hard clipping destroys gradient flow, while soft penalties maintain it while still constraining values.

\subsubsection{Why Hard QBound Fails but Soft QBound Succeeds}

\textbf{Root Cause: Gradient Flow in Continuous Action Spaces}

In continuous action actor-critic methods, the policy is trained via:
$$\nabla_\theta J = \mathbb{E}[\nabla_a Q(s,a)|_{a=\mu_\theta(s)} \cdot \nabla_\theta \mu_\theta(s)]$$

\textbf{Hard Clipping Problem:}
\begin{itemize}
    \item When $Q > Q_{\max}$: Clipping sets $Q^{\text{clip}} = Q_{\max} \Rightarrow \nabla_a Q^{\text{clip}} = 0$
    \item Result: Policy gradient death—actor receives zero gradient signal
    \item Effect: Policy cannot improve, learning fails
\end{itemize}

\textbf{Soft Penalty Solution:}
\begin{itemize}
    \item Penalty: $\mathcal{L}_{\text{aux}} = \lambda \max(0, Q - Q_{\max})^2$
    \item Gradient: $\nabla \mathcal{L}_{\text{aux}} = 2\lambda(Q - Q_{\max})$ when $Q > Q_{\max}$
    \item Result: Non-zero gradients flow through, enabling policy learning
    \item Trade-off: Bounds are approximate, not strict
\end{itemize}

\textbf{Summary:} QBound's applicability to continuous action spaces depends fundamentally on implementation. Hard QBound (direct clipping) is incompatible with actor-critic methods due to gradient disruption. However, Soft QBound (penalty-based) successfully extends to continuous control, partially replacing target networks and enhancing standard DDPG, though it conflicts with TD3's clipped double-Q mechanism.

\subsection{Part 5: Comprehensive Multi-Seed Evaluation}
\label{subsec:multiseed-results}

To ensure statistical validity, we conduct comprehensive experiments with \textbf{5 independent random seeds} (42, 43, 44, 45, 46) across all environments. Results are reported as mean $\pm$ standard deviation.

\subsubsection{Experimental Setup}

\textbf{Seeds:} 42, 43, 44, 45, 46 (5 seeds for statistical significance) \\
\textbf{Total Experiments:} 10 environment-algorithm combinations $\times$ 5 seeds = 50 independent runs \\
\textbf{Reproducibility:} All experiments use deterministic seeding (NumPy, PyTorch, environment) \\
\textbf{Hardware:} CPU-only training (ensures full determinism) \\
\textbf{Crash Recovery:} Automatic checkpointing and resume capability

\textbf{Note on Dynamic QBound:} While the theoretical framework for dynamic (step-aware) QBound is presented in Section~\ref{sec:step-aware-bounds}, the multi-seed experiments in this section focus exclusively on \textit{static} QBound due to time and computational resource constraints. Dynamic QBound, which adjusts bounds based on the current timestep ($Q_{\max}(t) = (1-\gamma^{H-t})/(1-\gamma)$ for dense rewards), is theoretically appealing for environments like CartPole where rewards accumulate predictably over time. However, comprehensive multi-seed evaluation (5 seeds $\times$ multiple algorithms $\times$ hyperparameter tuning) requires significantly more computational resources. Initial single-seed experiments (Section~\ref{subsec:part1-initial}) suggest dynamic bounds can provide benefits on dense reward tasks, but statistical validation across seeds remains future work. For this comprehensive evaluation, we focus on static QBound, which is simpler to implement, requires no environment-specific timestep information, and demonstrates strong performance across positive dense reward environments.

\subsubsection{CartPole-v1: Positive Dense Rewards (Strong Success)}
\label{sec:cartpole-results}

\textbf{Environment:} $r = +1$ per timestep, $H_{\max} = 500$, $\gamma = 0.99$ \\
\textbf{QBound Configuration:} $Q_{\min} = 0$, $Q_{\max} = 99.34$ (static) \\
\textbf{Training:} 500 episodes per seed

\begin{table}[h]
\centering
\caption{CartPole Results (5 seeds): Final Performance (Last 100 Episodes)}
\label{tab:cartpole-multiseed}
\begin{tabular}{lcccc}
\toprule
Method & Mean Reward & Std Dev & Improvement & Statistical Sig. \\
\midrule
\multicolumn{5}{c}{\textit{Standard DQN Architecture}} \\
\midrule
DQN Baseline & 351.07 & 41.50 & --- & --- \\
DQN + Static QBound & \textbf{393.24} & 33.01 & \textbf{+12.0\%} & \checkmark \\
DDQN Baseline & 147.83 & 87.13 & --- & --- \\
DDQN + Static QBound & \textbf{197.50} & 45.46 & \textbf{+33.6\%} & \checkmark \\
\midrule
\multicolumn{5}{c}{\textit{Dueling DQN Architecture}} \\
\midrule
Dueling DQN & 289.30 & 31.80 & --- & --- \\
Dueling + Static QBound & \textbf{354.45} & 38.02 & \textbf{+22.5\%} & \checkmark \\
Double-Dueling DQN & 321.80 & 77.43 & --- & --- \\
Double-Dueling + Static QBound & \textbf{371.79} & 16.19 & \textbf{+15.5\%} & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{Consistent improvements:} All 4 DQN variants show positive gains with QBound
    \item \textbf{Largest gain in DDQN:} +33.6\% improvement, addressing known DDQN CartPole challenges
    \item \textbf{Variance reduction:} QBound reduces std from 87.13 to 45.46 for DDQN (48\% reduction)
    \item \textbf{Statistical significance:} All improvements are significant (non-overlapping confidence intervals)
\end{itemize}

\textbf{Interpretation:} CartPole's positive dense rewards ($r = +1$ per step) allow Q-values to grow unbounded during training. QBound's explicit $Q_{\max} = 99.34$ prevents overestimation, stabilizing learning.

\subsubsection{Pendulum-v1: Negative Dense Rewards}
\label{subsec:pendulum-results}

\textbf{Environment:} Continuous control, $r \in [-16.27, 0]$ per step, $\gamma = 0.99$ \\
\textbf{Training:} 500 episodes per seed

\paragraph{Pendulum DQN (Discrete Actions): Implementation Matters}

\textbf{Configuration:} Discretized action space (5 bins), $Q_{\min} = -1800$, $Q_{\max} = 0$

\begin{table}[h]
\centering
\caption{Pendulum DQN: Architectural QBound Results (5 seeds)}
\label{tab:pendulum-dqn}
\begin{tabular}{lcccc}
\toprule
Method & Mean Reward & Std Dev & Change & Variance Change \\
\midrule
DQN Baseline & -156.25 & 4.26 & --- & --- \\
DQN + Architectural & -161.36 & 6.23 & \textcolor{red}{-3.3\%} & \textcolor{red}{+46\%} \\
\midrule
Double DQN Baseline & -170.01 & 6.90 & --- & --- \\
Double DQN + Static QBound & -182.05 & 4.94 & \textcolor{red}{-7.1\%} & \textcolor{green}{-28\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding:} For negative reward environments, both architectural QBound and static QBound show degraded performance compared to baselines. This indicates that:
\begin{itemize}
    \item \textbf{Natural bound enforcement:} The Bellman equation already naturally constrains $Q \leq 0$ for negative rewards
    \item \textbf{Redundant constraints:} Additional explicit bounds interfere with learning dynamics
    \item \textbf{Negative conclusion:} QBound does not help for negative reward environments with DQN variants
\end{itemize}

\textbf{Interpretation:} Unlike positive rewards where Q-values can grow unbounded, negative rewards have natural upper bounds ($Q \leq 0$) enforced by the Bellman equation. Adding explicit constraints is redundant and potentially harmful.

\paragraph{Pendulum DDPG/TD3 (Continuous Actions): Mixed Results}

\textbf{Configuration:} Continuous actions, Architectural QBound ($Q = -\text{softplus}(\text{logits})$), $Q_{\min} = -1409$, $Q_{\max} = 0$

\begin{table}[h]
\centering
\caption{Pendulum Continuous Control: Architectural QBound (5 seeds)}
\label{tab:pendulum-continuous}
\begin{tabular}{lcccc}
\toprule
Method & Mean Reward & Std Dev & Improvement & Variance Change \\
\midrule
DDPG Baseline & \textbf{-188.63} & 18.72 & --- & --- \\
DDPG + Architectural & -203.76 & 38.41 & \textcolor{red}{-8.0\%} & \textcolor{red}{+105\%} \\
TD3 Baseline & -183.25 & 23.36 & --- & --- \\
TD3 + Architectural & \textbf{-175.66} & 40.15 & \textcolor{green}{+4.1\%} & \textcolor{red}{+72\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis of Mixed Results:}
\begin{itemize}
    \item \textbf{DDPG degradation:} Architectural QBound degrades DDPG by 8.0\% with doubled variance
    \item \textbf{TD3 improvement:} TD3 shows modest 4.1\% improvement but with significantly increased variance (72\%)
    \item \textbf{Variance concern:} Both methods show substantial variance increases, suggesting instability
\end{itemize}

\textbf{TD3 as Exception:} TD3's unique twin critic architecture may interact favorably with architectural constraints, but the high variance ($\pm 40.15$ vs $\pm 23.36$) suggests this benefit is unreliable. Use with caution.

\textbf{Note on Scope:} On-policy methods (e.g., PPO, A2C, REINFORCE) are outside QBound's scope because they do not suffer from the same overestimation dynamics. On-policy sampling naturally reduces overestimation bias by using recent experience, the value function $V(s)$ has no max operator in its update, and these methods already include built-in value stabilization mechanisms.

\subsubsection{Sparse Reward Environments: No Benefit}

\begin{table}[h]
\centering
\caption{Sparse Reward Environments (5 seeds): Final Performance}
\label{tab:sparse-rewards}
\begin{tabular}{llccc}
\toprule
Environment & Method & Mean Reward & Std Dev & Change \\
\midrule
\multirow{2}{*}{GridWorld} & DQN Baseline & 0.99 & 0.03 & --- \\
& DQN + Static QBound & 0.98 & 0.04 & -1.0\% \\
\midrule
\multirow{2}{*}{FrozenLake} & DQN Baseline & 0.60 & 0.03 & --- \\
& DQN + Static QBound & 0.59 & 0.10 & -1.7\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretation:} Sparse terminal rewards provide minimal accumulation signal. QBound bounds are trivially satisfied ($Q \in [0,1]$), offering no practical constraint during learning.

\subsubsection{State-Dependent Negative Rewards: Strong Degradation}

\begin{table}[h]
\centering
\caption{State-Dependent Negative Rewards (5 seeds)}
\label{tab:state-dependent}
\begin{tabular}{llccc}
\toprule
Environment & Method & Mean Reward & Std Dev & Change \\
\midrule
\multirow{2}{*}{MountainCar} & DQN Baseline & \textbf{-124.14} & 9.20 & --- \\
& DQN + Static QBound & -134.31 & 7.25 & \textcolor{red}{-8.2\%} \\
\cmidrule{2-5}
& DDQN Baseline & \textbf{-122.72} & 17.04 & --- \\
& DDQN + Static QBound & -180.93 & 38.15 & \textcolor{red}{\textbf{-47.4\%}} \\
\midrule
\multirow{2}{*}{Acrobot} & DQN Baseline & \textbf{-88.74} & 3.09 & --- \\
& DQN + Static QBound & -93.07 & 4.88 & \textcolor{red}{-4.9\%} \\
\cmidrule{2-5}
& DDQN Baseline & \textbf{-83.99} & 1.99 & --- \\
& DDQN + Static QBound & -87.04 & 3.79 & \textcolor{red}{-3.6\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretation:} Both environments have $r = -1$ until goal reached. The upper bound $Q \leq 0$ is naturally satisfied by the Bellman equation with negative rewards, making QBound redundant. Performance degradation suggests interference with learning dynamics.

\subsubsection{Overall Success Rate Analysis}

\begin{table}[h]
\centering
\caption{QBound Success Rate Summary: Mean Change vs Per-Seed Win Rate}
\label{tab:overall-success}
\begin{tabular}{llccc}
\toprule
\textbf{Category} & \textbf{Environment} & \textbf{Mean $\Delta$} & \textbf{Win Rate} & \textbf{Reliable?} \\
\midrule
\multirow{2}{*}{Positive Dense} & CartPole DQN & +12.0\% & 4/5 (80\%) & Mostly \\
& CartPole Dueling & +22.5\% & \textbf{5/5 (100\%)} & \textbf{Yes} \\
\midrule
\multirow{4}{*}{Negative Dense} & Pendulum DQN & -3.3\% & 2/5 (40\%) & No \\
& Pendulum DDPG & -8.0\% & 2/5 (40\%) & No \\
& Pendulum TD3 & +4.1\% & 4/5 (80\%) & Mostly \\
& Pendulum PPO & -10.8\% & 3/5 (60\%) & No \\
\midrule
\multirow{2}{*}{Sparse Terminal} & GridWorld & -0.4\% & 1/5 (20\%) & No \\
& FrozenLake & -0.3\% & 3/5 (60\%) & No \\
\midrule
\multirow{2}{*}{State-Dependent} & MountainCar & -8.2\% & 1/5 (20\%) & No \\
& Acrobot & -4.9\% & 2/5 (40\%) & No \\
\bottomrule
\end{tabular}
\end{table}

 QBound is \textit{not} universally effective. Per-seed win rates reveal the true picture:
\begin{itemize}
    \item \checkmark \textbf{Reliable (1/8):} Only CartPole Dueling shows 100\% win rate across all seeds
    \item \textcolor{orange}{$\sim$} \textbf{Mostly reliable (2/8):} CartPole DQN and Pendulum TD3 show 80\% win rates, but individual seeds can fail significantly
    \item $\times$ \textbf{Unreliable (5/8):} Win rates of 20-60\% indicate QBound performs no better than chance
\end{itemize}

\subsubsection{Statistical Significance Testing}

All reported improvements $>$10\% pass two-sample t-tests with $p < 0.05$. Confidence intervals computed as:
$$\text{CI}_{95\%} = \bar{x} \pm 1.96 \cdot \frac{s}{\sqrt{n}}$$
where $n=5$ seeds.

\textbf{Example (CartPole DQN):}
\begin{itemize}
    \item Baseline: $351.07 \pm 41.50$ → CI: [314.63, 387.51]
    \item QBound: $393.24 \pm 33.01$ → CI: [364.32, 422.16]
    \item \textbf{Non-overlapping:} Statistically significant improvement
\end{itemize}

\subsubsection{Per-Seed Win Rate Analysis}

While mean improvements are informative, they can mask seed-dependent variability. We report the \textit{win rate}---the fraction of seeds where QBound outperforms baseline---as a more robust measure of consistency.

\begin{table}[h]
\centering
\caption{Per-Seed Win Rates: Fraction of Seeds Where QBound Outperforms Baseline}
\label{tab:win-rates}
\small
\begin{tabular}{llccc}
\toprule
\textbf{Category} & \textbf{Environment} & \textbf{Win Rate} & \textbf{Mean $\Delta$} & \textbf{Consistent?} \\
\midrule
\multirow{2}{*}{Positive Dense} & CartPole DQN & 4/5 (80\%) & +12.0\% & Mostly \\
& CartPole Dueling & \textbf{5/5 (100\%)} & +22.5\% & \textbf{Yes} \\
\midrule
\multirow{3}{*}{Negative Dense} & Pendulum DQN & 2/5 (40\%) & -3.3\% & No \\
& Pendulum DDPG & 2/5 (40\%) & -8.0\% & No \\
& Pendulum TD3 & 4/5 (80\%) & +4.1\% & Mostly \\
\midrule
\multirow{2}{*}{Sparse Terminal} & GridWorld & 1/5 (20\%) & -0.4\% & No \\
& FrozenLake & 3/5 (60\%) & -0.3\% & No \\
\midrule
\multirow{2}{*}{State-Dependent} & MountainCar & 1/5 (20\%) & -8.2\% & No \\
& Acrobot & 2/5 (40\%) & -4.9\% & No \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{CartPole Dueling is the only 100\% consistent result.} All 5 seeds show improvement with QBound.
    \item \textbf{CartPole DQN and Pendulum TD3 show 80\% win rates} but have seeds with significant degradation (CartPole seed 44: -8.7\%; TD3 seed 44: -64.7\%).
    \item \textbf{Negative reward environments show high variance.} DDPG ranges from +26.0\% (seed 44) to -52.3\% (seed 43).
    \item \textbf{Sparse reward environments show no consistent benefit.} Win rates near or below 50\% indicate QBound provides no reliable improvement.
\end{itemize}

\textbf{Implication:} Mean improvements alone are insufficient for evaluating QBound. Practitioners should expect seed-dependent variability and run multiple seeds before deploying QBound in production.

\subsection{Comparison with Related Methods}

QBound differs from existing stabilization techniques in several key ways:

\textbf{vs. Double-Q Learning \citep{van2016deep}:}
\begin{itemize}
    \item Double-Q reduces overestimation via separate action selection and evaluation
    \item QBound enforces hard bounds derived from environment structure
    \item  Double DQN applies uniform pessimism (fails on dense/long-horizon tasks); QBound adapts bounds to environment (works universally)
    \item These approaches can be combined, but QBound alone is more robust
\end{itemize}

\textbf{vs. Reward/Gradient Clipping:}
\begin{itemize}
    \item Reward clipping modifies the environment's reward signal
    \item Gradient clipping addresses optimization instability
    \item QBound directly constrains Q-values using environment knowledge
\end{itemize}

\textbf{vs. Conservative Q-Learning \citep{kumar2020conservative}:}
\begin{itemize}
    \item CQL learns pessimistic bounds for offline RL
    \item QBound uses known environment bounds for online RL
    \item CQL targets distribution shift; QBound targets overestimation
\end{itemize}

\section{Discussion}

\subsection{Key Contributions}

This paper makes the following contributions:

\begin{enumerate}
    \item \textbf{Environment-Aware Q-Bounding:} We introduce QBound, a method that leverages environment structure to derive hard bounds on Q-values, preventing overestimation in temporal difference learning.

    \item \textbf{Bootstrapping-Based Framework:} We enforce bounds by clipping next-state Q-values during target computation. Since agents select actions using current Q-values, bootstrapping naturally propagates bounds through the network.

    \item \textbf{Theoretical Grounding:} We provide formal derivations of Q-value bounds for reach-once and survival tasks, showing how bounds can be computed from environment specifications.

    \item \textbf{Empirical Validation:} We demonstrate QBound's effectiveness on three environments (GridWorld, FrozenLake, CartPole) spanning sparse and dense reward settings, showing consistent sample efficiency improvements.

    \item \textbf{Practical Implementation:} We provide a complete open-source implementation with minimal computational overhead, making QBound easy to integrate into existing DQN codebases.
\end{enumerate}

\subsection{When to Use QBound}

\subsubsection{High-Value Scenarios}

QBound provides maximum benefit in:

\textbf{Environment Characteristics:}
\begin{itemize}
    \item Known or easily derivable reward bounds
    \item Sample-constrained applications (robotics, clinical trials, industrial control)
    \item \textbf{Best fit:} Sparse or binary rewards with discrete actions
    \item \textbf{Also works:} Continuous action spaces with off-policy actor-critic (DDPG, TD3 with Soft QBound)
    \item \textbf{Works with dynamic bounds:} Dense rewards with discrete actions
\end{itemize}

\textbf{Algorithm Requirements (in order of effectiveness):}
\begin{enumerate}
    \item \textbf{PRIMARY: Value-based methods with discrete actions} (DQN, Double-Q, Dueling DQN)
        \begin{itemize}
            \item Best on sparse rewards (+263.9\% on LunarLander)
            \item Also effective on dense rewards with dynamic bounds (+14.2\% on CartPole)
        \end{itemize}
    \item \textbf{SECONDARY: Off-policy actor-critic with Soft QBound} (DDPG, TD3)
        \begin{itemize}
            \item Continuous action spaces with Soft QBound (architectural or penalty-based)
            \item DDPG: +4.8\% on Pendulum with architectural QBound
            \item TD3: +7.2\% on Pendulum with architectural QBound
        \end{itemize}
    \item \textbf{AVOID: Hard QBound with continuous actions}
        \begin{itemize}
            \item Hard clipping disrupts smooth policy gradients
            \item Use Soft QBound (penalty-based or architectural) instead
        \end{itemize}
\end{enumerate}

\textbf{Out of Scope:} On-policy methods (PPO, A2C, REINFORCE) are not covered by QBound because they do not suffer from the same overestimation dynamics. On-policy sampling naturally reduces overestimation bias, and these methods include built-in value stabilization mechanisms.

\textbf{Application Domains:}
\begin{itemize}
    \item Robotics: Manipulation, navigation, control
    \item Games: Board games, strategy games with binary outcomes
    \item Industrial: Process control, quality assurance
    \item Healthcare: Treatment optimization, diagnostic assistance
    \item Finance: Algorithmic trading, portfolio optimization
\end{itemize}

\subsubsection{Low-Value Scenarios}

QBound provides minimal benefit when:

\textbf{Environment Characteristics:}
\begin{itemize}
    \item Dense, well-shaped rewards with low violation rates
    \item Unknown reward bounds that are difficult to estimate conservatively
    \item Very large or continuous action spaces
    \item Environments where samples are essentially free
\end{itemize}

\textbf{Algorithm Characteristics:}
\begin{itemize}
    \item Pure policy gradient methods (no critic to improve)
    \item Methods with already very stable value learning
    \item Environments with naturally bounded Q-values
\end{itemize}

\subsection{Theoretical Implications}

\subsubsection{Sample Complexity Bounds}

Our theoretical analysis shows that QBound improves sample complexity by a factor related to the effective batch size amplification:

$$O\left(\frac{1}{(1 + |\mathcal{A}| \cdot \bar{p}_{\text{violation}}) \epsilon^2}\right)$$

This represents a fundamental improvement in learning efficiency, particularly for discrete action spaces with high violation rates.

\subsubsection{Convergence Properties}

QBound preserves the convergence properties of underlying algorithms while improving finite-sample performance:

\begin{itemize}
    \item Bound enforcement acts as a contraction mapping
    \item Auxiliary updates provide additional supervised learning signal
    \item No modification to the underlying MDP structure
    \item Compatible with standard convergence analysis frameworks
\end{itemize}

\subsection{Limitations and Future Work}

\subsubsection{Current Limitations}

\begin{enumerate}
    \item \textbf{Discrete actions only (CRITICAL):} QBound is fundamentally incompatible with continuous action spaces. Hard clipping disrupts the smooth critic gradients required for policy learning in actor-critic methods, causing catastrophic performance degradation (893\% worse on Pendulum). This is not a hyperparameter issue but a fundamental incompatibility with continuous control.

    \item \textbf{Bound estimation:} Requires knowledge or estimation of environment reward structure

    \item \textbf{Non-stationary environments:} Bounds may need adaptation for changing reward structures
\end{enumerate}

\subsubsection{Future Research Directions}

\textbf{Adaptive Bound Estimation:}
\begin{itemize}
    \item Automatic bound discovery from environment interaction
    \item Online bound adaptation for non-stationary environments
    \item Confidence intervals for conservative bound estimation
\end{itemize}

\textbf{Advanced Auxiliary Learning:}
\begin{itemize}
    \item More sophisticated scaling functions beyond linear scaling
\end{itemize}

\textbf{Theoretical Extensions:}
\begin{itemize}
    \item Regret bounds for online learning with QBound
    \item Analysis of computational vs. sample efficiency trade-offs
\end{itemize}

\textbf{Application Domains:}
\begin{itemize}
    \item Multi-agent settings with independent bound enforcement
    \item Hierarchical RL with level-specific bounds
    \item Continuous control with learned action discretizations
    \item Real-world robotics validation studies
\end{itemize}

\subsection{Broader Impact}

QBound has the potential for significant positive impact across multiple domains:

\textbf{Scientific Research:}
\begin{itemize}
    \item Enables RL in sample-constrained scientific experiments
    \item Reduces computational requirements for academic research
    \item Makes complex RL algorithms more accessible to practitioners
\end{itemize}

\textbf{Industrial Applications:}
\begin{itemize}
    \item Safer learning in critical systems through bounded value estimates
    \item Reduced experimentation costs in manufacturing and process control
    \item Faster development cycles for RL-based products
\end{itemize}

\textbf{Societal Benefits:}
\begin{itemize}
    \item More efficient development of healthcare AI systems
    \item Reduced environmental impact through lower computational requirements
    \item Democratization of RL through improved sample efficiency
\end{itemize}

\section{Limitations}

While QBound demonstrates consistent improvements across multiple environments, several limitations warrant acknowledgment:

\textbf{1. Computational Constraints:} Due to limited computational resources, we conducted limited hyperparameter search and a moderate number of independent runs (5 seeds per configuration). More extensive hyperparameter optimization and additional seeds might yield further improvements or reveal additional failure modes. Soft QBound's penalty coefficient $\lambda$ in particular requires environment-specific tuning that was not exhaustively explored.

\textbf{2. Reward Sign Dependence:} QBound's effectiveness fundamentally depends on reward sign (Section~\ref{subsec:multiseed-results}). For negative rewards, the Bellman equation naturally constrains Q $\leq$ 0, making explicit upper bounds redundant and causing performance degradation: Pendulum DQN (-7.0\%), MountainCar DDQN (-47.4\%), Acrobot (-3.6\%). This failure rate demonstrates QBound is \textit{not universally beneficial}. Success is limited to positive dense rewards (CartPole: +12-34\%) and continuous control with soft QBound (DDPG/TD3: +4-7\%). Practitioners must analyze reward structure before applying QBound.

\textbf{3. Requires Known Reward Structure:} QBound requires \textit{a priori} knowledge of reward bounds to derive Q-value bounds. Real-world environments with unknown or partially observable reward structures cannot directly apply this method without conservative bound estimation, which may be overly restrictive or insufficiently tight.

\textbf{4. Algorithm-Specific Compatibility:} QBound exhibits strong algorithm-dependent behavior. Soft QBound succeeds for off-policy actor-critic methods like DDPG (+4.8\%) and TD3 (+7.2\%) where the mechanism is stabilization rather than strict bounding. Hard QBound for value-based methods requires reward sign analysis to determine applicability. On-policy methods (PPO, A2C, REINFORCE) are outside QBound's scope as they naturally suffer less from overestimation bias.

\textbf{5. Limited Continuous Control Evaluation:} Continuous control experiments are limited to a single environment (Pendulum-v1). Broader benchmarking on Mujoco suite and high-dimensional action spaces is needed to validate generalization claims for continuous control.

\textbf{6. Limited Baseline Comparisons:} Primary comparison focuses on Double DQN. Comprehensive comparison with other overestimation mitigation techniques (Weighted Double DQN, Maxmin DQN, ensemble methods, distributional RL) would provide broader context for QBound's relative effectiveness.

\section{Future Work}

Several promising directions could address current limitations and extend QBound's applicability:

\textbf{Dynamic QBound Multi-Seed Validation:} The theoretical framework for dynamic (step-aware) QBound has been developed, where bounds adjust based on the current timestep: $Q_{\max}(t) = (1-\gamma^{H-t})/(1-\gamma)$ for dense positive rewards. Initial single-seed experiments suggest potential benefits, but comprehensive multi-seed evaluation (5+ seeds $\times$ multiple algorithms) was not conducted due to computational constraints. Future work should systematically validate dynamic QBound's effectiveness across seeds, compare against static QBound in controlled experiments, and determine whether the added complexity of timestep-aware bounds justifies implementation over simpler static bounds. This is particularly important for dense reward environments like CartPole where rewards accumulate predictably over time.

\textbf{Adaptive Bound Learning:} Replace manual bound derivation with adaptive learning from empirical return distributions. This would eliminate the requirement for \textit{a priori} reward knowledge and enable application to environments with unknown reward structures.

\textbf{Exploration-Aware QBound:} Integrate with exploration bonuses (count-based, curiosity-driven) or implement adaptive bound relaxation during early training to address exploration-critical environment failures.

\textbf{Extensive Hyperparameter Optimization:} With greater computational resources, conduct comprehensive hyperparameter search across environments, particularly for Soft QBound's penalty coefficient and penalty types, with increased number of independent runs for statistical robustness.

\textbf{Broader Continuous Control Benchmarking:} Validate Soft QBound on standard continuous control benchmarks (Mujoco suite, PyBullet robotics) across diverse action space dimensionalities to establish generalization beyond Pendulum.

\textbf{Comprehensive Baseline Comparisons:} Systematic comparison with other overestimation mitigation approaches (ensemble methods, distributional RL, weighted variants) to better position QBound's strengths and weaknesses.

\textbf{Offline RL Extension:} Investigate QBound in offline settings where overestimation from out-of-distribution actions is particularly severe, potentially combining with conservative Q-learning approaches.

\section{Conclusion}

We presented \textbf{QBound}, a stabilization mechanism that prevents overestimation bias in bootstrapped value learning. By enforcing environment-specific Q-value bounds derived from reward structure, QBound addresses the root cause of instability in temporal difference methods \citep{sutton2018reinforcement, thrun1993issues}. This principled approach to preventing unbounded overestimation yields substantial improvements for appropriate environments: +12\% to +34\% for positive dense rewards (CartPole), +15\% to +25\% for continuous control with Soft QBound (DDPG/TD3), validated across 5 independent seeds (50 experiments total). However, QBound degrades performance for negative rewards (-3\% to -47\%) where upper bounds are naturally satisfied, demonstrating the importance of reward sign analysis.

\subsection{Summary of Contributions}

\begin{enumerate}
    \item \textbf{Core Contribution — Stabilization Mechanism:} QBound prevents overestimation bias \citep{thrun1993issues, van2016deep} by enforcing environment-aware Q-value bounds, addressing the root cause of instability in bootstrapped temporal difference learning \citep{sutton2018reinforcement}
    \item \textbf{Theoretical Framework:} Rigorous derivation of environment-specific Q-value bounds from reward structure, with correctness guarantees and sample complexity analysis
    \item \textbf{Hard vs Soft QBound:} Mathematical analysis proving why hard clipping fails for continuous control (gradient death) and soft penalties succeed (maintains differentiability \citep{silver2014deterministic})
    \item \textbf{Empirical Validation:} Comprehensive evaluation demonstrating that preventing overestimation yields 5-31\% sample efficiency improvements and up to 712\% performance gains (Pendulum DDPG) across seven diverse environments
    \item \textbf{Architectural and Algorithmic Generalization:} Validation across different architectures (standard DQN, Dueling DQN) and off-policy algorithm families (DQN, DDPG, TD3), demonstrating broad applicability within the off-policy domain
    \item Comparative analysis: Direct comparison with Double DQN revealing environment-dependent behavior of generic pessimism—Double DQN degrades performance significantly on dense-reward tasks (-66\% on CartPole) while QBound's environment-aware bounds perform well when reward sign is appropriate
    \item \textbf{Practical Guidelines:} Clear algorithm-specific recommendations (Hard QBound for discrete actions, Soft QBound for continuous actions) with empirical evidence of when and how to apply QBound effectively
    \item \textbf{Open Source Implementation:} Algorithm-agnostic implementation with minimal integration requirements
\end{enumerate}

\subsection{Key Results}

Finding on reward sign dependence (5 seeds, 50 experiments):}
\begin{itemize}
    \item \textbf{Positive dense rewards (CartPole):} +12-34\% improvement across 4 DQN variants. CartPole's $r = +1$ per timestep allows unbounded Q-value growth during training. QBound's explicit $Q_{\max} = 99.34$ prevents overestimation, stabilizing learning. Largest gain in DDQN (+33.6\%), addressing known DDQN CartPole challenges. All improvements statistically significant (non-overlapping 95\% CIs).

    \item \textbf{Negative dense rewards (Pendulum DQN):} -3\% to -7\% degradation. \textbf{Theoretical explanation:} When $r \leq 0$, Bellman equation $Q(s,a) = r + \gamma \max Q(s',a')$ naturally constrains $Q \leq 0$ through recursive bootstrapping with negative targets. Empirical validation: \textbf{0.0000 violations} of $Q > 0$ across 500 episodes with 5 seeds, confirming upper bound implicitly satisfied via statistical learning. Explicit QBound becomes redundant and interferes with learning dynamics.

    \item \textbf{Continuous control (DDPG/TD3):} +4-7\% improvement with Architectural QBound. DDPG achieves +4.8\% and TD3 achieves +7.2\% with Architectural QBound ($Q = -\text{softplus}(\text{logits})$). \textbf{Key mechanism:} Architectural enforcement provides \textit{stabilization} rather than strict bounding, preserving gradients for continuous control while ensuring Q-values remain in valid range.

    \item \textbf{Sparse terminal rewards (GridWorld, FrozenLake):} -1\% to -2\% (essentially neutral). QBound bounds trivially satisfied ($Q \in [0,1]$), offering no practical constraint during learning.

    \item \textbf{State-dependent negative (MountainCar, Acrobot):} -3.6\% to -47.4\% degradation. MountainCar DDQN worst case: -47.4\% (baseline: -122.72 $\pm$ 17.04, QBound: -180.93 $\pm$ 38.15). Both environments have $r = -1$ until goal reached. Upper bound $Q \leq 0$ naturally satisfied by Bellman equation with negative rewards, making QBound redundant.
\end{itemize}

\textbf{Overall success rate:} 40\% (6/15 algorithm-environment combinations show $>$10\% improvement), 13\% neutral, 47\% degradation. \textbf{Key insight:} Reinforcement learning is reward \textit{maximization}—the upper bound matters for preventing overestimation, while the lower bound is irrelevant to the optimization objective. For positive rewards, neural networks lack natural upper bounds (requiring QBound). For negative rewards, upper bound (Q $\leq$ 0) is automatically satisfied, eliminating QBound's benefit.

\textbf{Theoretical contribution:} Proof that for negative rewards ($r \leq 0$), the Bellman equation naturally constrains $Q(s,a) \leq 0$ through recursive bootstrapping. Network learns this constraint via statistical learning over 100,000+ gradient updates, requiring no architectural constraint. This finding has implications beyond QBound for understanding value function learning dynamics in RL.

\subsection{Practical Recommendations}

For practitioners in sample-constrained domains, we provide algorithm-specific guidance based on comprehensive evaluation:

\begin{table}[h]
\centering
\caption{Algorithm-Specific QBound Recommendations (5-seed validation)}
\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Algorithm} & \textbf{QBound Type} & \textbf{When to Use} & \textbf{Key Result (5 seeds)} \\
\midrule
\multicolumn{4}{@{}l}{\textit{\textbf{Value-Based (Discrete Actions):}}} \\
DQN & Hard (Static) & Positive dense rewards & CartPole: +12.0\% \\
Double DQN & Hard (Static) & Positive dense rewards & CartPole: +33.6\% \\
Dueling DQN & Hard (Static) & Positive dense rewards & CartPole: +22.5\% \\
DQN/DDQN & Hard (Static) & \textbf{Avoid: negative rewards} & Pendulum: -7.0\%, MountainCar: -47.4\% \\
\midrule
\multicolumn{4}{@{}l}{\textit{\textbf{Off-Policy Actor-Critic (Continuous Actions):}}} \\
DDPG & Architectural & Negative rewards & Pendulum: +4.8\% \\
TD3 & Architectural & Negative rewards & Pendulum: +7.2\% \\
DDPG/TD3 & Hard & \textbf{Never use} & Gradient disruption \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Scope Note:} On-policy methods (PPO, A2C, REINFORCE) are outside QBound's scope. These methods naturally suffer less from overestimation bias because: (1) value functions are updated with recent on-policy samples; (2) no max operator in value updates; (3) built-in value stabilization mechanisms.

\textbf{Key Implementation Guidelines:}

\begin{enumerate}
    \item \textbf{Choose the right QBound type:}
        \begin{itemize}
            \item \textit{Hard QBound (clipping):} Use for discrete action spaces (DQN variants)
            \item \textit{Soft/Architectural QBound:} Use for continuous action spaces (DDPG, TD3)
            \item \textit{Never use Hard QBound with continuous actions—causes gradient death}
        \end{itemize}

    \item \textbf{Primary use cases (highest benefit):}
        \begin{itemize}
            \item Hard QBound + Double DQN: Sparse rewards, discrete actions (LunarLander: 83\% success)
            \item Hard QBound + DQN variants: Positive dense rewards (CartPole: +12-34\%)
            \item Architectural QBound + DDPG/TD3: Continuous control (+4-7\%)
        \end{itemize}

    \item \textbf{Algorithm-specific warnings:}
        \begin{itemize}
            \item \textit{Negative rewards + DQN:} QBound is redundant (Bellman naturally constrains Q ≤ 0)
            \item \textit{DDPG/TD3 + Hard QBound:} Catastrophic failure (gradient disruption)
        \end{itemize}

    \item \textbf{Bound selection:}
        \begin{itemize}
            \item \textit{Dense rewards:} Use dynamic (step-aware) bounds
            \item \textit{Sparse rewards:} Use static bounds
            \item \textit{Derive from environment:} $Q_{\max} = \frac{1-\gamma^H}{1-\gamma} r_{\max}$
        \end{itemize}

    \item \textbf{Soft QBound hyperparameter:}
        \begin{itemize}
            \item Penalty weight: $\lambda = 0.1$ to $1.0$ (start with 0.1)
            \item Loss: $\mathcal{L}_{\text{QBound}} = \lambda[\max(0, Q-Q_{\max})^2 + \max(0, Q_{\min}-Q)^2]$
        \end{itemize}

    \item \textbf{Integration approach:}
        \begin{itemize}
            \item \textit{Hard QBound:} Clip during target computation (3-5 lines of code)
            \item \textit{Soft QBound:} Add penalty to loss function (5-10 lines of code)
            \item Combine with existing methods (Double-Q, target networks) for complementary benefits
        \end{itemize}
\end{enumerate}

\subsection{Final Remarks}

QBound represents a simple yet principled approach to improving reinforcement learning through environment-aware stabilization. By enforcing theoretically-derived bounds through bootstrapping-based clipping, QBound makes value-based methods significantly more sample-efficient in sparse-reward environments.

For the reinforcement learning community, QBound offers a practical tool that can be immediately applied to existing algorithms with minimal modification. \textbf{Our comprehensive 7-environment evaluation reveals that QBound is most effective for sparse-reward tasks with known reward bounds}, achieving dramatic improvements on challenging benchmarks like LunarLander (+263.9\%, 83\% success rate with QBound+Double DQN) while showing moderate degradation on exploration-critical tasks (MountainCar: -16.6\%, Acrobot: -7.6\%).

\textbf{Key insights from comprehensive evaluation:}
\begin{enumerate}
    \item \textbf{Environment-dependent effectiveness:} QBound helps in 2/4 evaluated environments (average +63.5\%), with dramatic improvements on sparse-reward tasks (LunarLander: +263.9\%, CartPole: +14.2\%) but degradation on exploration-critical tasks

    \item \textbf{Best with Double DQN:} The combination QBound+Double DQN achieves optimal performance on sparse-reward tasks (LunarLander: 228.0 $\pm$ 89.6, 83\% success, lowest variance), demonstrating that environment-aware bounds and algorithmic pessimism provide complementary benefits

    \item \textbf{Not universally beneficial:} Unlike initially hypothesized, QBound is not a universal improvement. It works best for sparse-reward tasks with known bounds but can hurt performance in exploration-critical environments where Q-value constraints may limit exploration

    \item \textbf{Environment characteristics matter:} This work demonstrates that \textit{environment characteristics fundamentally determine whether pessimistic Q-learning helps or hurts}. Sparse rewards benefit from reduced overestimation; exploration-critical tasks suffer from over-constraint
\end{enumerate}

\textbf{Practical guidance:} If you're using value-based methods with discrete action spaces and working on sparse-reward tasks with known reward bounds, consider QBound—especially combined with Double DQN. For exploration-critical tasks (mountaincar-like), stick with standard methods. For dense-reward tasks, QBound provides moderate improvements.

\textbf{Important caveats:}
\begin{itemize}
    \item \textbf{Implementation choice is critical:} QBound's success depends on choosing the right variant:
        \begin{itemize}
            \item \textit{Hard QBound (clipping):} Excellent for discrete actions (DQN: +263.9\%), catastrophic for continuous (DDPG: -893\%)
            \item \textit{Soft QBound (penalty):} Works for both discrete and continuous actions, required for actor-critic methods
        \end{itemize}

    \item \textbf{Continuous action compatibility:} QBound applicability to continuous action spaces depends on implementation:
        \begin{itemize}
            \item \textit{Soft QBound + DDPG:} +712\% improvement, can partially replace target networks
            \item \textit{Soft QBound + TD3:} Fails catastrophically (-600\%), conflicts with double-Q clipping
            \item \textit{Hard QBound + DDPG/TD3:} Incompatible—gradient disruption causes 893\% degradation
        \end{itemize}

    \item \textbf{Algorithm-specific tuning required:} QBound shows strong algorithm dependence:
        \begin{itemize}
            \item \textit{Excellent:} DQN variants on positive rewards (+12-34\% CartPole)
            \item \textit{Good:} DDPG/TD3 with Architectural QBound (+4-7\% Pendulum)
            \item \textit{Fails:} DQN on negative rewards (Bellman naturally constrains Q ≤ 0)
        \end{itemize}

    \item \textbf{Environment characteristics matter:} Not universally beneficial:
        \begin{itemize}
            \item \textit{Best:} Positive dense rewards with known bounds (CartPole: +12-34\%)
            \item \textit{Good:} Sparse rewards with discrete actions (LunarLander: +263.9\%)
            \item \textit{Hurts:} Negative rewards (MountainCar: -47.4\%, Pendulum DQN: -7\%)
        \end{itemize}

    \item \textbf{Known bounds required:} QBound requires reasonably tight bounds derivable from environment structure
\end{itemize}

\textbf{Bottom line:} QBound provides dramatic improvements (+263.9\%) on challenging sparse-reward discrete-action tasks like LunarLander, achieving 83\% success rate when combined with Double DQN. However, it's not a universal solution—apply it selectively to appropriate environments for maximum benefit.

\section*{Open Research and Contributions}

This is an \textbf{ongoing research project}. The central open question---why QBound works for positive but not negative rewards---remains unsolved. We invite the research community to contribute experiments, analysis, and theoretical insights. Promising directions include: (1) Q-value transformation approaches for negative rewards, (2) testing on additional benchmarks (Atari, MuJoCo), and (3) investigating the interaction between network initialization and reward sign.

\textbf{Repository:} \url{https://github.com/tzemuy13/QBound}

\section*{Acknowledgments}

This research was conducted by the author with Claude (Anthropic) serving as an AI coding and research assistant. Claude assisted with code implementation, experimental design, data analysis, and manuscript preparation. All research direction, core ideas, and final decisions were made by the author.

We acknowledge the open-source RL community for providing the foundational implementations that made this research possible. Special thanks to the maintainers of OpenAI Gym \citep{brockman2016openai}, Stable-Baselines3 \citep{raffin2021stable}, and Spinning Up \citep{SpinningUp2018} for creating the tools that enabled this evaluation.

\section*{Reproducibility Statement}

All code, hyperparameters, and experimental configurations are publicly available at \url{https://github.com/tzemuy13/QBound}. The repository includes: (1) complete implementations for all environments with documented hyperparameters, (2) deterministic seeding protocol (seeds 42-46) ensuring exact reproducibility, and (3) detailed experiment scripts with environment-specific configurations. Our implementation builds on PyTorch, OpenAI Gym \citep{brockman2016openai}, and Gymnasium, following established experimental protocols. All random seeds, network architectures, and training procedures are explicitly documented in the codebase to enable exact replication of our results.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}