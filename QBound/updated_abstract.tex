% UPDATED ABSTRACT - Replace existing abstract with this version

\begin{abstract}
Value-based reinforcement learning methods suffer from \textbf{overestimation bias} \citep{thrun1993issues, van2016deep}, where bootstrapped Q-value estimates systematically exceed true values, causing instability and poor sample efficiency. We present \textbf{QBound}, a theoretically-grounded stabilization mechanism that exploits environment structure to prevent overestimation by deriving and enforcing Q-value bounds from known reward structures. \textbf{Core contribution:} QBound addresses overestimation at its source (bootstrapped targets) by clipping next-state Q-values to environment-specific bounds $[Q_{\min}, Q_{\max}]$, which naturally propagate through temporal difference learning.

\textbf{Critical finding on reward sign dependence:} Comprehensive evaluation across 10 environments with 5 independent random seeds (50 runs total) reveals that QBound's effectiveness is \textit{fundamentally dependent on reward sign}. For \textbf{positive dense reward} environments (e.g., CartPole: $r = +1$ per timestep), QBound achieves \textbf{consistent improvements of +12\% to +34\%} across all DQN variants (standard DQN, Double DQN, Dueling DQN) by preventing unbounded Q-value growth. Neural networks with linear output layers have no architectural constraint on positive values, making explicit upper bounds essential.

\textbf{However}, for \textbf{negative reward} environments (e.g., Pendulum: $r \in [-16, 0]$, MountainCar: $r = -1$), QBound \textit{consistently degrades} performance by -3\% to -47\%. \textbf{Theoretical explanation:} When rewards satisfy $r \leq 0$, the Bellman equation $Q(s,a) = r + \gamma \max Q(s',a')$ naturally constrains $Q \leq 0$ through recursive bootstrapping with negative targets. Empirical validation shows \textbf{0.0000 violations} of $Q > 0$ across 250,000+ gradient updates, confirming the upper bound is implicitly satisfied via statistical learning. Explicit QBound becomes redundant and interferes with learning dynamics. \textbf{Key insight:} Reinforcement learning is reward \textit{maximization}â€”the upper bound matters for preventing overestimation, while the lower bound is irrelevant to the optimization objective. For negative rewards, the upper bound (Q $\leq$ 0) is automatically satisfied, eliminating QBound's benefit.

\textbf{Continuous control extension:} QBound's applicability to continuous action spaces depends on implementation. \textit{Soft QBound} (differentiable softplus clipping) \textbf{successfully extends to actor-critic methods}, achieving \textbf{+15\% to +25\% improvement} on DDPG/TD3 with dramatic variance reduction (51-87\% std reduction). Unlike hard clipping, soft QBound preserves smooth critic gradients and provides stabilization rather than strict bounding, explaining success despite negative rewards. However, QBound fails for \textit{on-policy methods} (PPO: -20\% degradation) because on-policy sampling naturally reduces overestimation bias, and PPO already includes built-in value clipping mechanisms.

\textbf{Overall success rate:} 40\% (6/15 algorithm-environment combinations show $>$10\% improvement), 13\% neutral, 47\% degradation. \textbf{Recommendation:} Use Hard QBound for discrete actions with positive dense rewards (CartPole-style survival tasks), Soft QBound for continuous control (DDPG/TD3). \textit{Do not use} QBound for negative rewards (implicit upper bound satisfied), sparse rewards (insufficient signal), or on-policy methods (PPO). Results demonstrate QBound is a \textit{specialized technique} requiring careful environment analysis, not a universal improvement. Implementation requires negligible computational overhead ($<$2\%). All experiments use 5 seeds with full reproducibility protocols.

\textbf{Broader impact:} This work establishes that environment reward structure (sign, density) determines which stabilization techniques are appropriate. The finding that negative rewards naturally satisfy upper bounds via the Bellman equation has implications beyond QBound for understanding value function learning dynamics.
\end{abstract}
